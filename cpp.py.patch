# Patch generated by Pyment v0.3.3

--- a/..//venv/lib/python3.8/site-packages/pycparser/ply/cpp.py
+++ b/..//venv/lib/python3.8/site-packages/pycparser/ply/cpp.py
@@ -29,6 +29,17 @@
 
 # Whitespace
 def t_CPP_WS(t):
+    """
+
+    Parameters
+    ----------
+    t :
+        
+
+    Returns
+    -------
+
+    """
     r'\s+'
     t.lexer.lineno += t.value.count("\n")
     return t
@@ -41,6 +52,17 @@
 
 # Integer literal
 def CPP_INTEGER(t):
+    """
+
+    Parameters
+    ----------
+    t :
+        
+
+    Returns
+    -------
+
+    """
     r'(((((0x)|(0X))[0-9a-fA-F]+)|(\d+))([uU][lL]|[lL][uU]|[uU]|[lL])?)'
     return t
 
@@ -51,18 +73,51 @@
 
 # String literal
 def t_CPP_STRING(t):
+    """
+
+    Parameters
+    ----------
+    t :
+        
+
+    Returns
+    -------
+
+    """
     r'\"([^\\\n]|(\\(.|\n)))*?\"'
     t.lexer.lineno += t.value.count("\n")
     return t
 
 # Character constant 'c' or L'c'
 def t_CPP_CHAR(t):
+    """
+
+    Parameters
+    ----------
+    t :
+        
+
+    Returns
+    -------
+
+    """
     r'(L)?\'([^\\\n]|(\\(.|\n)))*?\''
     t.lexer.lineno += t.value.count("\n")
     return t
 
 # Comment
 def t_CPP_COMMENT1(t):
+    """
+
+    Parameters
+    ----------
+    t :
+        
+
+    Returns
+    -------
+
+    """
     r'(/\*(.|\n)*?\*/)'
     ncr = t.value.count("\n")
     t.lexer.lineno += ncr
@@ -72,12 +127,34 @@
 
 # Line comment
 def t_CPP_COMMENT2(t):
+    """
+
+    Parameters
+    ----------
+    t :
+        
+
+    Returns
+    -------
+
+    """
     r'(//.*?(\n|$))'
     # replace with '/n'
     t.type = 'CPP_WS'; t.value = '\n'
     return t
 
 def t_error(t):
+    """
+
+    Parameters
+    ----------
+    t :
+        
+
+    Returns
+    -------
+
+    """
     t.type = t.value[0]
     t.value = t.value[0]
     t.lexer.skip(1)
@@ -119,6 +196,17 @@
 }
 
 def trigraph(input):
+    """
+
+    Parameters
+    ----------
+    input :
+        
+
+    Returns
+    -------
+
+    """
     return _trigraph_pat.sub(lambda g: _trigraph_rep[g.group()[-1]],input)
 
 # ------------------------------------------------------------------
@@ -138,6 +226,7 @@
 # ------------------------------------------------------------------
 
 class Macro(object):
+    """ """
     def __init__(self,name,value,arglist=None,variadic=False):
         self.name = name
         self.value = value
@@ -155,6 +244,7 @@
 # ------------------------------------------------------------------
 
 class Preprocessor(object):
+    """ """
     def __init__(self,lexer=None):
         if lexer is None:
             lexer = lex.lexer
@@ -178,6 +268,17 @@
     # -----------------------------------------------------------------------------
 
     def tokenize(self,text):
+        """
+
+        Parameters
+        ----------
+        text :
+            
+
+        Returns
+        -------
+
+        """
         tokens = []
         self.lexer.input(text)
         while True:
@@ -193,6 +294,21 @@
     # ----------------------------------------------------------------------
 
     def error(self,file,line,msg):
+        """
+
+        Parameters
+        ----------
+        file :
+            
+        line :
+            
+        msg :
+            
+
+        Returns
+        -------
+
+        """
         print("%s:%d %s" % (file,line,msg))
 
     # ----------------------------------------------------------------------
@@ -205,6 +321,7 @@
     # ----------------------------------------------------------------------
 
     def lexprobe(self):
+        """ """
 
         # Determine the token type for identifiers
         self.lexer.input("identifier")
@@ -265,6 +382,17 @@
     # ----------------------------------------------------------------------
 
     def add_path(self,path):
+        """
+
+        Parameters
+        ----------
+        path :
+            
+
+        Returns
+        -------
+
+        """
         self.path.append(path)
 
     # ----------------------------------------------------------------------
@@ -277,6 +405,17 @@
     # ----------------------------------------------------------------------
 
     def group_lines(self,input):
+        """
+
+        Parameters
+        ----------
+        input :
+            
+
+        Returns
+        -------
+
+        """
         lex = self.lexer.clone()
         lines = [x.rstrip() for x in input.splitlines()]
         for i in xrange(len(lines)):
@@ -310,6 +449,17 @@
     # ----------------------------------------------------------------------
 
     def tokenstrip(self,tokens):
+        """
+
+        Parameters
+        ----------
+        tokens :
+            
+
+        Returns
+        -------
+
+        """
         i = 0
         while i < len(tokens) and tokens[i].type in self.t_WS:
             i += 1
@@ -338,6 +488,17 @@
     # ----------------------------------------------------------------------
 
     def collect_args(self,tokenlist):
+        """
+
+        Parameters
+        ----------
+        tokenlist :
+            
+
+        Returns
+        -------
+
+        """
         args = []
         positions = []
         current_arg = []
@@ -391,6 +552,17 @@
     # ----------------------------------------------------------------------
 
     def macro_prescan(self,macro):
+        """
+
+        Parameters
+        ----------
+        macro :
+            
+
+        Returns
+        -------
+
+        """
         macro.patch     = []             # Standard macro arguments
         macro.str_patch = []             # String conversion expansion
         macro.var_comma_patch = []       # Variadic macro comma patch
@@ -434,6 +606,19 @@
     # ----------------------------------------------------------------------
 
     def macro_expand_args(self,macro,args):
+        """
+
+        Parameters
+        ----------
+        macro :
+            
+        args :
+            
+
+        Returns
+        -------
+
+        """
         # Make a copy of the macro token sequence
         rep = [copy.copy(_x) for _x in macro.value]
 
@@ -484,6 +669,19 @@
     # ----------------------------------------------------------------------
 
     def expand_macros(self,tokens,expanded=None):
+        """
+
+        Parameters
+        ----------
+        tokens :
+            
+        expanded :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         if expanded is None:
             expanded = {}
         i = 0
@@ -550,6 +748,17 @@
     # ----------------------------------------------------------------------
 
     def evalexpr(self,tokens):
+        """
+
+        Parameters
+        ----------
+        tokens :
+            
+
+        Returns
+        -------
+
+        """
         # tokens = tokenize(line)
         # Search for defined macros
         i = 0
@@ -609,6 +818,19 @@
     # Parse an input string/
     # ----------------------------------------------------------------------
     def parsegen(self,input,source=None):
+        """
+
+        Parameters
+        ----------
+        input :
+            
+        source :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
 
         # Replace trigraph sequences
         t = trigraph(input)
@@ -740,6 +962,17 @@
     # ----------------------------------------------------------------------
 
     def include(self,tokens):
+        """
+
+        Parameters
+        ----------
+        tokens :
+            
+
+        Returns
+        -------
+
+        """
         # Try to extract the filename and then process an include file
         if not tokens:
             return
@@ -789,6 +1022,17 @@
     # ----------------------------------------------------------------------
 
     def define(self,tokens):
+        """
+
+        Parameters
+        ----------
+        tokens :
+            
+
+        Returns
+        -------
+
+        """
         if isinstance(tokens,STRING_TYPES):
             tokens = self.tokenize(tokens)
 
@@ -859,6 +1103,17 @@
     # ----------------------------------------------------------------------
 
     def undef(self,tokens):
+        """
+
+        Parameters
+        ----------
+        tokens :
+            
+
+        Returns
+        -------
+
+        """
         id = tokens[0].value
         try:
             del self.macros[id]
@@ -871,6 +1126,21 @@
     # Parse input text.
     # ----------------------------------------------------------------------
     def parse(self,input,source=None,ignore={}):
+        """
+
+        Parameters
+        ----------
+        input :
+            
+        source :
+             (Default value = None)
+        ignore :
+             (Default value = {})
+
+        Returns
+        -------
+
+        """
         self.ignore = ignore
         self.parser = self.parsegen(input,source)
 
@@ -880,6 +1150,7 @@
     # Method to return individual tokens
     # ----------------------------------------------------------------------
     def token(self):
+        """ """
         try:
             while True:
                 tok = next(self.parser)
