# Patch generated by Pyment v0.3.3

--- a/..//venv/lib/python3.8/site-packages/pandas/tests/io/test_parquet.py
+++ b/..//venv/lib/python3.8/site-packages/pandas/tests/io/test_parquet.py
@@ -59,11 +59,23 @@
     ]
 )
 def engine(request):
+    """
+
+    Parameters
+    ----------
+    request :
+        
+
+    Returns
+    -------
+
+    """
     return request.param
 
 
 @pytest.fixture
 def pa():
+    """ """
     if not _HAVE_PYARROW:
         pytest.skip("pyarrow is not installed")
     return "pyarrow"
@@ -71,6 +83,7 @@
 
 @pytest.fixture
 def fp():
+    """ """
     if not _HAVE_FASTPARQUET:
         pytest.skip("fastparquet is not installed")
     return "fastparquet"
@@ -78,11 +91,13 @@
 
 @pytest.fixture
 def df_compat():
+    """ """
     return pd.DataFrame({"A": [1, 2, 3], "B": "foo"})
 
 
 @pytest.fixture
 def df_cross_compat():
+    """ """
     df = pd.DataFrame(
         {
             "a": list("abc"),
@@ -101,6 +116,7 @@
 
 @pytest.fixture
 def df_full():
+    """ """
     return pd.DataFrame(
         {
             "string": list("abc"),
@@ -135,26 +151,35 @@
     repeat=2,
 ):
     """Verify parquet serializer and deserializer produce the same results.
-
+    
     Performs a pandas to disk and disk to pandas round trip,
     then compares the 2 resulting DataFrames to verify equality.
 
     Parameters
     ----------
-    df: Dataframe
-    engine: str, optional
-        'pyarrow' or 'fastparquet'
-    path: str, optional
-    write_kwargs: dict of str:str, optional
-    read_kwargs: dict of str:str, optional
-    expected: DataFrame, optional
-        Expected deserialization result, otherwise will be equal to `df`
-    check_names: list of str, optional
-        Closed set of column names to be compared
-    check_like: bool, optional
-        If True, ignore the order of index & columns.
-    repeat: int, optional
-        How many times to repeat the test
+    df :
+        
+    engine :
+         (Default value = None)
+    path :
+         (Default value = None)
+    write_kwargs :
+         (Default value = None)
+    read_kwargs :
+         (Default value = None)
+    expected :
+         (Default value = None)
+    check_names :
+         (Default value = True)
+    check_like :
+         (Default value = False)
+    repeat :
+         (Default value = 2)
+
+    Returns
+    -------
+
+    
     """
     write_kwargs = write_kwargs or {"compression": None}
     read_kwargs = read_kwargs or {}
@@ -167,6 +192,17 @@
         read_kwargs["engine"] = engine
 
     def compare(repeat):
+        """
+
+        Parameters
+        ----------
+        repeat :
+            
+
+        Returns
+        -------
+
+        """
         for _ in range(repeat):
             df.to_parquet(path, **write_kwargs)
             with catch_warnings(record=True):
@@ -184,11 +220,35 @@
 
 
 def test_invalid_engine(df_compat):
+    """
+
+    Parameters
+    ----------
+    df_compat :
+        
+
+    Returns
+    -------
+
+    """
     with pytest.raises(ValueError):
         check_round_trip(df_compat, "foo", "bar")
 
 
 def test_options_py(df_compat, pa):
+    """
+
+    Parameters
+    ----------
+    df_compat :
+        
+    pa :
+        
+
+    Returns
+    -------
+
+    """
     # use the set option
 
     with pd.option_context("io.parquet.engine", "pyarrow"):
@@ -196,6 +256,19 @@
 
 
 def test_options_fp(df_compat, fp):
+    """
+
+    Parameters
+    ----------
+    df_compat :
+        
+    fp :
+        
+
+    Returns
+    -------
+
+    """
     # use the set option
 
     with pd.option_context("io.parquet.engine", "fastparquet"):
@@ -203,6 +276,21 @@
 
 
 def test_options_auto(df_compat, fp, pa):
+    """
+
+    Parameters
+    ----------
+    df_compat :
+        
+    fp :
+        
+    pa :
+        
+
+    Returns
+    -------
+
+    """
     # use the set option
 
     with pd.option_context("io.parquet.engine", "auto"):
@@ -210,6 +298,19 @@
 
 
 def test_options_get_engine(fp, pa):
+    """
+
+    Parameters
+    ----------
+    fp :
+        
+    pa :
+        
+
+    Returns
+    -------
+
+    """
     assert isinstance(get_engine("pyarrow"), PyArrowImpl)
     assert isinstance(get_engine("fastparquet"), FastParquetImpl)
 
@@ -230,6 +331,7 @@
 
 
 def test_get_engine_auto_error_message():
+    """ """
     # Expect different error messages from get_engine(engine="auto")
     # if engines aren't installed vs. are installed but bad version
     from pandas.compat._optional import VERSIONS
@@ -273,6 +375,21 @@
 
 
 def test_cross_engine_pa_fp(df_cross_compat, pa, fp):
+    """
+
+    Parameters
+    ----------
+    df_cross_compat :
+        
+    pa :
+        
+    fp :
+        
+
+    Returns
+    -------
+
+    """
     # cross-compat with differing reading/writing engines
 
     df = df_cross_compat
@@ -287,6 +404,21 @@
 
 
 def test_cross_engine_fp_pa(df_cross_compat, pa, fp):
+    """
+
+    Parameters
+    ----------
+    df_cross_compat :
+        
+    pa :
+        
+    fp :
+        
+
+    Returns
+    -------
+
+    """
     # cross-compat with differing reading/writing engines
 
     if (
@@ -311,7 +443,23 @@
 
 
 class Base:
+    """ """
     def check_error_on_write(self, df, engine, exc):
+        """
+
+        Parameters
+        ----------
+        df :
+            
+        engine :
+            
+        exc :
+            
+
+        Returns
+        -------
+
+        """
         # check that we are raising the exception on writing
         with tm.ensure_clean() as path:
             with pytest.raises(exc):
@@ -319,7 +467,19 @@
 
 
 class TestBasic(Base):
+    """ """
     def test_error(self, engine):
+        """
+
+        Parameters
+        ----------
+        engine :
+            
+
+        Returns
+        -------
+
+        """
         for obj in [
             pd.Series([1, 2, 3]),
             1,
@@ -330,6 +490,17 @@
             self.check_error_on_write(obj, engine, ValueError)
 
     def test_columns_dtypes(self, engine):
+        """
+
+        Parameters
+        ----------
+        engine :
+            
+
+        Returns
+        -------
+
+        """
         df = pd.DataFrame({"string": list("abc"), "int": list(range(1, 4))})
 
         # unicode
@@ -337,6 +508,17 @@
         check_round_trip(df, engine)
 
     def test_columns_dtypes_invalid(self, engine):
+        """
+
+        Parameters
+        ----------
+        engine :
+            
+
+        Returns
+        -------
+
+        """
         df = pd.DataFrame({"string": list("abc"), "int": list(range(1, 4))})
 
         # numeric
@@ -356,6 +538,19 @@
 
     @pytest.mark.parametrize("compression", [None, "gzip", "snappy", "brotli"])
     def test_compression(self, engine, compression):
+        """
+
+        Parameters
+        ----------
+        engine :
+            
+        compression :
+            
+
+        Returns
+        -------
+
+        """
 
         if compression == "snappy":
             pytest.importorskip("snappy")
@@ -367,6 +562,17 @@
         check_round_trip(df, engine, write_kwargs={"compression": compression})
 
     def test_read_columns(self, engine):
+        """
+
+        Parameters
+        ----------
+        engine :
+            
+
+        Returns
+        -------
+
+        """
         # GH18154
         df = pd.DataFrame({"string": list("abc"), "int": list(range(1, 4))})
 
@@ -376,6 +582,17 @@
         )
 
     def test_write_index(self, engine):
+        """
+
+        Parameters
+        ----------
+        engine :
+            
+
+        Returns
+        -------
+
+        """
         check_names = engine != "fastparquet"
 
         df = pd.DataFrame({"A": [1, 2, 3]})
@@ -400,6 +617,17 @@
         check_round_trip(df, engine)
 
     def test_write_multiindex(self, pa):
+        """
+
+        Parameters
+        ----------
+        pa :
+            
+
+        Returns
+        -------
+
+        """
         # Not supported in fastparquet as of 0.1.3 or older pyarrow version
         engine = pa
 
@@ -409,12 +637,34 @@
         check_round_trip(df, engine)
 
     def test_write_column_multiindex(self, engine):
+        """
+
+        Parameters
+        ----------
+        engine :
+            
+
+        Returns
+        -------
+
+        """
         # column multi-index
         mi_columns = pd.MultiIndex.from_tuples([("a", 1), ("a", 2), ("b", 1)])
         df = pd.DataFrame(np.random.randn(4, 3), columns=mi_columns)
         self.check_error_on_write(df, engine, ValueError)
 
     def test_multiindex_with_columns(self, pa):
+        """
+
+        Parameters
+        ----------
+        pa :
+            
+
+        Returns
+        -------
+
+        """
         engine = pa
         dates = pd.date_range("01-Jan-2018", "01-Dec-2018", freq="MS")
         df = pd.DataFrame(np.random.randn(2 * len(dates), 3), columns=list("ABC"))
@@ -431,6 +681,17 @@
             )
 
     def test_write_ignoring_index(self, engine):
+        """
+
+        Parameters
+        ----------
+        engine :
+            
+
+        Returns
+        -------
+
+        """
         # ENH 20768
         # Ensure index=False omits the index from the written Parquet file.
         df = pd.DataFrame({"a": [1, 2, 3], "b": ["q", "r", "s"]})
@@ -464,7 +725,21 @@
 
 
 class TestParquetPyArrow(Base):
+    """ """
     def test_basic(self, pa, df_full):
+        """
+
+        Parameters
+        ----------
+        pa :
+            
+        df_full :
+            
+
+        Returns
+        -------
+
+        """
 
         df = df_full
 
@@ -477,6 +752,19 @@
         check_round_trip(df, pa)
 
     def test_basic_subset_columns(self, pa, df_full):
+        """
+
+        Parameters
+        ----------
+        pa :
+            
+        df_full :
+            
+
+        Returns
+        -------
+
+        """
         # GH18628
 
         df = df_full
@@ -491,11 +779,33 @@
         )
 
     def test_duplicate_columns(self, pa):
+        """
+
+        Parameters
+        ----------
+        pa :
+            
+
+        Returns
+        -------
+
+        """
         # not currently able to handle duplicate columns
         df = pd.DataFrame(np.arange(12).reshape(4, 3), columns=list("aaa")).copy()
         self.check_error_on_write(df, pa, ValueError)
 
     def test_unsupported(self, pa):
+        """
+
+        Parameters
+        ----------
+        pa :
+            
+
+        Returns
+        -------
+
+        """
         if LooseVersion(pyarrow.__version__) < LooseVersion("0.15.1.dev"):
             # period - will be supported using an extension type with pyarrow 1.0
             df = pd.DataFrame({"a": pd.period_range("2013", freq="M", periods=3)})
@@ -514,6 +824,17 @@
         self.check_error_on_write(df, pa, Exception)
 
     def test_categorical(self, pa):
+        """
+
+        Parameters
+        ----------
+        pa :
+            
+
+        Returns
+        -------
+
+        """
 
         # supported in >= 0.7.0
         df = pd.DataFrame()
@@ -538,6 +859,21 @@
             check_round_trip(df, pa, expected=expected)
 
     def test_s3_roundtrip_explicit_fs(self, df_compat, s3_resource, pa):
+        """
+
+        Parameters
+        ----------
+        df_compat :
+            
+        s3_resource :
+            
+        pa :
+            
+
+        Returns
+        -------
+
+        """
         s3fs = pytest.importorskip("s3fs")
         s3 = s3fs.S3FileSystem()
         kw = dict(filesystem=s3)
@@ -550,12 +886,44 @@
         )
 
     def test_s3_roundtrip(self, df_compat, s3_resource, pa):
+        """
+
+        Parameters
+        ----------
+        df_compat :
+            
+        s3_resource :
+            
+        pa :
+            
+
+        Returns
+        -------
+
+        """
         # GH #19134
         check_round_trip(df_compat, pa, path="s3://pandas-test/pyarrow.parquet")
 
     @td.skip_if_no("s3fs")
     @pytest.mark.parametrize("partition_col", [["A"], []])
     def test_s3_roundtrip_for_dir(self, df_compat, s3_resource, pa, partition_col):
+        """
+
+        Parameters
+        ----------
+        df_compat :
+            
+        s3_resource :
+            
+        pa :
+            
+        partition_col :
+            
+
+        Returns
+        -------
+
+        """
         # GH #26388
         expected_df = df_compat.copy()
 
@@ -591,6 +959,17 @@
     @tm.network
     @td.skip_if_no("pyarrow")
     def test_parquet_read_from_url(self, df_compat):
+        """
+
+        Parameters
+        ----------
+        df_compat :
+            
+
+        Returns
+        -------
+
+        """
         url = (
             "https://raw.githubusercontent.com/pandas-dev/pandas/"
             "master/pandas/tests/io/data/parquet/simple.parquet"
@@ -600,6 +979,17 @@
 
     @td.skip_if_no("pyarrow")
     def test_read_file_like_obj_support(self, df_compat):
+        """
+
+        Parameters
+        ----------
+        df_compat :
+            
+
+        Returns
+        -------
+
+        """
         buffer = BytesIO()
         df_compat.to_parquet(buffer)
         df_from_buf = pd.read_parquet(buffer)
@@ -607,6 +997,19 @@
 
     @td.skip_if_no("pyarrow")
     def test_expand_user(self, df_compat, monkeypatch):
+        """
+
+        Parameters
+        ----------
+        df_compat :
+            
+        monkeypatch :
+            
+
+        Returns
+        -------
+
+        """
         monkeypatch.setenv("HOME", "TestingUser")
         monkeypatch.setenv("USERPROFILE", "TestingUser")
         with pytest.raises(OSError, match=r".*TestingUser.*"):
@@ -615,6 +1018,19 @@
             df_compat.to_parquet("~/file.parquet")
 
     def test_partition_cols_supported(self, pa, df_full):
+        """
+
+        Parameters
+        ----------
+        pa :
+            
+        df_full :
+            
+
+        Returns
+        -------
+
+        """
         # GH #23283
         partition_cols = ["bool", "int"]
         df = df_full
@@ -627,6 +1043,19 @@
             assert dataset.partitions.partition_names == set(partition_cols)
 
     def test_partition_cols_string(self, pa, df_full):
+        """
+
+        Parameters
+        ----------
+        pa :
+            
+        df_full :
+            
+
+        Returns
+        -------
+
+        """
         # GH #27117
         partition_cols = "bool"
         partition_cols_list = [partition_cols]
@@ -640,11 +1069,33 @@
             assert dataset.partitions.partition_names == set(partition_cols_list)
 
     def test_empty_dataframe(self, pa):
+        """
+
+        Parameters
+        ----------
+        pa :
+            
+
+        Returns
+        -------
+
+        """
         # GH #27339
         df = pd.DataFrame()
         check_round_trip(df, pa)
 
     def test_write_with_schema(self, pa):
+        """
+
+        Parameters
+        ----------
+        pa :
+            
+
+        Returns
+        -------
+
+        """
         import pyarrow
 
         df = pd.DataFrame({"x": [0, 1]})
@@ -654,6 +1105,17 @@
 
     @td.skip_if_no("pyarrow", min_version="0.15.0")
     def test_additional_extension_arrays(self, pa):
+        """
+
+        Parameters
+        ----------
+        pa :
+            
+
+        Returns
+        -------
+
+        """
         # test additional ExtensionArrays that are supported through the
         # __arrow_array__ protocol
         df = pd.DataFrame(
@@ -682,6 +1144,17 @@
 
     @td.skip_if_no("pyarrow", min_version="0.16.0")
     def test_additional_extension_types(self, pa):
+        """
+
+        Parameters
+        ----------
+        pa :
+            
+
+        Returns
+        -------
+
+        """
         # test additional ExtensionArrays that are supported through the
         # __arrow_array__ protocol + by defining a custom ExtensionType
         df = pd.DataFrame(
@@ -695,6 +1168,17 @@
 
     @td.skip_if_no("pyarrow", min_version="0.14")
     def test_timestamp_nanoseconds(self, pa):
+        """
+
+        Parameters
+        ----------
+        pa :
+            
+
+        Returns
+        -------
+
+        """
         # with version 2.0, pyarrow defaults to writing the nanoseconds, so
         # this should work without error
         df = pd.DataFrame({"a": pd.date_range("2017-01-01", freq="1n", periods=10)})
@@ -702,6 +1186,17 @@
 
     @td.skip_if_no("pyarrow", min_version="0.17")
     def test_filter_row_groups(self, pa):
+        """
+
+        Parameters
+        ----------
+        pa :
+            
+
+        Returns
+        -------
+
+        """
         # https://github.com/pandas-dev/pandas/issues/26551
         df = pd.DataFrame({"a": list(range(0, 3))})
         with tm.ensure_clean() as path:
@@ -713,8 +1208,22 @@
 
 
 class TestParquetFastParquet(Base):
+    """ """
     @td.skip_if_no("fastparquet", min_version="0.3.2")
     def test_basic(self, fp, df_full):
+        """
+
+        Parameters
+        ----------
+        fp :
+            
+        df_full :
+            
+
+        Returns
+        -------
+
+        """
         df = df_full
 
         dti = pd.date_range("20130101", periods=3, tz="US/Eastern")
@@ -725,17 +1234,50 @@
 
     @pytest.mark.skip(reason="not supported")
     def test_duplicate_columns(self, fp):
+        """
+
+        Parameters
+        ----------
+        fp :
+            
+
+        Returns
+        -------
+
+        """
 
         # not currently able to handle duplicate columns
         df = pd.DataFrame(np.arange(12).reshape(4, 3), columns=list("aaa")).copy()
         self.check_error_on_write(df, fp, ValueError)
 
     def test_bool_with_none(self, fp):
+        """
+
+        Parameters
+        ----------
+        fp :
+            
+
+        Returns
+        -------
+
+        """
         df = pd.DataFrame({"a": [True, None, False]})
         expected = pd.DataFrame({"a": [1.0, np.nan, 0.0]}, dtype="float16")
         check_round_trip(df, fp, expected=expected)
 
     def test_unsupported(self, fp):
+        """
+
+        Parameters
+        ----------
+        fp :
+            
+
+        Returns
+        -------
+
+        """
 
         # period
         df = pd.DataFrame({"a": pd.period_range("2013", freq="M", periods=3)})
@@ -746,10 +1288,32 @@
         self.check_error_on_write(df, fp, ValueError)
 
     def test_categorical(self, fp):
+        """
+
+        Parameters
+        ----------
+        fp :
+            
+
+        Returns
+        -------
+
+        """
         df = pd.DataFrame({"a": pd.Categorical(list("abc"))})
         check_round_trip(df, fp)
 
     def test_filter_row_groups(self, fp):
+        """
+
+        Parameters
+        ----------
+        fp :
+            
+
+        Returns
+        -------
+
+        """
         d = {"a": list(range(0, 3))}
         df = pd.DataFrame(d)
         with tm.ensure_clean() as path:
@@ -758,10 +1322,38 @@
         assert len(result) == 1
 
     def test_s3_roundtrip(self, df_compat, s3_resource, fp):
+        """
+
+        Parameters
+        ----------
+        df_compat :
+            
+        s3_resource :
+            
+        fp :
+            
+
+        Returns
+        -------
+
+        """
         # GH #19134
         check_round_trip(df_compat, fp, path="s3://pandas-test/fastparquet.parquet")
 
     def test_partition_cols_supported(self, fp, df_full):
+        """
+
+        Parameters
+        ----------
+        fp :
+            
+        df_full :
+            
+
+        Returns
+        -------
+
+        """
         # GH #23283
         partition_cols = ["bool", "int"]
         df = df_full
@@ -779,6 +1371,19 @@
             assert len(actual_partition_cols) == 2
 
     def test_partition_cols_string(self, fp, df_full):
+        """
+
+        Parameters
+        ----------
+        fp :
+            
+        df_full :
+            
+
+        Returns
+        -------
+
+        """
         # GH #27117
         partition_cols = "bool"
         df = df_full
@@ -796,6 +1401,19 @@
             assert len(actual_partition_cols) == 1
 
     def test_partition_on_supported(self, fp, df_full):
+        """
+
+        Parameters
+        ----------
+        fp :
+            
+        df_full :
+            
+
+        Returns
+        -------
+
+        """
         # GH #23283
         partition_cols = ["bool", "int"]
         df = df_full
@@ -813,6 +1431,19 @@
             assert len(actual_partition_cols) == 2
 
     def test_error_on_using_partition_cols_and_partition_on(self, fp, df_full):
+        """
+
+        Parameters
+        ----------
+        fp :
+            
+        df_full :
+            
+
+        Returns
+        -------
+
+        """
         # GH #23283
         partition_cols = ["bool", "int"]
         df = df_full
@@ -827,6 +1458,17 @@
                 )
 
     def test_empty_dataframe(self, fp):
+        """
+
+        Parameters
+        ----------
+        fp :
+            
+
+        Returns
+        -------
+
+        """
         # GH #27339
         df = pd.DataFrame()
         expected = df.copy()
