# Patch generated by Pyment v0.3.3

--- a/..//venv/lib/python3.8/site-packages/pygments/lexers/robotframework.py
+++ b/..//venv/lib/python3.8/site-packages/pygments/lexers/robotframework.py
@@ -46,6 +46,19 @@
 
 
 def normalize(string, remove=''):
+    """
+
+    Parameters
+    ----------
+    string :
+        
+    remove :
+         (Default value = '')
+
+    Returns
+    -------
+
+    """
     string = string.lower()
     for char in remove + ' ':
         if char in string:
@@ -54,12 +67,18 @@
 
 
 class RobotFrameworkLexer(Lexer):
-    """
-    For `Robot Framework <http://robotframework.org>`_ test data.
-
+    """For `Robot Framework <http://robotframework.org>`_ test data.
+    
     Supports both space and pipe separated plain text formats.
-
+    
     .. versionadded:: 1.6
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'RobotFramework'
     aliases = ['robotframework']
@@ -72,6 +91,17 @@
         Lexer.__init__(self, **options)
 
     def get_tokens_unprocessed(self, text):
+        """
+
+        Parameters
+        ----------
+        text :
+            
+
+        Returns
+        -------
+
+        """
         row_tokenizer = RowTokenizer()
         var_tokenizer = VariableTokenizer()
         index = 0
@@ -84,8 +114,22 @@
 
 
 class VariableTokenizer:
+    """ """
 
     def tokenize(self, string, token):
+        """
+
+        Parameters
+        ----------
+        string :
+            
+        token :
+            
+
+        Returns
+        -------
+
+        """
         var = VariableSplitter(string, identifiers='$@%&')
         if var.start < 0 or token in (COMMENT, ERROR):
             yield string, token
@@ -95,6 +139,21 @@
                 yield value, token
 
     def _tokenize(self, var, string, orig_token):
+        """
+
+        Parameters
+        ----------
+        var :
+            
+        string :
+            
+        orig_token :
+            
+
+        Returns
+        -------
+
+        """
         before = string[:var.start]
         yield before, orig_token
         yield var.identifier + '{', SYNTAX
@@ -108,6 +167,7 @@
 
 
 class RowTokenizer:
+    """ """
 
     def __init__(self):
         self._table = UnknownTable()
@@ -125,6 +185,17 @@
                         'userkeywords': keywords, 'userkeyword': keywords}
 
     def tokenize(self, row):
+        """
+
+        Parameters
+        ----------
+        row :
+            
+
+        Returns
+        -------
+
+        """
         commented = False
         heading = False
         for index, value in enumerate(self._splitter.split(row)):
@@ -140,10 +211,40 @@
         self._table.end_row()
 
     def _start_table(self, header):
+        """
+
+        Parameters
+        ----------
+        header :
+            
+
+        Returns
+        -------
+
+        """
         name = normalize(header, remove='*')
         return self._tables.get(name, UnknownTable())
 
     def _tokenize(self, value, index, commented, separator, heading):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+        index :
+            
+        commented :
+            
+        separator :
+            
+        heading :
+            
+
+        Returns
+        -------
+
+        """
         if commented:
             yield value, COMMENT
         elif separator:
@@ -155,20 +256,54 @@
 
 
 class RowSplitter:
+    """ """
     _space_splitter = re.compile('( {2,})')
     _pipe_splitter = re.compile(r'((?:^| +)\|(?: +|$))')
 
     def split(self, row):
+        """
+
+        Parameters
+        ----------
+        row :
+            
+
+        Returns
+        -------
+
+        """
         splitter = (row.startswith('| ') and self._split_from_pipes
                     or self._split_from_spaces)
         yield from splitter(row)
         yield '\n'
 
     def _split_from_spaces(self, row):
+        """
+
+        Parameters
+        ----------
+        row :
+            
+
+        Returns
+        -------
+
+        """
         yield ''  # Start with (pseudo)separator similarly as with pipes
         yield from self._space_splitter.split(row)
 
     def _split_from_pipes(self, row):
+        """
+
+        Parameters
+        ----------
+        row :
+            
+
+        Returns
+        -------
+
+        """
         _, separator, rest = self._pipe_splitter.split(row, 1)
         yield separator
         while self._pipe_splitter.search(rest):
@@ -179,12 +314,24 @@
 
 
 class Tokenizer:
+    """ """
     _tokens = None
 
     def __init__(self):
         self._index = 0
 
     def tokenize(self, value):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+
+        Returns
+        -------
+
+        """
         values_and_tokens = self._tokenize(value, self._index)
         self._index += 1
         if isinstance(values_and_tokens, type(Token)):
@@ -192,10 +339,34 @@
         return values_and_tokens
 
     def _tokenize(self, value, index):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         index = min(index, len(self._tokens) - 1)
         return self._tokens[index]
 
     def _is_assign(self, value):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+
+        Returns
+        -------
+
+        """
         if value.endswith('='):
             value = value[:-1].strip()
         var = VariableSplitter(value, identifiers='$@&')
@@ -203,10 +374,12 @@
 
 
 class Comment(Tokenizer):
+    """ """
     _tokens = (COMMENT,)
 
 
 class Setting(Tokenizer):
+    """ """
     _tokens = (SETTING, ARGUMENT)
     _keyword_settings = ('suitesetup', 'suiteprecondition', 'suiteteardown',
                          'suitepostcondition', 'testsetup', 'tasksetup', 'testprecondition',
@@ -221,6 +394,19 @@
         self._template_setter = template_setter
 
     def _tokenize(self, value, index):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         if index == 1 and self._template_setter:
             self._template_setter(value)
         if index == 0:
@@ -237,16 +423,31 @@
 
 
 class ImportSetting(Tokenizer):
+    """ """
     _tokens = (IMPORT, ARGUMENT)
 
 
 class TestCaseSetting(Setting):
+    """ """
     _keyword_settings = ('setup', 'precondition', 'teardown', 'postcondition',
                          'template')
     _import_settings = ()
     _other_settings = ('documentation', 'tags', 'timeout')
 
     def _tokenize(self, value, index):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         if index == 0:
             type = Setting._tokenize(self, value[1:-1], index)
             return [('[', SYNTAX), (value[1:-1], type), (']', SYNTAX)]
@@ -254,20 +455,36 @@
 
 
 class KeywordSetting(TestCaseSetting):
+    """ """
     _keyword_settings = ('teardown',)
     _other_settings = ('documentation', 'arguments', 'return', 'timeout', 'tags')
 
 
 class Variable(Tokenizer):
+    """ """
     _tokens = (SYNTAX, ARGUMENT)
 
     def _tokenize(self, value, index):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         if index == 0 and not self._is_assign(value):
             return ERROR
         return Tokenizer._tokenize(self, value, index)
 
 
 class KeywordCall(Tokenizer):
+    """ """
     _tokens = (KEYWORD, ARGUMENT)
 
     def __init__(self, support_assign=True):
@@ -276,6 +493,19 @@
         self._assigns = 0
 
     def _tokenize(self, value, index):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         if not self._keyword_found and self._is_assign(value):
             self._assigns += 1
             return SYNTAX  # VariableTokenizer tokenizes this later.
@@ -286,9 +516,23 @@
 
 
 class GherkinTokenizer:
+    """ """
     _gherkin_prefix = re.compile('^(Given|When|Then|And) ', re.IGNORECASE)
 
     def tokenize(self, value, token):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+        token :
+            
+
+        Returns
+        -------
+
+        """
         match = self._gherkin_prefix.match(value)
         if not match:
             return [(value, token)]
@@ -297,16 +541,31 @@
 
 
 class TemplatedKeywordCall(Tokenizer):
+    """ """
     _tokens = (ARGUMENT,)
 
 
 class ForLoop(Tokenizer):
+    """ """
 
     def __init__(self):
         Tokenizer.__init__(self)
         self._in_arguments = False
 
     def _tokenize(self, value, index):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         token = self._in_arguments and ARGUMENT or SYNTAX
         if value.upper() in ('IN', 'IN RANGE'):
             self._in_arguments = True
@@ -314,6 +573,7 @@
 
 
 class _Table:
+    """ """
     _tokenizer_class = None
 
     def __init__(self, prev_tokenizer=None):
@@ -322,6 +582,19 @@
         self._prev_values_on_row = []
 
     def tokenize(self, value, index):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         if self._continues(value, index):
             self._tokenizer = self._prev_tokenizer
             yield value, SYNTAX
@@ -330,31 +603,85 @@
         self._prev_values_on_row.append(value)
 
     def _continues(self, value, index):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         return value == '...' and all(self._is_empty(t)
                                       for t in self._prev_values_on_row)
 
     def _is_empty(self, value):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+
+        Returns
+        -------
+
+        """
         return value in ('', '\\')
 
     def _tokenize(self, value, index):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         return self._tokenizer.tokenize(value)
 
     def end_row(self):
+        """ """
         self.__init__(prev_tokenizer=self._tokenizer)
 
 
 class UnknownTable(_Table):
+    """ """
     _tokenizer_class = Comment
 
     def _continues(self, value, index):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         return False
 
 
 class VariableTable(_Table):
+    """ """
     _tokenizer_class = Variable
 
 
 class SettingTable(_Table):
+    """ """
     _tokenizer_class = Setting
 
     def __init__(self, template_setter, prev_tokenizer=None):
@@ -362,30 +689,72 @@
         self._template_setter = template_setter
 
     def _tokenize(self, value, index):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         if index == 0 and normalize(value) == 'testtemplate':
             self._tokenizer = Setting(self._template_setter)
         return _Table._tokenize(self, value, index)
 
     def end_row(self):
+        """ """
         self.__init__(self._template_setter, prev_tokenizer=self._tokenizer)
 
 
 class TestCaseTable(_Table):
+    """ """
     _setting_class = TestCaseSetting
     _test_template = None
     _default_template = None
 
     @property
     def _tokenizer_class(self):
+        """ """
         if self._test_template or (self._default_template and
                                    self._test_template is not False):
             return TemplatedKeywordCall
         return KeywordCall
 
     def _continues(self, value, index):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         return index > 0 and _Table._continues(self, value, index)
 
     def _tokenize(self, value, index):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         if index == 0:
             if value:
                 self._test_template = None
@@ -403,35 +772,114 @@
         return _Table._tokenize(self, value, index)
 
     def _is_setting(self, value):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+
+        Returns
+        -------
+
+        """
         return value.startswith('[') and value.endswith(']')
 
     def _is_template(self, value):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+
+        Returns
+        -------
+
+        """
         return normalize(value) == '[template]'
 
     def _is_for_loop(self, value):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+
+        Returns
+        -------
+
+        """
         return value.startswith(':') and normalize(value, remove=':') == 'for'
 
     def set_test_template(self, template):
+        """
+
+        Parameters
+        ----------
+        template :
+            
+
+        Returns
+        -------
+
+        """
         self._test_template = self._is_template_set(template)
 
     def set_default_template(self, template):
+        """
+
+        Parameters
+        ----------
+        template :
+            
+
+        Returns
+        -------
+
+        """
         self._default_template = self._is_template_set(template)
 
     def _is_template_set(self, template):
+        """
+
+        Parameters
+        ----------
+        template :
+            
+
+        Returns
+        -------
+
+        """
         return normalize(template) not in ('', '\\', 'none', '${empty}')
 
 
 class KeywordTable(TestCaseTable):
+    """ """
     _tokenizer_class = KeywordCall
     _setting_class = KeywordSetting
 
     def _is_template(self, value):
+        """
+
+        Parameters
+        ----------
+        value :
+            
+
+        Returns
+        -------
+
+        """
         return False
 
 
 # Following code copied directly from Robot Framework 2.7.5.
 
 class VariableSplitter:
+    """ """
 
     def __init__(self, string, identifiers):
         self.identifier = None
@@ -449,11 +897,23 @@
             self._finalize()
 
     def get_replaced_base(self, variables):
+        """
+
+        Parameters
+        ----------
+        variables :
+            
+
+        Returns
+        -------
+
+        """
         if self._may_have_internal_variables:
             return variables.replace_string(self.base)
         return self.base
 
     def _finalize(self):
+        """ """
         self.identifier = self._variable_chars[0]
         self.base = ''.join(self._variable_chars[2:-1])
         self.end = self.start + len(self._variable_chars)
@@ -462,10 +922,22 @@
             self.end += len(self._list_and_dict_variable_index_chars)
 
     def _has_list_or_dict_variable_index(self):
+        """ """
         return self._list_and_dict_variable_index_chars\
         and self._list_and_dict_variable_index_chars[-1] == ']'
 
     def _split(self, string):
+        """
+
+        Parameters
+        ----------
+        string :
+            
+
+        Returns
+        -------
+
+        """
         start_index, max_index = self._find_variable(string)
         self.start = start_index
         self._open_curly = 1
@@ -484,10 +956,22 @@
                 return
 
     def _scanning_list_variable_index(self):
+        """ """
         return self._state in [self._waiting_list_variable_index_state,
                                self._list_variable_index_state]
 
     def _find_variable(self, string):
+        """
+
+        Parameters
+        ----------
+        string :
+            
+
+        Returns
+        -------
+
+        """
         max_end_index = string.rfind('}')
         if max_end_index == -1:
             raise ValueError('No variable end found')
@@ -499,6 +983,21 @@
         return start_index, max_end_index
 
     def _find_start_index(self, string, start, end):
+        """
+
+        Parameters
+        ----------
+        string :
+            
+        start :
+            
+        end :
+            
+
+        Returns
+        -------
+
+        """
         index = string.find('{', start, end) - 1
         if index < 0:
             return -1
@@ -507,10 +1006,36 @@
         return self._find_start_index(string, index+2, end)
 
     def _start_index_is_ok(self, string, index):
+        """
+
+        Parameters
+        ----------
+        string :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         return string[index] in self._identifiers\
         and not self._is_escaped(string, index)
 
     def _is_escaped(self, string, index):
+        """
+
+        Parameters
+        ----------
+        string :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         escaped = False
         while index > 0 and string[index-1] == '\\':
             index -= 1
@@ -518,6 +1043,19 @@
         return escaped
 
     def _variable_state(self, char, index):
+        """
+
+        Parameters
+        ----------
+        char :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         self._variable_chars.append(char)
         if char == '}' and not self._is_escaped(self._string, index):
             self._open_curly -= 1
@@ -529,9 +1067,23 @@
             self._state = self._internal_variable_start_state
 
     def _is_list_or_dict_variable(self):
+        """ """
         return self._variable_chars[0] in ('@','&')
 
     def _internal_variable_start_state(self, char, index):
+        """
+
+        Parameters
+        ----------
+        char :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         self._state = self._variable_state
         if char == '{':
             self._variable_chars.append(char)
@@ -541,12 +1093,38 @@
             self._variable_state(char, index)
 
     def _waiting_list_variable_index_state(self, char, index):
+        """
+
+        Parameters
+        ----------
+        char :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         if char != '[':
             raise StopIteration
         self._list_and_dict_variable_index_chars.append(char)
         self._state = self._list_variable_index_state
 
     def _list_variable_index_state(self, char, index):
+        """
+
+        Parameters
+        ----------
+        char :
+            
+        index :
+            
+
+        Returns
+        -------
+
+        """
         self._list_and_dict_variable_index_chars.append(char)
         if char == ']':
             raise StopIteration
