# Patch generated by Pyment v0.3.3

--- a/..//venv/lib/python3.8/site-packages/pandas/core/generic.py
+++ b/..//venv/lib/python3.8/site-packages/pandas/core/generic.py
@@ -121,9 +121,23 @@
 
 
 def _single_replace(self, to_replace, method, inplace, limit):
-    """
-    Replaces values in a Series using the fill method specified when no
+    """Replaces values in a Series using the fill method specified when no
     replacement value is given in the replace method
+
+    Parameters
+    ----------
+    to_replace :
+        
+    method :
+        
+    inplace :
+        
+    limit :
+        
+
+    Returns
+    -------
+
     """
     if self.ndim != 1:
         raise TypeError(
@@ -154,15 +168,16 @@
 
 
 class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):
-    """
-    N-dimensional analogue of DataFrame. Store multi-dimensional in a
+    """N-dimensional analogue of DataFrame. Store multi-dimensional in a
     size-mutable, labeled data structure
 
     Parameters
     ----------
-    data : BlockManager
-    axes : list
-    copy : bool, default False
+
+    Returns
+    -------
+
+    
     """
 
     _internal_names: List[str] = [
@@ -211,7 +226,23 @@
 
     @classmethod
     def _init_mgr(cls, mgr, axes, dtype=None, copy: bool = False) -> BlockManager:
-        """ passed a manager and a axes dict """
+        """passed a manager and a axes dict
+
+        Parameters
+        ----------
+        mgr :
+            
+        axes :
+            
+        dtype :
+             (Default value = None)
+        copy: bool :
+             (Default value = False)
+
+        Returns
+        -------
+
+        """
         for a, axe in axes.items():
             if axe is not None:
                 axe = ensure_index(axe)
@@ -231,12 +262,18 @@
 
     @property
     def attrs(self) -> Dict[Optional[Hashable], Any]:
-        """
-        Dictionary of global attributes on this object.
-
+        """Dictionary of global attributes on this object.
+        
         .. warning::
-
+        
            attrs is experimental and may change without warning.
+
+        Parameters
+        ----------
+
+        Returns
+        -------
+
         """
         if self._attrs is None:
             self._attrs = {}
@@ -244,11 +281,34 @@
 
     @attrs.setter
     def attrs(self, value: Mapping[Optional[Hashable], Any]) -> None:
+        """
+
+        Parameters
+        ----------
+        value: Mapping[Optional[Hashable] :
+            
+        Any] :
+            
+
+        Returns
+        -------
+
+        """
         self._attrs = dict(value)
 
     @classmethod
     def _validate_dtype(cls, dtype):
-        """ validate the passed dtype """
+        """validate the passed dtype
+
+        Parameters
+        ----------
+        dtype :
+            
+
+        Returns
+        -------
+
+        """
         if dtype is not None:
             dtype = pandas_dtype(dtype)
 
@@ -266,25 +326,45 @@
 
     @property
     def _constructor(self: FrameOrSeries) -> Type[FrameOrSeries]:
-        """
-        Used when a manipulation result has the same dimensions as the
+        """Used when a manipulation result has the same dimensions as the
         original.
+
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
+
+        Returns
+        -------
+
         """
         raise AbstractMethodError(self)
 
     @property
     def _constructor_sliced(self):
-        """
-        Used when a manipulation result has one lower dimension(s) as the
+        """Used when a manipulation result has one lower dimension(s) as the
         original, such as DataFrame single columns slicing.
+
+        Parameters
+        ----------
+
+        Returns
+        -------
+
         """
         raise AbstractMethodError(self)
 
     @property
     def _constructor_expanddim(self):
-        """
-        Used when a manipulation result has one higher dimension as the
+        """Used when a manipulation result has one higher dimension as the
         original, such as Series.to_frame()
+
+        Parameters
+        ----------
+
+        Returns
+        -------
+
         """
         raise NotImplementedError
 
@@ -293,6 +373,7 @@
 
     @property
     def _data(self):
+        """ """
         # GH#33054 retained because some downstream packages uses this,
         #  e.g. fastparquet
         return self._mgr
@@ -326,7 +407,21 @@
         return {0: "index"}
 
     def _construct_axes_dict(self, axes=None, **kwargs):
-        """Return an axes dictionary for myself."""
+        """
+
+        Parameters
+        ----------
+        axes :
+             (Default value = None)
+        **kwargs :
+            
+
+        Returns
+        -------
+        type
+            
+
+        """
         d = {a: self._get_axis(a) for a in (axes or self._AXIS_ORDERS)}
         d.update(kwargs)
         return d
@@ -335,15 +430,28 @@
     def _construct_axes_from_arguments(
         cls, args, kwargs, require_all: bool = False, sentinel=None
     ):
-        """
-        Construct and returns axes if supplied in args/kwargs.
-
+        """Construct and returns axes if supplied in args/kwargs.
+        
         If require_all, raise if all axis arguments are not supplied
-        return a tuple of (axes, kwargs).
-
-        sentinel specifies the default parameter when an axis is not
-        supplied; useful to distinguish when a user explicitly passes None
-        in scenarios where None has special meaning.
+
+        Parameters
+        ----------
+        args :
+            
+        kwargs :
+            
+        require_all: bool :
+             (Default value = False)
+        sentinel :
+             (Default value = None)
+
+        Returns
+        -------
+        type
+            sentinel specifies the default parameter when an axis is not
+            supplied; useful to distinguish when a user explicitly passes None
+            in scenarios where None has special meaning.
+
         """
         # construct the args
         args = list(args)
@@ -364,6 +472,17 @@
 
     @classmethod
     def _get_axis_number(cls, axis: Axis) -> int:
+        """
+
+        Parameters
+        ----------
+        axis: Axis :
+            
+
+        Returns
+        -------
+
+        """
         try:
             return cls._AXIS_TO_AXIS_NUMBER[axis]
         except KeyError:
@@ -371,17 +490,49 @@
 
     @classmethod
     def _get_axis_name(cls, axis: Axis) -> str:
+        """
+
+        Parameters
+        ----------
+        axis: Axis :
+            
+
+        Returns
+        -------
+
+        """
         axis_number = cls._get_axis_number(axis)
         return cls._AXIS_ORDERS[axis_number]
 
     def _get_axis(self, axis: Axis) -> Index:
+        """
+
+        Parameters
+        ----------
+        axis: Axis :
+            
+
+        Returns
+        -------
+
+        """
         axis_number = self._get_axis_number(axis)
         assert axis_number in {0, 1}
         return self.index if axis_number == 0 else self.columns
 
     @classmethod
     def _get_block_manager_axis(cls, axis: Axis) -> int:
-        """Map the axis to the block_manager axis."""
+        """Map the axis to the block_manager axis.
+
+        Parameters
+        ----------
+        axis: Axis :
+            
+
+        Returns
+        -------
+
+        """
         axis = cls._get_axis_number(axis)
         if cls._AXIS_REVERSED:
             m = cls._AXIS_LEN - 1
@@ -389,6 +540,17 @@
         return axis
 
     def _get_axis_resolvers(self, axis: str) -> Dict[str, ABCSeries]:
+        """
+
+        Parameters
+        ----------
+        axis: str :
+            
+
+        Returns
+        -------
+
+        """
         # index or columns
         axis_index = getattr(self, axis)
         d = dict()
@@ -419,6 +581,7 @@
         return d
 
     def _get_index_resolvers(self) -> Dict[str, ABCSeries]:
+        """ """
         from pandas.core.computation.parsing import clean_column_name
 
         d: Dict[str, ABCSeries] = {}
@@ -429,11 +592,17 @@
 
     def _get_cleaned_column_resolvers(self) -> Dict[str, ABCSeries]:
         """
-        Return the special character free column resolvers of a dataframe.
-
-        Column names with special characters are 'cleaned up' so that they can
-        be referred to by backtick quoting.
-        Used in :meth:`DataFrame.eval`.
+
+        Parameters
+        ----------
+
+        Returns
+        -------
+        type
+            Column names with special characters are 'cleaned up' so that they can
+            be referred to by backtick quoting.
+            Used in :meth:`DataFrame.eval`.
+
         """
         from pandas.core.computation.parsing import clean_column_name
 
@@ -446,45 +615,47 @@
 
     @property
     def _info_axis(self) -> Index:
+        """ """
         return getattr(self, self._info_axis_name)
 
     @property
     def _stat_axis(self) -> Index:
+        """ """
         return getattr(self, self._stat_axis_name)
 
     @property
     def shape(self) -> Tuple[int, ...]:
-        """
-        Return a tuple of axis dimensions
-        """
+        """ """
         return tuple(len(self._get_axis(a)) for a in self._AXIS_ORDERS)
 
     @property
     def axes(self) -> List[Index]:
-        """
-        Return index label(s) of the internal NDFrame
-        """
+        """ """
         # we do it this way because if we have reversed axes, then
         # the block manager shows then reversed
         return [self._get_axis(a) for a in self._AXIS_ORDERS]
 
     @property
     def ndim(self) -> int:
-        """
-        Return an int representing the number of axes / array dimensions.
-
+        """Return an int representing the number of axes / array dimensions.
+        
         Return 1 if Series. Otherwise return 2 if DataFrame.
+
+        Parameters
+        ----------
+
+        Returns
+        -------
 
         See Also
         --------
         ndarray.ndim : Number of array dimensions.
-
         Examples
         --------
         >>> s = pd.Series({'a': 1, 'b': 2, 'c': 3})
         >>> s.ndim
         1
-
+        
         >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
         >>> df.ndim
         2
@@ -493,22 +664,26 @@
 
     @property
     def size(self) -> int:
-        """
-        Return an int representing the number of elements in this object.
-
+        """Return an int representing the number of elements in this object.
+        
         Return the number of rows if Series. Otherwise return the number of
         rows times number of columns if DataFrame.
+
+        Parameters
+        ----------
+
+        Returns
+        -------
 
         See Also
         --------
         ndarray.size : Number of elements in the array.
-
         Examples
         --------
         >>> s = pd.Series({'a': 1, 'b': 2, 'c': 3})
         >>> s.size
         3
-
+        
         >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
         >>> df.size
         4
@@ -517,18 +692,37 @@
 
     @property
     def _selected_obj(self: FrameOrSeries) -> FrameOrSeries:
-        """ internal compat with SelectionMixin """
+        """internal compat with SelectionMixin
+
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
+
+        Returns
+        -------
+
+        """
         return self
 
     @property
     def _obj_with_exclusions(self: FrameOrSeries) -> FrameOrSeries:
-        """ internal compat with SelectionMixin """
+        """internal compat with SelectionMixin
+
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
+
+        Returns
+        -------
+
+        """
         return self
 
     def set_axis(self, labels, axis: Axis = 0, inplace: bool = False):
-        """
-        Assign desired index to given axis.
-
+        """Assign desired index to given axis.
+        
         Indexes for%(extended_summary_sub)s row labels can be changed by assigning
         a list-like or Index.
 
@@ -536,12 +730,14 @@
         ----------
         labels : list-like, Index
             The values for the new index.
-
         axis : %(axes_single_arg)s, default 0
             The axis to update. The value 0 identifies the rows%(axis_description_sub)s.
-
         inplace : bool, default False
             Whether to return a new %(klass)s instance.
+        axis: Axis :
+             (Default value = 0)
+        inplace: bool :
+             (Default value = False)
 
         Returns
         -------
@@ -560,17 +756,41 @@
             return obj
 
     def _set_axis(self, axis: int, labels: Index) -> None:
+        """
+
+        Parameters
+        ----------
+        axis: int :
+            
+        labels: Index :
+            
+
+        Returns
+        -------
+
+        """
         labels = ensure_index(labels)
         self._mgr.set_axis(axis, labels)
         self._clear_item_cache()
 
     def swapaxes(self: FrameOrSeries, axis1, axis2, copy=True) -> FrameOrSeries:
-        """
-        Interchange axes and swap values axes appropriately.
-
-        Returns
-        -------
-        y : same as input
+        """Interchange axes and swap values axes appropriately.
+
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
+        axis1 :
+            
+        axis2 :
+            
+        copy :
+             (Default value = True)
+
+        Returns
+        -------
+
+        
         """
         i = self._get_axis_number(axis1)
         j = self._get_axis_number(axis2)
@@ -594,9 +814,8 @@
         )
 
     def droplevel(self: FrameOrSeries, level, axis=0) -> FrameOrSeries:
-        """
-        Return DataFrame with requested index / column level(s) removed.
-
+        """Return DataFrame with requested index / column level(s) removed.
+        
         .. versionadded:: 0.24.0
 
         Parameters
@@ -605,12 +824,12 @@
             If a string is given, must be the name of a level
             If list-like, elements must be names or positional indexes
             of levels.
-
         axis : {0 or 'index', 1 or 'columns'}, default 0
             Axis along which the level(s) is removed:
-
             * 0 or 'index': remove level(s) in column.
-            * 1 or 'columns': remove level(s) in row.
+            * 1 or 'columns': remove level(s) in row. (Default value = 0)
+        self: FrameOrSeries :
+            
 
         Returns
         -------
@@ -624,11 +843,11 @@
         ...     [5, 6, 7, 8],
         ...     [9, 10, 11, 12]
         ... ]).set_index([0, 1]).rename_axis(['a', 'b'])
-
+        
         >>> df.columns = pd.MultiIndex.from_tuples([
         ...     ('c', 'e'), ('d', 'f')
         ... ], names=['level_1', 'level_2'])
-
+        
         >>> df
         level_1   c   d
         level_2   e   f
@@ -636,7 +855,7 @@
         1 2      3   4
         5 6      7   8
         9 10    11  12
-
+        
         >>> df.droplevel('a')
         level_1   c   d
         level_2   e   f
@@ -644,7 +863,7 @@
         2        3   4
         6        7   8
         10      11  12
-
+        
         >>> df.droplevel('level_2', axis=1)
         level_1   c   d
         a b
@@ -658,6 +877,17 @@
         return result
 
     def pop(self, item: Label) -> Union["Series", Any]:
+        """
+
+        Parameters
+        ----------
+        item: Label :
+            
+
+        Returns
+        -------
+
+        """
         result = self[item]
         del self[item]
         if self.ndim == 2:
@@ -666,13 +896,12 @@
         return result
 
     def squeeze(self, axis=None):
-        """
-        Squeeze 1 dimensional axis objects into scalars.
-
+        """Squeeze 1 dimensional axis objects into scalars.
+        
         Series or DataFrames with a single element are squeezed to a scalar.
         DataFrames with a single column or a single row are squeezed to a
         Series. Otherwise the object is unchanged.
-
+        
         This method is most useful when you don't know if your
         object is a Series or DataFrame, but you do know it has just a single
         column. In that case you can safely call `squeeze` to ensure you have a
@@ -695,76 +924,84 @@
         DataFrame.iloc : Integer-location based indexing for selecting Series.
         Series.to_frame : Inverse of DataFrame.squeeze for a
             single-column DataFrame.
-
         Examples
         --------
+        
+        Slicing might produce a Series with a single value:
+        
+        
+        
+        Squeezing objects with more than one value in every axis does nothing:
+        
+        
+        
+        Squeezing is even more effective when used with DataFrames.
+        
+        
+        Slicing a single column will produce a DataFrame with the columns
+        having only one value:
+        
+        
+        So the columns can be squeezed down, resulting in a Series:
+        
+        
+        Slicing a single row from a single column will produce a single
+        scalar DataFrame:
+        
+        
+        Squeezing the rows produces a single scalar Series:
+        
+        
+        Squeezing all axes will project directly into a scalar:
         >>> primes = pd.Series([2, 3, 5, 7])
-
-        Slicing might produce a Series with a single value:
-
+        
         >>> even_primes = primes[primes % 2 == 0]
         >>> even_primes
         0    2
         dtype: int64
-
+        
         >>> even_primes.squeeze()
         2
-
-        Squeezing objects with more than one value in every axis does nothing:
-
+        
         >>> odd_primes = primes[primes % 2 == 1]
         >>> odd_primes
         1    3
         2    5
         3    7
         dtype: int64
-
+        
         >>> odd_primes.squeeze()
         1    3
         2    5
         3    7
         dtype: int64
-
-        Squeezing is even more effective when used with DataFrames.
-
+        
         >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['a', 'b'])
         >>> df
            a  b
         0  1  2
         1  3  4
-
-        Slicing a single column will produce a DataFrame with the columns
-        having only one value:
-
+        
         >>> df_a = df[['a']]
         >>> df_a
            a
         0  1
         1  3
-
-        So the columns can be squeezed down, resulting in a Series:
-
+        
         >>> df_a.squeeze('columns')
         0    1
         1    3
         Name: a, dtype: int64
-
-        Slicing a single row from a single column will produce a single
-        scalar DataFrame:
-
+        
         >>> df_0a = df.loc[df.index < 1, ['a']]
         >>> df_0a
            a
         0  1
-
-        Squeezing the rows produces a single scalar Series:
-
+        
         >>> df_0a.squeeze('rows')
         a    1
         Name: 0, dtype: int64
-
-        Squeezing all axes will project directly into a scalar:
-
+        
         >>> df_0a.squeeze()
         1
         """
@@ -791,8 +1028,7 @@
         level: Optional[Level] = None,
         errors: str = "ignore",
     ) -> Optional[FrameOrSeries]:
-        """
-        Alter axes input function or functions. Function / dict values must be
+        """Alter axes input function or functions. Function / dict values must be
         unique (1-to-1). Labels not contained in a dict / Series will be left
         as-is. Extra labels listed don't throw an error. Alternatively, change
         ``Series.name`` with a scalar value (Series only).
@@ -818,10 +1054,31 @@
             being transformed.
             If 'ignore', existing keys will be renamed and extra keys will be
             ignored.
+        self: FrameOrSeries :
+            
+        mapper: Optional[Renamer] :
+             (Default value = None)
+        * :
+            
+        index: Optional[Renamer] :
+             (Default value = None)
+        columns: Optional[Renamer] :
+             (Default value = None)
+        axis: Optional[Axis] :
+             (Default value = None)
+        copy: bool :
+             (Default value = True)
+        inplace: bool :
+             (Default value = False)
+        level: Optional[Level] :
+             (Default value = None)
+        errors: str :
+             (Default value = "ignore")
 
         Returns
         -------
         renamed : {klass} (new object)
+            
 
         Raises
         ------
@@ -832,9 +1089,28 @@
         See Also
         --------
         NDFrame.rename_axis
-
         Examples
         --------
+        
+        Since ``DataFrame`` doesn't have a ``.name`` attribute,
+        only mapping-type arguments are allowed.
+        
+        
+        ``DataFrame.rename`` supports two calling conventions
+        
+        * ``(index=index_mapper, columns=columns_mapper, ...)``
+        * ``(mapper, axis={'index', 'columns'}, ...)``
+        
+        We *highly* recommend using keyword arguments to clarify your
+        intent.
+        
+        
+        
+        Using axis-style parameters
+        
+        
+        
+        See the :ref:`user guide <basics.rename>` for more.
         >>> s = pd.Series([1, 2, 3])
         >>> s
         0    1
@@ -856,51 +1132,36 @@
         3    2
         5    3
         dtype: int64
-
-        Since ``DataFrame`` doesn't have a ``.name`` attribute,
-        only mapping-type arguments are allowed.
-
+        
         >>> df = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})
         >>> df.rename(2)
         Traceback (most recent call last):
         ...
         TypeError: 'int' object is not callable
-
-        ``DataFrame.rename`` supports two calling conventions
-
-        * ``(index=index_mapper, columns=columns_mapper, ...)``
-        * ``(mapper, axis={'index', 'columns'}, ...)``
-
-        We *highly* recommend using keyword arguments to clarify your
-        intent.
-
+        
         >>> df.rename(index=str, columns={"A": "a", "B": "c"})
            a  c
         0  1  4
         1  2  5
         2  3  6
-
+        
         >>> df.rename(index=str, columns={"A": "a", "C": "c"})
            a  B
         0  1  4
         1  2  5
         2  3  6
-
-        Using axis-style parameters
-
+        
         >>> df.rename(str.lower, axis='columns')
            a  b
         0  1  4
         1  2  5
         2  3  6
-
+        
         >>> df.rename({1: 2, 2: 4}, axis='index')
            A  B
         0  1  4
         2  2  5
         4  3  6
-
-        See the :ref:`user guide <basics.rename>` for more.
         """
         if mapper is None and index is None and columns is None:
             raise TypeError("must pass an index to rename")
@@ -956,26 +1217,22 @@
 
     @rewrite_axis_style_signature("mapper", [("copy", True), ("inplace", False)])
     def rename_axis(self, mapper=lib.no_default, **kwargs):
-        """
-        Set the name of the axis for the index or columns.
+        """Set the name of the axis for the index or columns.
 
         Parameters
         ----------
         mapper : scalar, list-like, optional
-            Value to set the axis name attribute.
+            Value to set the axis name attribute. (Default value = lib.no_default)
         index, columns : scalar, list-like, dict-like or function, optional
             A scalar, list-like, dict-like or functions transformations to
             apply to that axis' values.
             Note that the ``columns`` parameter is not allowed if the
             object is a Series. This parameter only apply for DataFrame
             type objects.
-
             Use either ``mapper`` and ``axis`` to
             specify the axis to target with ``mapper``, or ``index``
             and/or ``columns``.
-
             .. versionchanged:: 0.24.0
-
         axis : {0 or 'index', 1 or 'columns'}, default 0
             The axis to rename.
         copy : bool, default True
@@ -983,6 +1240,8 @@
         inplace : bool, default False
             Modifies the object directly, instead of creating a new Series
             or DataFrame.
+        **kwargs :
+            
 
         Returns
         -------
@@ -994,30 +1253,33 @@
         Series.rename : Alter Series index labels or name.
         DataFrame.rename : Alter DataFrame index labels or name.
         Index.rename : Set new names on index.
-
         Notes
         -----
         ``DataFrame.rename_axis`` supports two calling conventions
-
+        
         * ``(index=index_mapper, columns=columns_mapper, ...)``
         * ``(mapper, axis={'index', 'columns'}, ...)``
-
+        
         The first calling convention will only modify the names of
         the index and/or the names of the Index object that is the columns.
         In this case, the parameter ``copy`` is ignored.
-
+        
         The second calling convention will modify the names of the
         the corresponding index if mapper is a list or a scalar.
         However, if mapper is dict-like or a function, it will use the
         deprecated behavior of modifying the axis *labels*.
-
+        
         We *highly* recommend using keyword arguments to clarify your
         intent.
-
         Examples
         --------
         **Series**
-
+        
+        
+        **DataFrame**
+        
+        
+        **MultiIndex**
         >>> s = pd.Series(["dog", "cat", "monkey"])
         >>> s
         0       dog
@@ -1030,9 +1292,7 @@
         1    cat
         2    monkey
         dtype: object
-
-        **DataFrame**
-
+        
         >>> df = pd.DataFrame({"num_legs": [4, 4, 2],
         ...                    "num_arms": [0, 0, 2]},
         ...                   ["dog", "cat", "monkey"])
@@ -1055,9 +1315,7 @@
         dog            4         0
         cat            4         0
         monkey         2         2
-
-        **MultiIndex**
-
+        
         >>> df.index = pd.MultiIndex.from_product([['mammal'],
         ...                                        ['dog', 'cat', 'monkey']],
         ...                                       names=['type', 'name'])
@@ -1067,14 +1325,14 @@
         mammal dog            4         0
                cat            4         0
                monkey         2         2
-
+        
         >>> df.rename_axis(index={'type': 'class'})
         limbs          num_legs  num_arms
         class  name
         mammal dog            4         0
                cat            4         0
                monkey         2         2
-
+        
         >>> df.rename_axis(columns=str.upper)
         LIMBS          num_legs  num_arms
         type   name
@@ -1129,8 +1387,7 @@
                 return result
 
     def _set_axis_name(self, name, axis=0, inplace=False):
-        """
-        Set the name(s) of the axis.
+        """Set the name(s) of the axis.
 
         Parameters
         ----------
@@ -1138,9 +1395,9 @@
             Name(s) to set.
         axis : {0 or 'index', 1 or 'columns'}, default 0
             The axis to set the label. The value 0 or 'index' specifies index,
-            and the value 1 or 'columns' specifies columns.
+            and the value 1 or 'columns' specifies columns. (Default value = 0)
         inplace : bool, default False
-            If `True`, do operation inplace and return None.
+            If `True`, do operation inplace and return None. (Default value = False)
 
         Returns
         -------
@@ -1153,7 +1410,6 @@
         Series.rename : Alter the index labels or set the index name
             of :class:`Series`.
         Index.rename : Set the name of :class:`Index` or :class:`MultiIndex`.
-
         Examples
         --------
         >>> df = pd.DataFrame({"num_legs": [4, 4, 2]},
@@ -1191,14 +1447,24 @@
     # Comparison Methods
 
     def _indexed_same(self, other) -> bool:
+        """
+
+        Parameters
+        ----------
+        other :
+            
+
+        Returns
+        -------
+
+        """
         return all(
             self._get_axis(a).equals(other._get_axis(a)) for a in self._AXIS_ORDERS
         )
 
     def equals(self, other):
-        """
-        Test whether two objects contain the same elements.
-
+        """Test whether two objects contain the same elements.
+        
         This function allows two Series or DataFrames to be compared against
         each other to see if they have the same shape and elements. NaNs in
         the same location are considered equal. The column headers do not
@@ -1231,46 +1497,46 @@
             DataFrames.
         numpy.array_equal : Return True if two arrays have the same shape
             and elements, False otherwise.
-
         Notes
         -----
         This function requires that the elements have the same dtype as their
         respective elements in the other Series or DataFrame. However, the
         column labels do not need to have the same type, as long as they are
         still considered equal.
-
         Examples
         --------
+        
+        DataFrames df and exactly_equal have the same types and values for
+        their elements and column labels, which will return True.
+        
+        
+        DataFrames df and different_column_type have the same element
+        types and values, but have different types for the column labels,
+        which will still return True.
+        
+        
+        DataFrames df and different_data_type have different types for the
+        same values for their elements, and will return False even though
+        their column labels are the same values and types.
         >>> df = pd.DataFrame({1: [10], 2: [20]})
         >>> df
             1   2
         0  10  20
-
-        DataFrames df and exactly_equal have the same types and values for
-        their elements and column labels, which will return True.
-
+        
         >>> exactly_equal = pd.DataFrame({1: [10], 2: [20]})
         >>> exactly_equal
             1   2
         0  10  20
         >>> df.equals(exactly_equal)
         True
-
-        DataFrames df and different_column_type have the same element
-        types and values, but have different types for the column labels,
-        which will still return True.
-
+        
         >>> different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]})
         >>> different_column_type
            1.0  2.0
         0   10   20
         >>> df.equals(different_column_type)
         True
-
-        DataFrames df and different_data_type have different types for the
-        same values for their elements, and will return False even though
-        their column labels are the same values and types.
-
+        
         >>> different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]})
         >>> different_data_type
               1     2
@@ -1334,12 +1600,14 @@
     __bool__ = __nonzero__
 
     def bool(self):
-        """
-        Return the bool of a single element Series or DataFrame.
-
+        """Return the bool of a single element Series or DataFrame.
+        
         This must be a boolean scalar value, either True or False. It will raise a
         ValueError if the Series or DataFrame does not have exactly 1 element, or that
         element is not boolean (integer values 0 and 1 will also raise an exception).
+
+        Parameters
+        ----------
 
         Returns
         -------
@@ -1351,16 +1619,14 @@
         Series.astype : Change the data type of a Series, including to boolean.
         DataFrame.astype : Change the data type of a DataFrame, including to boolean.
         numpy.bool_ : NumPy boolean data type, used by pandas for boolean values.
-
         Examples
         --------
         The method will only work for single element objects with a boolean value:
-
         >>> pd.Series([True]).bool()
         True
         >>> pd.Series([False]).bool()
         False
-
+        
         >>> pd.DataFrame({'col': [True]}).bool()
         True
         >>> pd.DataFrame({'col': [False]}).bool()
@@ -1392,9 +1658,8 @@
     # have consistent precedence and validation logic throughout the library.
 
     def _is_level_reference(self, key, axis=0):
-        """
-        Test whether a key is a level reference for a given axis.
-
+        """Test whether a key is a level reference for a given axis.
+        
         To be considered a level reference, `key` must be a string that:
           - (axis=0): Matches the name of an index level and does NOT match
             a column label.
@@ -1406,11 +1671,12 @@
         key : str
             Potential level name for the given axis
         axis : int, default 0
-            Axis that levels are associated with (0 for index, 1 for columns)
-
-        Returns
-        -------
-        is_level : bool
+            Axis that levels are associated with (0 for index, 1 for columns) (Default value = 0)
+
+        Returns
+        -------
+
+        
         """
         axis = self._get_axis_number(axis)
 
@@ -1422,24 +1688,24 @@
         )
 
     def _is_label_reference(self, key, axis=0) -> bool_t:
-        """
-        Test whether a key is a label reference for a given axis.
-
+        """Test whether a key is a label reference for a given axis.
+        
         To be considered a label reference, `key` must be a string that:
           - (axis=0): Matches a column label
           - (axis=1): Matches an index label
 
         Parameters
         ----------
-        key: str
+        key : str
             Potential label name
-        axis: int, default 0
+        axis : int, default 0
             Axis perpendicular to the axis that labels are associated with
-            (0 means search for column labels, 1 means search for index labels)
-
-        Returns
-        -------
-        is_label: bool
+            (0 means search for column labels, 1 means search for index labels) (Default value = 0)
+
+        Returns
+        -------
+
+        
         """
         axis = self._get_axis_number(axis)
         other_axes = (ax for ax in range(self._AXIS_LEN) if ax != axis)
@@ -1451,9 +1717,8 @@
         )
 
     def _is_label_or_level_reference(self, key: str, axis: int = 0) -> bool_t:
-        """
-        Test whether a key is a label or level reference for a given axis.
-
+        """Test whether a key is a label or level reference for a given axis.
+        
         To be considered either a label or a level reference, `key` must be a
         string that:
           - (axis=0): Matches a column label or an index level
@@ -1461,36 +1726,43 @@
 
         Parameters
         ----------
-        key: str
+        key : str
             Potential label or level name
-        axis: int, default 0
+        axis : int, default 0
             Axis that levels are associated with (0 for index, 1 for columns)
-
-        Returns
-        -------
-        is_label_or_level: bool
+        key: str :
+            
+        axis: int :
+             (Default value = 0)
+
+        Returns
+        -------
+
+        
         """
         return self._is_level_reference(key, axis=axis) or self._is_label_reference(
             key, axis=axis
         )
 
     def _check_label_or_level_ambiguity(self, key, axis: int = 0) -> None:
-        """
-        Check whether `key` is ambiguous.
-
+        """Check whether `key` is ambiguous.
+        
         By ambiguous, we mean that it matches both a level of the input
         `axis` and a label of the other axis.
 
         Parameters
         ----------
-        key: str or object
+        key : str or object
             Label or level name.
-        axis: int, default 0
+        axis : int, default 0
             Axis that levels are associated with (0 for index, 1 for columns).
-
-        Raises
-        ------
-        ValueError: `key` is ambiguous
+        axis: int :
+             (Default value = 0)
+
+        Returns
+        -------
+
+        
         """
         axis = self._get_axis_number(axis)
         other_axes = (ax for ax in range(self._AXIS_LEN) if ax != axis)
@@ -1518,10 +1790,9 @@
             raise ValueError(msg)
 
     def _get_label_or_level_values(self, key: str, axis: int = 0) -> np.ndarray:
-        """
-        Return a 1-D array of values associated with `key`, a label or level
+        """Return a 1-D array of values associated with `key`, a label or level
         from the given `axis`.
-
+        
         Retrieval logic:
           - (axis=0): Return column values if `key` matches a column label.
             Otherwise return index level values if `key` matches an index
@@ -1532,24 +1803,21 @@
 
         Parameters
         ----------
-        key: str
+        key : str
             Label or level name.
-        axis: int, default 0
+        axis : int, default 0
             Axis that levels are associated with (0 for index, 1 for columns)
-
-        Returns
-        -------
-        values: np.ndarray
-
-        Raises
-        ------
-        KeyError
-            if `key` matches neither a label nor a level
-        ValueError
-            if `key` matches multiple labels
-        FutureWarning
-            if `key` is ambiguous. This will become an ambiguity error in a
-            future version
+        key: str :
+            
+        axis: int :
+             (Default value = 0)
+
+        Returns
+        -------
+        values : np.ndarray
+            
+
+        
         """
         axis = self._get_axis_number(axis)
         other_axes = [ax for ax in range(self._AXIS_LEN) if ax != axis]
@@ -1585,9 +1853,8 @@
         return values
 
     def _drop_labels_or_levels(self, keys, axis: int = 0):
-        """
-        Drop labels and/or levels for the given `axis`.
-
+        """Drop labels and/or levels for the given `axis`.
+        
         For each key in `keys`:
           - (axis=0): If key matches a column label then drop the column.
             Otherwise if key matches an index level then drop the level.
@@ -1596,19 +1863,19 @@
 
         Parameters
         ----------
-        keys: str or list of str
+        keys : str or list of str
             labels or levels to drop
-        axis: int, default 0
+        axis : int, default 0
             Axis that levels are associated with (0 for index, 1 for columns)
-
-        Returns
-        -------
-        dropped: DataFrame
-
-        Raises
-        ------
-        ValueError
-            if any `keys` match neither a label nor a level
+        axis: int :
+             (Default value = 0)
+
+        Returns
+        -------
+        dropped : DataFrame
+            
+
+        
         """
         axis = self._get_axis_number(axis)
 
@@ -1683,33 +1950,39 @@
 
     # can we get a better explanation of this?
     def keys(self):
-        """
-        Get the 'info axis' (see Indexing for more).
-
+        """Get the 'info axis' (see Indexing for more).
+        
         This is index for Series, columns for DataFrame.
 
-        Returns
-        -------
-        Index
-            Info axis.
+        Parameters
+        ----------
+
+        Returns
+        -------
+
+        
         """
         return self._info_axis
 
     def items(self):
-        """
-        Iterate over (label, values) on info axis
-
+        """Iterate over (label, values) on info axis
+        
         This is index for Series and columns for DataFrame.
 
-        Returns
-        -------
-        Generator
+        Parameters
+        ----------
+
+        Returns
+        -------
+
+        
         """
         for h in self._info_axis:
             yield h, self[h]
 
     @doc(items)
     def iteritems(self):
+        """ """
         return self.items()
 
     def __len__(self) -> int:
@@ -1722,11 +1995,13 @@
 
     @property
     def empty(self) -> bool_t:
-        """
-        Indicator whether DataFrame is empty.
-
+        """Indicator whether DataFrame is empty.
+        
         True if DataFrame is entirely empty (no items), meaning any of the
         axes are of length 0.
+
+        Parameters
+        ----------
 
         Returns
         -------
@@ -1738,16 +2013,17 @@
         Series.dropna : Return series without null values.
         DataFrame.dropna : Return DataFrame with labels on given axis omitted
             where (all or any) data are missing.
-
         Notes
         -----
         If DataFrame contains only NaNs, it is still not considered empty. See
         the example below.
-
         Examples
         --------
         An example of an actual empty DataFrame. Notice the index is empty:
-
+        
+        
+        If we only have NaNs in our DataFrame, it is not considered empty! We
+        will need to drop the NaNs to make the DataFrame empty:
         >>> df_empty = pd.DataFrame({'A' : []})
         >>> df_empty
         Empty DataFrame
@@ -1755,10 +2031,7 @@
         Index: []
         >>> df_empty.empty
         True
-
-        If we only have NaNs in our DataFrame, it is not considered empty! We
-        will need to drop the NaNs to make the DataFrame empty:
-
+        
         >>> df = pd.DataFrame({'A' : [np.nan]})
         >>> df
             A
@@ -1856,9 +2129,15 @@
         return f"{type(self).__name__}({prepr})"
 
     def _repr_latex_(self):
-        """
-        Returns a LaTeX representation for a particular object.
+        """Returns a LaTeX representation for a particular object.
         Mainly for use with nbconvert (jupyter notebook conversion to pdf).
+
+        Parameters
+        ----------
+
+        Returns
+        -------
+
         """
         if config.get_option("display.latex.repr"):
             return self.to_latex()
@@ -1866,9 +2145,15 @@
             return None
 
     def _repr_data_resource_(self):
-        """
-        Not a real Jupyter special repr method, but we use the same
+        """Not a real Jupyter special repr method, but we use the same
         naming convention.
+
+        Parameters
+        ----------
+
+        Returns
+        -------
+
         """
         if config.get_option("display.html.table_schema"):
             data = self.head(config.get_option("display.max_rows"))
@@ -1900,14 +2185,13 @@
         verbose=True,
         freeze_panes=None,
     ) -> None:
-        """
-        Write {klass} to an Excel sheet.
-
+        """Write {klass} to an Excel sheet.
+        
         To write a single {klass} to an Excel .xlsx file it is only necessary to
         specify a target file name. To write to multiple sheets it is necessary to
         create an `ExcelWriter` object with a target file name, and specify a sheet
         in the file to write to.
-
+        
         Multiple sheets may be written to by specifying unique `sheet_name`.
         With all data written to the file it is necessary to save the changes.
         Note that creating an `ExcelWriter` object with a file name that already
@@ -1918,44 +2202,47 @@
         excel_writer : str or ExcelWriter object
             File path or existing ExcelWriter.
         sheet_name : str, default 'Sheet1'
-            Name of sheet which will contain DataFrame.
+            Name of sheet which will contain DataFrame. (Default value = "Sheet1")
         na_rep : str, default ''
-            Missing data representation.
+            Missing data representation. (Default value = "")
         float_format : str, optional
             Format string for floating point numbers. For example
-            ``float_format="%.2f"`` will format 0.1234 to 0.12.
+            ``float_format="%.2f"`` will format 0.1234 to 0.12. (Default value = None)
         columns : sequence or list of str, optional
-            Columns to write.
+            Columns to write. (Default value = None)
         header : bool or list of str, default True
             Write out the column names. If a list of string is given it is
-            assumed to be aliases for the column names.
+            assumed to be aliases for the column names. (Default value = True)
         index : bool, default True
-            Write row names (index).
+            Write row names (index). (Default value = True)
         index_label : str or sequence, optional
             Column label for index column(s) if desired. If not specified, and
             `header` and `index` are True, then the index names are used. A
-            sequence should be given if the DataFrame uses MultiIndex.
+            sequence should be given if the DataFrame uses MultiIndex. (Default value = None)
         startrow : int, default 0
-            Upper left cell row to dump data frame.
+            Upper left cell row to dump data frame. (Default value = 0)
         startcol : int, default 0
-            Upper left cell column to dump data frame.
+            Upper left cell column to dump data frame. (Default value = 0)
         engine : str, optional
             Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set this
             via the options ``io.excel.xlsx.writer``, ``io.excel.xls.writer``, and
-            ``io.excel.xlsm.writer``.
+            ``io.excel.xlsm.writer``. (Default value = None)
         merge_cells : bool, default True
-            Write MultiIndex and Hierarchical Rows as merged cells.
+            Write MultiIndex and Hierarchical Rows as merged cells. (Default value = True)
         encoding : str, optional
             Encoding of the resulting excel file. Only necessary for xlwt,
-            other writers support unicode natively.
+            other writers support unicode natively. (Default value = None)
         inf_rep : str, default 'inf'
             Representation for infinity (there is no native representation for
-            infinity in Excel).
+            infinity in Excel). (Default value = "inf")
         verbose : bool, default True
-            Display more information in the error logs.
+            Display more information in the error logs. (Default value = True)
         freeze_panes : tuple of int (length 2), optional
             Specifies the one-based bottommost row and rightmost column that
-            is to be frozen.
+            is to be frozen. (Default value = None)
+
+        Returns
+        -------
 
         See Also
         --------
@@ -1963,48 +2250,49 @@
         ExcelWriter : Class for writing DataFrame objects into excel sheets.
         read_excel : Read an Excel file into a pandas DataFrame.
         read_csv : Read a comma-separated values (csv) file into DataFrame.
-
         Notes
         -----
         For compatibility with :meth:`~DataFrame.to_csv`,
         to_excel serializes lists and dicts to strings before writing.
-
+        
         Once a workbook has been saved it is not possible write further data
         without rewriting the whole workbook.
-
         Examples
         --------
-
+        
         Create, write to and save a workbook:
-
+        
+        
+        To specify the sheet name:
+        
+        
+        If you wish to write to more than one sheet in the workbook, it is
+        necessary to specify an ExcelWriter object:
+        
+        
+        ExcelWriter can also be used to append to an existing Excel file:
+        
+        
+        To set the library that is used to write the Excel file,
+        you can pass the `engine` keyword (the default engine is
+        automatically chosen depending on the file extension):
         >>> df1 = pd.DataFrame([['a', 'b'], ['c', 'd']],
         ...                    index=['row 1', 'row 2'],
         ...                    columns=['col 1', 'col 2'])
         >>> df1.to_excel("output.xlsx")  # doctest: +SKIP
-
-        To specify the sheet name:
-
+        
         >>> df1.to_excel("output.xlsx",
         ...              sheet_name='Sheet_name_1')  # doctest: +SKIP
-
-        If you wish to write to more than one sheet in the workbook, it is
-        necessary to specify an ExcelWriter object:
-
+        
         >>> df2 = df1.copy()
         >>> with pd.ExcelWriter('output.xlsx') as writer:  # doctest: +SKIP
         ...     df1.to_excel(writer, sheet_name='Sheet_name_1')
         ...     df2.to_excel(writer, sheet_name='Sheet_name_2')
-
-        ExcelWriter can also be used to append to an existing Excel file:
-
+        
         >>> with pd.ExcelWriter('output.xlsx',
         ...                     mode='a') as writer:  # doctest: +SKIP
         ...     df.to_excel(writer, sheet_name='Sheet_name_3')
-
-        To set the library that is used to write the Excel file,
-        you can pass the `engine` keyword (the default engine is
-        automatically chosen depending on the file extension):
-
+        
         >>> df1.to_excel('output1.xlsx', engine='xlsxwriter')  # doctest: +SKIP
         """
 
@@ -2046,9 +2334,8 @@
         index: bool_t = True,
         indent: Optional[int] = None,
     ) -> Optional[str]:
-        """
-        Convert the object to a JSON string.
-
+        """Convert the object to a JSON string.
+        
         Note NaN's and None will be converted to null and datetime objects
         will be converted to UNIX timestamps.
 
@@ -2059,32 +2346,23 @@
             a string.
         orient : str
             Indication of expected JSON string format.
-
             * Series:
-
-                - default is 'index'
-                - allowed values are: {'split','records','index','table'}.
-
+            - default is 'index'
+            - allowed values are: {'split','records','index','table'}.
             * DataFrame:
-
-                - default is 'columns'
-                - allowed values are: {'split', 'records', 'index', 'columns',
-                  'values', 'table'}.
-
+            - default is 'columns'
+            - allowed values are: {'split', 'records', 'index', 'columns',
+            'values', 'table'}.
             * The format of the JSON string:
-
-                - 'split' : dict like {'index' -> [index], 'columns' -> [columns],
-                  'data' -> [values]}
-                - 'records' : list like [{column -> value}, ... , {column -> value}]
-                - 'index' : dict like {index -> {column -> value}}
-                - 'columns' : dict like {column -> {index -> value}}
-                - 'values' : just the values array
-                - 'table' : dict like {'schema': {schema}, 'data': {data}}
-
-                Describing the data, where data component is like ``orient='records'``.
-
+            - 'split' : dict like {'index' -> [index], 'columns' -> [columns],
+            'data' -> [values]}
+            - 'records' : list like [{column -> value}, ... , {column -> value}]
+            - 'index' : dict like {index -> {column -> value}}
+            - 'columns' : dict like {column -> {index -> value}}
+            - 'values' : just the values array
+            - 'table' : dict like {'schema': {schema}, 'data': {data}}
+            Describing the data, where data component is like ``orient='records'``.
             .. versionchanged:: 0.20.0
-
         date_format : {None, 'epoch', 'iso'}
             Type of date conversion. 'epoch' = epoch milliseconds,
             'iso' = ISO8601. The default depends on the `orient`. For
@@ -2107,26 +2385,44 @@
             If 'orient' is 'records' write out line delimited json format. Will
             throw ValueError if incorrect 'orient' since others are not list
             like.
-
         compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}
-
             A string representing the compression to use in the output file,
             only used when the first argument is a filename. By default, the
             compression is inferred from the filename.
-
             .. versionchanged:: 0.24.0
-               'infer' option added and set to default
+            'infer' option added and set to default
         index : bool, default True
             Whether to include the index values in the JSON string. Not
             including the index (``index=False``) is only supported when
             orient is 'split' or 'table'.
-
             .. versionadded:: 0.23.0
-
         indent : int, optional
-           Length of whitespace used to indent each record.
-
-           .. versionadded:: 1.0.0
+            Length of whitespace used to indent each record.
+            .. versionadded:: 1.0.0
+        path_or_buf: Optional[FilePathOrBuffer] :
+             (Default value = None)
+        orient: Optional[str] :
+             (Default value = None)
+        date_format: Optional[str] :
+             (Default value = None)
+        double_precision: int :
+             (Default value = 10)
+        force_ascii: bool_t :
+             (Default value = True)
+        date_unit: str :
+             (Default value = "ms")
+        default_handler: Optional[Callable[[Any] :
+            
+        JSONSerializable]] :
+             (Default value = None)
+        lines: bool_t :
+             (Default value = False)
+        compression: Optional[str] :
+             (Default value = "infer")
+        index: bool_t :
+             (Default value = True)
+        indent: Optional[int] :
+             (Default value = None)
 
         Returns
         -------
@@ -2137,23 +2433,37 @@
         See Also
         --------
         read_json : Convert a JSON string to pandas object.
-
         Notes
         -----
         The behavior of ``indent=0`` varies from the stdlib, which does not
         indent the output but does insert newlines. Currently, ``indent=0``
         and the default ``indent=None`` are equivalent in pandas, though this
         may change in a future release.
-
         Examples
         --------
+        
+        
+        Encoding/decoding a Dataframe using ``'records'`` formatted JSON.
+        Note that index labels are not preserved with this encoding.
+        
+        
+        Encoding/decoding a Dataframe using ``'index'`` formatted JSON:
+        
+        
+        Encoding/decoding a Dataframe using ``'columns'`` formatted JSON:
+        
+        
+        Encoding/decoding a Dataframe using ``'values'`` formatted JSON:
+        
+        
+        Encoding with Table Schema:
         >>> import json
         >>> df = pd.DataFrame(
         ...     [["a", "b"], ["c", "d"]],
         ...     index=["row 1", "row 2"],
         ...     columns=["col 1", "col 2"],
         ... )
-
+        
         >>> result = df.to_json(orient="split")
         >>> parsed = json.loads(result)
         >>> json.dumps(parsed, indent=4)  # doctest: +SKIP
@@ -2177,10 +2487,7 @@
                 ]
             ]
         }
-
-        Encoding/decoding a Dataframe using ``'records'`` formatted JSON.
-        Note that index labels are not preserved with this encoding.
-
+        
         >>> result = df.to_json(orient="records")
         >>> parsed = json.loads(result)
         >>> json.dumps(parsed, indent=4)  # doctest: +SKIP
@@ -2194,9 +2501,7 @@
                 "col 2": "d"
             }
         ]
-
-        Encoding/decoding a Dataframe using ``'index'`` formatted JSON:
-
+        
         >>> result = df.to_json(orient="index")
         >>> parsed = json.loads(result)
         >>> json.dumps(parsed, indent=4)  # doctest: +SKIP
@@ -2210,9 +2515,7 @@
                 "col 2": "d"
             }
         }
-
-        Encoding/decoding a Dataframe using ``'columns'`` formatted JSON:
-
+        
         >>> result = df.to_json(orient="columns")
         >>> parsed = json.loads(result)
         >>> json.dumps(parsed, indent=4)  # doctest: +SKIP
@@ -2226,9 +2529,7 @@
                 "row 2": "d"
             }
         }
-
-        Encoding/decoding a Dataframe using ``'values'`` formatted JSON:
-
+        
         >>> result = df.to_json(orient="values")
         >>> parsed = json.loads(result)
         >>> json.dumps(parsed, indent=4)  # doctest: +SKIP
@@ -2242,9 +2543,7 @@
                 "d"
             ]
         ]
-
-        Encoding with Table Schema:
-
+        
         >>> result = df.to_json(orient="table")
         >>> parsed = json.loads(result)
         >>> json.dumps(parsed, indent=4)  # doctest: +SKIP
@@ -2325,17 +2624,16 @@
         errors: str = "strict",
         encoding: str = "UTF-8",
     ) -> None:
-        """
-        Write the contained data to an HDF5 file using HDFStore.
-
+        """Write the contained data to an HDF5 file using HDFStore.
+        
         Hierarchical Data Format (HDF) is self-describing, allowing an
         application to interpret the structure and contents of a file with
         no outside information. One HDF file can hold a mix of related objects
         which can be accessed as a group or as individual objects.
-
+        
         In order to add another DataFrame or Series to an existing HDF file
         please use append mode and a different a key.
-
+        
         For more information see the :ref:`user guide <io.hdf5>`.
 
         Parameters
@@ -2346,11 +2644,10 @@
             Identifier for the group in the store.
         mode : {'a', 'w', 'r+'}, default 'a'
             Mode to open file:
-
             - 'w': write, a new file is created (an existing file with
-              the same name would be deleted).
+            the same name would be deleted).
             - 'a': append, an existing file is opened for reading and
-              writing, and if the file does not exist it is created.
+            writing, and if the file does not exist it is created.
             - 'r+': similar to 'a', but the file must already exist.
         complevel : {0-9}, optional
             Specifies a compression level for data.
@@ -2367,29 +2664,62 @@
             For Table formats, append the input data to the existing.
         format : {'fixed', 'table', None}, default 'fixed'
             Possible values:
-
             - 'fixed': Fixed format. Fast writing/reading. Not-appendable,
-              nor searchable.
+            nor searchable.
             - 'table': Table format. Write as a PyTables Table structure
-              which may perform worse but allow more flexible operations
-              like searching / selecting subsets of the data.
+            which may perform worse but allow more flexible operations
+            like searching / selecting subsets of the data.
             - If None, pd.get_option('io.hdf.default_format') is checked,
-              followed by fallback to "fixed"
+            followed by fallback to "fixed"
         errors : str, default 'strict'
             Specifies how encoding and decoding errors are to be handled.
             See the errors argument for :func:`open` for a full list
             of options.
         encoding : str, default "UTF-8"
+            
         min_itemsize : dict or int, optional
             Map column names to minimum string sizes for columns.
         nan_rep : Any, optional
             How to represent null values as str.
-            Not allowed with append=True.
+            Not allowed with append=True. (Default value = None)
         data_columns : list of columns or True, optional
             List of columns to create as indexed data columns for on-disk
             queries, or True to use all columns. By default only the axes
             of the object are indexed. See :ref:`io.hdf5-query-data-columns`.
             Applicable only to format='table'.
+        key: str :
+            
+        mode: str :
+             (Default value = "a")
+        complevel: Optional[int] :
+             (Default value = None)
+        complib: Optional[str] :
+             (Default value = None)
+        append: bool_t :
+             (Default value = False)
+        format: Optional[str] :
+             (Default value = None)
+        index: bool_t :
+             (Default value = True)
+        min_itemsize: Optional[Union[int :
+            
+        Dict[str :
+            
+        int]]] :
+             (Default value = None)
+        dropna: Optional[bool_t] :
+             (Default value = None)
+        data_columns: Optional[Union[bool_t :
+            
+        List[str]]] :
+             (Default value = None)
+        errors: str :
+             (Default value = "strict")
+        encoding: str :
+             (Default value = "UTF-8")
+
+        Returns
+        -------
 
         See Also
         --------
@@ -2398,20 +2728,23 @@
         DataFrame.to_sql : Write to a sql table.
         DataFrame.to_feather : Write out feather-format for DataFrames.
         DataFrame.to_csv : Write out to a csv file.
-
         Examples
         --------
+        
+        We can add another object to the same file:
+        
+        
+        Reading from HDF file:
+        
+        
+        Deleting file with data:
         >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},
         ...                   index=['a', 'b', 'c'])
         >>> df.to_hdf('data.h5', key='df', mode='w')
-
-        We can add another object to the same file:
-
+        
         >>> s = pd.Series([1, 2, 3, 4])
         >>> s.to_hdf('data.h5', key='s')
-
-        Reading from HDF file:
-
+        
         >>> pd.read_hdf('data.h5', 'df')
         A  B
         a  1  4
@@ -2423,9 +2756,7 @@
         2    3
         3    4
         dtype: int64
-
-        Deleting file with data:
-
+        
         >>> import os
         >>> os.remove('data.h5')
         """
@@ -2461,9 +2792,8 @@
         dtype=None,
         method=None,
     ) -> None:
-        """
-        Write records stored in a DataFrame to a SQL database.
-
+        """Write records stored in a DataFrame to a SQL database.
+        
         Databases supported by SQLAlchemy [1]_ are supported. Tables can be
         newly created, appended to, or overwritten.
 
@@ -2476,18 +2806,15 @@
             library. Legacy support is provided for sqlite3.Connection objects. The user
             is responsible for engine disposal and connection closure for the SQLAlchemy
             connectable See `here \
-                <https://docs.sqlalchemy.org/en/13/core/connections.html>`_.
-
+            <https://docs.sqlalchemy.org/en/13/core/connections.html>`_.
         schema : str, optional
             Specify the schema (if database flavor supports this). If None, use
             default schema.
         if_exists : {'fail', 'replace', 'append'}, default 'fail'
             How to behave if the table already exists.
-
             * fail: Raise a ValueError.
             * replace: Drop the table before inserting new values.
             * append: Insert new values to the existing table.
-
         index : bool, default True
             Write DataFrame index as a column. Uses `index_label` as the column
             name in the table.
@@ -2502,18 +2829,24 @@
             Specifying the datatype for columns. If a dictionary is used, the
             keys should be the column names and the values should be the
             SQLAlchemy types or strings for the sqlite3 legacy mode. If a
-            scalar is provided, it will be applied to all columns.
+            scalar is provided, it will be applied to all columns. (Default value = None)
         method : {None, 'multi', callable}, optional
             Controls the SQL insertion clause used:
-
             * None : Uses standard SQL ``INSERT`` clause (one per row).
             * 'multi': Pass multiple values in a single ``INSERT`` clause.
             * callable with signature ``(pd_table, conn, keys, data_iter)``.
-
             Details and a sample callable implementation can be found in the
             section :ref:`insert method <io.sql.method>`.
-
-            .. versionadded:: 0.24.0
+            .. versionadded:: 0.24.0 (Default value = None)
+        name: str :
+            
+        if_exists: str :
+             (Default value = "fail")
+        index: bool_t :
+             (Default value = True)
+
+        Returns
+        -------
 
         Raises
         ------
@@ -2524,79 +2857,81 @@
         See Also
         --------
         read_sql : Read a DataFrame from a table.
-
         Notes
         -----
         Timezone aware datetime columns will be written as
         ``Timestamp with timezone`` type with SQLAlchemy if supported by the
         database. Otherwise, the datetimes will be stored as timezone unaware
         timestamps local to the original timezone.
-
+        
         .. versionadded:: 0.24.0
-
         References
         ----------
         .. [1] https://docs.sqlalchemy.org
         .. [2] https://www.python.org/dev/peps/pep-0249/
-
         Examples
         --------
         Create an in-memory SQLite database.
-
+        
+        
+        Create a table from scratch with 3 rows.
+        
+        
+        
+        An `sqlalchemy.engine.Connection` can also be passed to to `con`:
+        
+        This is allowed to support operations that require that the same
+        DBAPI connection is used for the entire operation.
+        
+        
+        Overwrite the table with just ``df2``.
+        
+        
+        Specify the dtype (especially useful for integers with missing values).
+        Notice that while pandas is forced to store the data as floating point,
+        the database supports nullable integers. When fetching the data with
+        Python, we get back integer scalars.
         >>> from sqlalchemy import create_engine
         >>> engine = create_engine('sqlite://', echo=False)
-
-        Create a table from scratch with 3 rows.
-
+        
         >>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})
         >>> df
              name
         0  User 1
         1  User 2
         2  User 3
-
+        
         >>> df.to_sql('users', con=engine)
         >>> engine.execute("SELECT * FROM users").fetchall()
         [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]
-
-        An `sqlalchemy.engine.Connection` can also be passed to to `con`:
+        
         >>> with engine.begin() as connection:
         ...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})
         ...     df1.to_sql('users', con=connection, if_exists='append')
-
-        This is allowed to support operations that require that the same
-        DBAPI connection is used for the entire operation.
-
+        
         >>> df2 = pd.DataFrame({'name' : ['User 6', 'User 7']})
         >>> df2.to_sql('users', con=engine, if_exists='append')
         >>> engine.execute("SELECT * FROM users").fetchall()
         [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),
          (0, 'User 4'), (1, 'User 5'), (0, 'User 6'),
          (1, 'User 7')]
-
-        Overwrite the table with just ``df2``.
-
+        
         >>> df2.to_sql('users', con=engine, if_exists='replace',
         ...            index_label='id')
         >>> engine.execute("SELECT * FROM users").fetchall()
         [(0, 'User 6'), (1, 'User 7')]
-
-        Specify the dtype (especially useful for integers with missing values).
-        Notice that while pandas is forced to store the data as floating point,
-        the database supports nullable integers. When fetching the data with
-        Python, we get back integer scalars.
-
+        
         >>> df = pd.DataFrame({"A": [1, None, 2]})
         >>> df
              A
         0  1.0
         1  NaN
         2  2.0
-
+        
         >>> from sqlalchemy.types import Integer
         >>> df.to_sql('integers', con=engine, index=False,
         ...           dtype={"A": Integer()})
-
+        
         >>> engine.execute("SELECT * FROM integers").fetchall()
         [(1,), (None,), (2,)]
         """
@@ -2621,15 +2956,15 @@
         compression: Optional[str] = "infer",
         protocol: int = pickle.HIGHEST_PROTOCOL,
     ) -> None:
-        """
-        Pickle (serialize) object to file.
+        """Pickle (serialize) object to file.
 
         Parameters
         ----------
         path : str
             File path where the pickled object will be stored.
         compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, \
-        default 'infer'
+            
+        default 'infer' :
             A string representing the compression to use in the output file. By
             default, infers from the file extension in specified path.
         protocol : int
@@ -2637,8 +2972,14 @@
             default HIGHEST_PROTOCOL (see [1]_ paragraph 12.1.2). The possible
             values are 0, 1, 2, 3, 4. A negative value for the protocol
             parameter is equivalent to setting its value to HIGHEST_PROTOCOL.
-
             .. [1] https://docs.python.org/3/library/pickle.html.
+        compression: Optional[str] :
+             (Default value = "infer")
+        protocol: int :
+             (Default value = pickle.HIGHEST_PROTOCOL)
+
+        Returns
+        -------
 
         See Also
         --------
@@ -2646,7 +2987,6 @@
         DataFrame.to_hdf : Write DataFrame to an HDF5 file.
         DataFrame.to_sql : Write DataFrame to a SQL database.
         DataFrame.to_parquet : Write a DataFrame to the binary parquet format.
-
         Examples
         --------
         >>> original_df = pd.DataFrame({"foo": range(5), "bar": range(5, 10)})
@@ -2658,7 +2998,7 @@
         3    3    8
         4    4    9
         >>> original_df.to_pickle("./dummy.pkl")
-
+        
         >>> unpickled_df = pd.read_pickle("./dummy.pkl")
         >>> unpickled_df
            foo  bar
@@ -2667,7 +3007,7 @@
         2    2    7
         3    3    8
         4    4    9
-
+        
         >>> import os
         >>> os.remove("./dummy.pkl")
         """
@@ -2678,9 +3018,9 @@
     def to_clipboard(
         self, excel: bool_t = True, sep: Optional[str] = None, **kwargs
     ) -> None:
-        r"""
+        """r"""
         Copy object to the system clipboard.
-
+        
         Write a text representation of object to the system clipboard.
         This can be pasted into Excel, for example.
 
@@ -2688,57 +3028,62 @@
         ----------
         excel : bool, default True
             Produce output in a csv format for easy pasting into excel.
-
             - True, use the provided separator for csv pasting.
             - False, write a string representation of the object to the clipboard.
-
         sep : str, default ``'\t'``
             Field delimiter.
-        **kwargs
+        **kwargs :
             These parameters will be passed to DataFrame.to_csv.
+        excel: bool_t :
+             (Default value = True)
+        sep: Optional[str] :
+             (Default value = None)
+
+        Returns
+        -------
 
         See Also
         --------
         DataFrame.to_csv : Write a DataFrame to a comma-separated values
             (csv) file.
         read_clipboard : Read text from clipboard and pass to read_table.
-
         Notes
         -----
         Requirements for your platform.
-
+        
           - Linux : `xclip`, or `xsel` (with `PyQt4` modules)
           - Windows : none
           - OS X : none
-
         Examples
         --------
         Copy the contents of a DataFrame to the clipboard.
-
+        
+        
+        
+        We can omit the index by passing the keyword `index` and setting
+        it to false.
         >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C'])
-
+        
         >>> df.to_clipboard(sep=',')  # doctest: +SKIP
         ... # Wrote the following to the system clipboard:
         ... # ,A,B,C
         ... # 0,1,2,3
         ... # 1,4,5,6
-
-        We can omit the index by passing the keyword `index` and setting
-        it to false.
-
+        
         >>> df.to_clipboard(sep=',', index=False)  # doctest: +SKIP
         ... # Wrote the following to the system clipboard:
         ... # A,B,C
         ... # 1,2,3
         ... # 4,5,6
-        """
         from pandas.io import clipboards
 
         clipboards.to_clipboard(self, excel=excel, sep=sep, **kwargs)
 
     def to_xarray(self):
-        """
-        Return an xarray object from the pandas object.
+        """Return an xarray object from the pandas object.
+
+        Parameters
+        ----------
 
         Returns
         -------
@@ -2750,11 +3095,9 @@
         --------
         DataFrame.to_hdf : Write DataFrame to an HDF5 file.
         DataFrame.to_parquet : Write a DataFrame to the binary parquet format.
-
         Notes
         -----
         See the `xarray docs <https://xarray.pydata.org/en/stable/>`__
-
         Examples
         --------
         >>> df = pd.DataFrame([('falcon', 'bird', 389.0, 2),
@@ -2769,7 +3112,7 @@
         1  parrot    bird       24.0         2
         2    lion  mammal       80.5         4
         3  monkey  mammal        NaN         4
-
+        
         >>> df.to_xarray()
         <xarray.Dataset>
         Dimensions:    (index: 4)
@@ -2780,13 +3123,13 @@
             class      (index) object 'bird' 'bird' 'mammal' 'mammal'
             max_speed  (index) float64 389.0 24.0 80.5 nan
             num_legs   (index) int64 2 2 4 4
-
+        
         >>> df['max_speed'].to_xarray()
         <xarray.DataArray 'max_speed' (index: 4)>
         array([389. ,  24. ,  80.5,   nan])
         Coordinates:
           * index    (index) int64 0 1 2 3
-
+        
         >>> dates = pd.to_datetime(['2018-01-01', '2018-01-01',
         ...                         '2018-01-02', '2018-01-02'])
         >>> df_multiindex = pd.DataFrame({'date': dates,
@@ -2794,7 +3137,7 @@
         ...                                          'falcon', 'parrot'],
         ...                               'speed': [350, 18, 361, 15]})
         >>> df_multiindex = df_multiindex.set_index(['date', 'animal'])
-
+        
         >>> df_multiindex
                            speed
         date       animal
@@ -2802,7 +3145,7 @@
                    parrot     18
         2018-01-02 falcon    361
                    parrot     15
-
+        
         >>> df_multiindex.to_xarray()
         <xarray.Dataset>
         Dimensions:  (animal: 2, date: 2)
@@ -2844,50 +3187,50 @@
         caption=None,
         label=None,
     ):
-        r"""
+        """r"""
         Render object to a LaTeX tabular, longtable, or nested table/tabular.
-
+        
         Requires ``\usepackage{booktabs}``.  The output can be copy/pasted
         into a main LaTeX document or read from an external file
         with ``\input{table.tex}``.
-
+        
         .. versionchanged:: 0.20.2
            Added to Series.
-
+        
         .. versionchanged:: 1.0.0
            Added caption and label arguments.
 
         Parameters
         ----------
         buf : str, Path or StringIO-like, optional, default None
-            Buffer to write to. If None, the output is returned as a string.
+            Buffer to write to. If None, the output is returned as a string. (Default value = None)
         columns : list of label, optional
             The subset of columns to write. Writes all columns by default.
         col_space : int, optional
-            The minimum width of each column.
+            The minimum width of each column. (Default value = None)
         header : bool or list of str, default True
             Write out the column names. If a list of strings is given,
-            it is assumed to be aliases for the column names.
+            it is assumed to be aliases for the column names. (Default value = True)
         index : bool, default True
-            Write row names (index).
+            Write row names (index). (Default value = True)
         na_rep : str, default 'NaN'
-            Missing data representation.
+            Missing data representation. (Default value = "NaN")
         formatters : list of functions or dict of {str: function}, optional
             Formatter functions to apply to columns' elements by position or
             name. The result of each function must be a unicode string.
-            List must be of length equal to the number of columns.
+            List must be of length equal to the number of columns. (Default value = None)
         float_format : one-parameter function or str, optional, default None
             Formatter for floating point numbers. For example
             ``float_format="%%.2f"`` and ``float_format="{:0.2f}".format`` will
-            both result in 0.1234 being formatted as 0.12.
+            both result in 0.1234 being formatted as 0.12. (Default value = None)
         sparsify : bool, optional
             Set to False for a DataFrame with a hierarchical index to print
             every multiindex key at each row. By default, the value will be
             read from the config module.
         index_names : bool, default True
-            Prints the names of the indexes.
+            Prints the names of the indexes. (Default value = True)
         bold_rows : bool, default False
-            Make the row labels bold in the output.
+            Make the row labels bold in the output. (Default value = False)
         column_format : str, optional
             The columns format as specified in `LaTeX table format
             <https://en.wikibooks.org/wiki/LaTeX/Tables>`__ e.g. 'rcl' for 3
@@ -2905,7 +3248,7 @@
             A string representing the encoding to use in the output file,
             defaults to 'utf-8'.
         decimal : str, default '.'
-            Character recognized as decimal separator, e.g. ',' in Europe.
+            Character recognized as decimal separator, e.g. ',' in Europe. (Default value = ".")
         multicolumn : bool, default True
             Use \multicolumn to enhance MultiIndex columns.
             The default will be read from the config module.
@@ -2920,21 +3263,20 @@
             from the pandas config module.
         caption : str, optional
             The LaTeX caption to be placed inside ``\caption{}`` in the output.
-
-            .. versionadded:: 1.0.0
-
+            .. versionadded:: 1.0.0 (Default value = None)
         label : str, optional
             The LaTeX label to be placed inside ``\label{}`` in the output.
             This is used with ``\ref{}`` in the main ``.tex`` file.
-
-            .. versionadded:: 1.0.0
-        %(returns)s
+            .. versionadded:: 1.0.0 (Default value = None)
+
+        Returns
+        -------
+
         See Also
         --------
         DataFrame.to_string : Render a DataFrame to a console-friendly
             tabular output.
         DataFrame.to_html : Render a DataFrame as an HTML table.
-
         Examples
         --------
         >>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],
@@ -2949,7 +3291,6 @@
           Donatello &  purple &  bo staff \\
         \bottomrule
         \end{tabular}
-        """
         # Get defaults from the pandas config
         if self.ndim == 1:
             self = self.to_frame()
@@ -3014,9 +3355,9 @@
         decimal: Optional[str] = ".",
         errors: str = "strict",
     ) -> Optional[str]:
-        r"""
+        """r"""
         Write object to a comma-separated values (csv) file.
-
+        
         .. versionchanged:: 0.24.0
             The order of arguments for Series was changed.
 
@@ -3026,11 +3367,8 @@
             File path or object, if None is provided the result is returned as
             a string.  If a file object is passed it should be opened with
             `newline=''`, disabling universal newlines.
-
             .. versionchanged:: 0.24.0
-
-               Was previously named "path" for Series.
-
+            Was previously named "path" for Series.
         sep : str, default ','
             String of length 1. Field delimiter for the output file.
         na_rep : str, default ''
@@ -3042,11 +3380,8 @@
         header : bool or list of str, default True
             Write out the column names. If a list of strings is given it is
             assumed to be aliases for the column names.
-
             .. versionchanged:: 0.24.0
-
-               Previously defaulted to False for Series.
-
+            Previously defaulted to False for Series.
         index : bool, default True
             Write row names (index).
         index_label : str or sequence, or False, default None
@@ -3070,19 +3405,14 @@
             and mode is one of {'zip', 'gzip', 'bz2'}, or inferred as
             one of the above, other entries passed as
             additional compression options.
-
             .. versionchanged:: 1.0.0
-
-               May now be a dict with key 'method' as compression mode
-               and other entries as additional compression options if
-               compression mode is 'zip'.
-
+            May now be a dict with key 'method' as compression mode
+            and other entries as additional compression options if
+            compression mode is 'zip'.
             .. versionchanged:: 1.1.0
-
-               Passing compression options as keys in dict is
-               supported for compression modes 'gzip' and 'bz2'
-               as well as 'zip'.
-
+            Passing compression options as keys in dict is
+            supported for compression modes 'gzip' and 'bz2'
+            as well as 'zip'.
         quoting : optional constant from csv module
             Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`
             then floats are converted to strings and thus csv.QUOTE_NONNUMERIC
@@ -3093,7 +3423,6 @@
             The newline character or character sequence to use in the output
             file. Defaults to `os.linesep`, which depends on the OS in which
             this method is called ('\n' for linux, '\r\n' for Windows, i.e.).
-
             .. versionchanged:: 0.24.0
         chunksize : int or None
             Rows to write at a time.
@@ -3111,8 +3440,59 @@
             Specifies how encoding and decoding errors are to be handled.
             See the errors argument for :func:`open` for a full list
             of options.
-
             .. versionadded:: 1.1.0
+        path_or_buf: Optional[FilePathOrBuffer] :
+             (Default value = None)
+        sep: str :
+             (Default value = ")
+        " :
+            
+        na_rep: str :
+             (Default value = "")
+        float_format: Optional[str] :
+             (Default value = None)
+        columns: Optional[Sequence[Label]] :
+             (Default value = None)
+        header: Union[bool_t :
+            
+        List[str]] :
+             (Default value = True)
+        index: bool_t :
+             (Default value = True)
+        index_label: Optional[Union[bool_t :
+            
+        str :
+            
+        Sequence[Label]]] :
+             (Default value = None)
+        mode: str :
+             (Default value = "w")
+        encoding: Optional[str] :
+             (Default value = None)
+        compression: Optional[Union[str :
+            
+        Mapping[str :
+            
+        str]]] :
+             (Default value = "infer")
+        quoting: Optional[int] :
+             (Default value = None)
+        quotechar: str :
+             (Default value = '"')
+        line_terminator: Optional[str] :
+             (Default value = None)
+        chunksize: Optional[int] :
+             (Default value = None)
+        date_format: Optional[str] :
+             (Default value = None)
+        doublequote: bool_t :
+             (Default value = True)
+        escapechar: Optional[str] :
+             (Default value = None)
+        decimal: Optional[str] :
+             (Default value = ".")
+        errors: str :
+             (Default value = "strict")
 
         Returns
         -------
@@ -3124,22 +3504,20 @@
         --------
         read_csv : Load a CSV file into a DataFrame.
         to_excel : Write DataFrame to an Excel file.
-
         Examples
         --------
+        
+        Create 'out.zip' containing 'out.csv'
         >>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],
         ...                    'mask': ['red', 'purple'],
         ...                    'weapon': ['sai', 'bo staff']})
         >>> df.to_csv(index=False)
         'name,mask,weapon\nRaphael,red,sai\nDonatello,purple,bo staff\n'
-
-        Create 'out.zip' containing 'out.csv'
-
+        
         >>> compression_opts = dict(method='zip',
         ...                         archive_name='out.csv')  # doctest: +SKIP
         >>> df.to_csv('out.zip', index=False,
         ...           compression=compression_opts)  # doctest: +SKIP
-        """
         df = self if isinstance(self, ABCDataFrame) else self.to_frame()
 
         from pandas.io.formats.csvs import CSVFormatter
@@ -3178,33 +3556,51 @@
     # Lookup Caching
 
     def _set_as_cached(self, item, cacher) -> None:
-        """
-        Set the _cacher attribute on the calling object with a weakref to
+        """Set the _cacher attribute on the calling object with a weakref to
         cacher.
+
+        Parameters
+        ----------
+        item :
+            
+        cacher :
+            
+
+        Returns
+        -------
+
         """
         self._cacher = (item, weakref.ref(cacher))
 
     def _reset_cacher(self) -> None:
-        """
-        Reset the cacher.
-        """
+        """Reset the cacher."""
         if hasattr(self, "_cacher"):
             del self._cacher
 
     def _maybe_cache_changed(self, item, value) -> None:
-        """
-        The object has called back to us saying maybe it has changed.
+        """The object has called back to us saying maybe it has changed.
+
+        Parameters
+        ----------
+        item :
+            
+        value :
+            
+
+        Returns
+        -------
+
         """
         loc = self._info_axis.get_loc(item)
         self._mgr.iset(loc, value)
 
     @property
     def _is_cached(self) -> bool_t:
-        """Return boolean indicating if self is cached or not."""
+        """ """
         return getattr(self, "_cacher", None) is not None
 
     def _get_cacher(self):
-        """return my cacher or None"""
+        """ """
         cacher = getattr(self, "_cacher", None)
         if cacher is not None:
             cacher = cacher[1]()
@@ -3213,16 +3609,20 @@
     def _maybe_update_cacher(
         self, clear: bool_t = False, verify_is_copy: bool_t = True
     ) -> None:
-        """
-        See if we need to update our parent cacher if clear, then clear our
+        """See if we need to update our parent cacher if clear, then clear our
         cache.
 
         Parameters
         ----------
-        clear : bool, default False
-            Clear the item cache.
-        verify_is_copy : bool, default True
-            Provide is_copy checks.
+        clear: bool_t :
+             (Default value = False)
+        verify_is_copy: bool_t :
+             (Default value = True)
+
+        Returns
+        -------
+
+        
         """
         cacher = getattr(self, "_cacher", None)
         if cacher is not None:
@@ -3248,6 +3648,7 @@
             self._clear_item_cache()
 
     def _clear_item_cache(self) -> None:
+        """ """
         self._item_cache.clear()
 
     # ----------------------------------------------------------------------
@@ -3256,9 +3657,8 @@
     def take(
         self: FrameOrSeries, indices, axis=0, is_copy: Optional[bool_t] = None, **kwargs
     ) -> FrameOrSeries:
-        """
-        Return the elements in the given *positional* indices along an axis.
-
+        """Return the elements in the given *positional* indices along an axis.
+        
         This means that we are not indexing according to actual values in
         the index attribute of the object. We are indexing according to the
         actual position of the element in the object.
@@ -3269,17 +3669,20 @@
             An array of ints indicating which positions to take.
         axis : {0 or 'index', 1 or 'columns', None}, default 0
             The axis on which to select elements. ``0`` means that we are
-            selecting rows, ``1`` means that we are selecting columns.
+            selecting rows, ``1`` means that we are selecting columns. (Default value = 0)
         is_copy : bool
             Before pandas 1.0, ``is_copy=False`` can be specified to ensure
             that the return value is an actual copy. Starting with pandas 1.0,
             ``take`` always returns a copy, and the keyword is therefore
             deprecated.
-
             .. deprecated:: 1.0.0
-        **kwargs
+        **kwargs :
             For compatibility with :meth:`numpy.take`. Has no effect on the
             output.
+        self: FrameOrSeries :
+            
+        is_copy: Optional[bool_t] :
+             (Default value = None)
 
         Returns
         -------
@@ -3291,9 +3694,21 @@
         DataFrame.loc : Select a subset of a DataFrame by labels.
         DataFrame.iloc : Select a subset of a DataFrame by positions.
         numpy.take : Take elements from an array along an axis.
-
         Examples
         --------
+        
+        Take elements at positions 0 and 3 along the axis 0 (default).
+        
+        Note how the actual indices selected (0 and 1) do not correspond to
+        our selected indices 0 and 3. That's because we are selecting the 0th
+        and 3rd rows, not rows whose indices equal 0 and 3.
+        
+        
+        Take elements at indices 1 and 2 along the axis 1 (column selection).
+        
+        
+        We may take elements using negative integers for positive indices,
+        starting from the end of the object, just like with Python lists.
         >>> df = pd.DataFrame([('falcon', 'bird', 389.0),
         ...                    ('parrot', 'bird', 24.0),
         ...                    ('lion', 'mammal', 80.5),
@@ -3306,30 +3721,19 @@
         2  parrot    bird       24.0
         3    lion  mammal       80.5
         1  monkey  mammal        NaN
-
-        Take elements at positions 0 and 3 along the axis 0 (default).
-
-        Note how the actual indices selected (0 and 1) do not correspond to
-        our selected indices 0 and 3. That's because we are selecting the 0th
-        and 3rd rows, not rows whose indices equal 0 and 3.
-
+        
         >>> df.take([0, 3])
              name   class  max_speed
         0  falcon    bird      389.0
         1  monkey  mammal        NaN
-
-        Take elements at indices 1 and 2 along the axis 1 (column selection).
-
+        
         >>> df.take([1, 2], axis=1)
             class  max_speed
         0    bird      389.0
         2    bird       24.0
         3  mammal       80.5
         1  mammal        NaN
-
-        We may take elements using negative integers for positive indices,
-        starting from the end of the object, just like with Python lists.
-
+        
         >>> df.take([-1, -2])
              name   class  max_speed
         1  monkey  mammal        NaN
@@ -3353,12 +3757,24 @@
         return self._constructor(new_data).__finalize__(self, method="take")
 
     def _take_with_is_copy(self: FrameOrSeries, indices, axis=0) -> FrameOrSeries:
-        """
-        Internal version of the `take` method that sets the `_is_copy`
+        """Internal version of the `take` method that sets the `_is_copy`
         attribute to keep track of the parent dataframe (using in indexing
         for the SettingWithCopyWarning).
-
+        
         See the docstring of `take` for full explanation of the parameters.
+
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
+        indices :
+            
+        axis :
+             (Default value = 0)
+
+        Returns
+        -------
+
         """
         result = self.take(indices=indices, axis=axis)
         # Maybe set copy if we didn't actually change the index.
@@ -3367,9 +3783,8 @@
         return result
 
     def xs(self, key, axis=0, level=None, drop_level: bool_t = True):
-        """
-        Return cross-section from the Series/DataFrame.
-
+        """Return cross-section from the Series/DataFrame.
+        
         This method takes a `key` argument to select data at a particular
         level of a MultiIndex.
 
@@ -3378,12 +3793,14 @@
         key : label or tuple of label
             Label contained in the index, or partially in a MultiIndex.
         axis : {0 or 'index', 1 or 'columns'}, default 0
-            Axis to retrieve cross-section on.
+            Axis to retrieve cross-section on. (Default value = 0)
         level : object, defaults to first n levels (n=1 or len(key))
             In case of a key partially contained in a MultiIndex, indicate
-            which levels are used. Levels can be referred by label or position.
+            which levels are used. Levels can be referred by label or position. (Default value = None)
         drop_level : bool, default True
             If False, returns object with same levels as self.
+        drop_level: bool_t :
+             (Default value = True)
 
         Returns
         -------
@@ -3397,18 +3814,30 @@
             by label(s) or a boolean array.
         DataFrame.iloc : Purely integer-location based indexing
             for selection by position.
-
         Notes
         -----
         `xs` can not be used to set values.
-
+        
         MultiIndex Slicers is a generic way to get/set values on
         any level or levels.
         It is a superset of `xs` functionality, see
         :ref:`MultiIndex Slicers <advanced.mi_slicers>`.
-
         Examples
         --------
+        
+        Get values at specified index
+        
+        
+        Get values at several indexes
+        
+        
+        Get values at specified index and level
+        
+        
+        Get values at several indexes and levels
+        
+        
+        Get values at specified column and axis
         >>> d = {'num_legs': [4, 4, 2, 2],
         ...      'num_wings': [0, 0, 2, 2],
         ...      'class': ['mammal', 'mammal', 'mammal', 'bird'],
@@ -3423,40 +3852,30 @@
                dog     walks              4          0
                bat     flies              2          2
         bird   penguin walks              2          2
-
-        Get values at specified index
-
+        
         >>> df.xs('mammal')
                            num_legs  num_wings
         animal locomotion
         cat    walks              4          0
         dog    walks              4          0
         bat    flies              2          2
-
-        Get values at several indexes
-
+        
         >>> df.xs(('mammal', 'dog'))
                     num_legs  num_wings
         locomotion
         walks              4          0
-
-        Get values at specified index and level
-
+        
         >>> df.xs('cat', level=1)
                            num_legs  num_wings
         class  locomotion
         mammal walks              4          0
-
-        Get values at several indexes and levels
-
+        
         >>> df.xs(('bird', 'walks'),
         ...       level=[0, 'locomotion'])
                  num_legs  num_wings
         animal
         penguin         2          2
-
-        Get values at specified column and axis
-
+        
         >>> df.xs('num_wings', axis=1)
         class   animal   locomotion
         mammal  cat      walks         0
@@ -3530,7 +3949,19 @@
         raise AbstractMethodError(self)
 
     def _get_item_cache(self, item):
-        """Return the cached item, item represents a label indexer."""
+        """
+
+        Parameters
+        ----------
+        item :
+            
+
+        Returns
+        -------
+        type
+            
+
+        """
         cache = self._item_cache
         res = cache.get(item)
         if res is None:
@@ -3549,10 +3980,22 @@
         return res
 
     def _slice(self: FrameOrSeries, slobj: slice, axis=0) -> FrameOrSeries:
-        """
-        Construct a slice of this container.
-
+        """Construct a slice of this container.
+        
         Slicing with this method is *always* positional.
+
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
+        slobj: slice :
+            
+        axis :
+             (Default value = 0)
+
+        Returns
+        -------
+
         """
         assert isinstance(slobj, slice), type(slobj)
         axis = self._get_block_manager_axis(axis)
@@ -3566,10 +4009,36 @@
         return result
 
     def _iset_item(self, loc: int, value) -> None:
+        """
+
+        Parameters
+        ----------
+        loc: int :
+            
+        value :
+            
+
+        Returns
+        -------
+
+        """
         self._mgr.iset(loc, value)
         self._clear_item_cache()
 
     def _set_item(self, key, value) -> None:
+        """
+
+        Parameters
+        ----------
+        key :
+            
+        value :
+            
+
+        Returns
+        -------
+
+        """
         try:
             loc = self._info_axis.get_loc(key)
         except KeyError:
@@ -3580,6 +4049,19 @@
         NDFrame._iset_item(self, loc, value)
 
     def _set_is_copy(self, ref, copy: bool_t = True) -> None:
+        """
+
+        Parameters
+        ----------
+        ref :
+            
+        copy: bool_t :
+             (Default value = True)
+
+        Returns
+        -------
+
+        """
         if not copy:
             self._is_copy = None
         else:
@@ -3587,15 +4069,21 @@
             self._is_copy = weakref.ref(ref)
 
     def _check_is_chained_assignment_possible(self) -> bool_t:
-        """
-        Check if we are a view, have a cacher, and are of mixed type.
+        """Check if we are a view, have a cacher, and are of mixed type.
         If so, then force a setitem_copy check.
-
+        
         Should be called just near setting a value
-
+        
         Will return a boolean if it we are a view and are cached, but a
         single-dtype meaning that the cacher should be updated following
         setting.
+
+        Parameters
+        ----------
+
+        Returns
+        -------
+
         """
         if self._is_view and self._is_cached:
             ref = self._get_cacher()
@@ -3611,30 +4099,17 @@
 
         Parameters
         ----------
-        stacklevel : int, default 4
-           the level to show of the stack when the error is output
-        t : str, the type of setting error
-        force : bool, default False
-           If True, then force showing an error.
-
-        validate if we are doing a setitem on a chained copy.
-
-        If you call this function, be sure to set the stacklevel such that the
-        user will see the error *at the level of setting*
-
-        It is technically possible to figure out that we are setting on
-        a copy even WITH a multi-dtyped pandas object. In other words, some
-        blocks may be views while other are not. Currently _is_view will ALWAYS
-        return False for multi-blocks to avoid having to handle this case.
-
-        df = DataFrame(np.arange(0,9), columns=['count'])
-        df['group'] = 'b'
-
-        # This technically need not raise SettingWithCopy if both are view
-        # (which is not # generally guaranteed but is usually True.  However,
-        # this is in general not a good practice and we recommend using .loc.
-        df.iloc[0:5]['group'] = 'a'
-
+        stacklevel :
+             (Default value = 4)
+        t :
+             (Default value = "setting")
+        force :
+             (Default value = False)
+
+        Returns
+        -------
+
+        
         """
         # return early if the check is not needed
         if not (force or self._is_copy):
@@ -3721,18 +4196,21 @@
     # Unsorted
 
     def get(self, key, default=None):
-        """
-        Get item from object for given key (ex: DataFrame column).
-
+        """Get item from object for given key (ex: DataFrame column).
+        
         Returns default value if not found.
 
         Parameters
         ----------
         key : object
-
-        Returns
-        -------
-        value : same type as items contained in object
+            
+        default :
+             (Default value = None)
+
+        Returns
+        -------
+
+        
         """
         try:
             return self[key]
@@ -3741,7 +4219,7 @@
 
     @property
     def _is_view(self) -> bool_t:
-        """Return boolean indicating if self is view of another array """
+        """ """
         return self._mgr.is_view
 
     def reindex_like(
@@ -3752,9 +4230,8 @@
         limit=None,
         tolerance=None,
     ) -> FrameOrSeries:
-        """
-        Return an object with matching indices as other object.
-
+        """Return an object with matching indices as other object.
+        
         Conform the object to the same index on all axes. Optional
         filling logic, placing NaN in locations having no value
         in the previous index. A new object is produced unless the
@@ -3769,27 +4246,30 @@
             Method to use for filling holes in reindexed DataFrame.
             Please note: this is only applicable to DataFrames/Series with a
             monotonically increasing/decreasing index.
-
             * None (default): don't fill gaps
             * pad / ffill: propagate last valid observation forward to next
-              valid
+            valid
             * backfill / bfill: use next valid observation to fill gap
             * nearest: use nearest valid observations to fill gap.
-
         copy : bool, default True
             Return a new object, even if the passed indexes are the same.
         limit : int, default None
-            Maximum number of consecutive labels to fill for inexact matches.
+            Maximum number of consecutive labels to fill for inexact matches. (Default value = None)
         tolerance : optional
             Maximum distance between original and new labels for inexact
             matches. The values of the index at the matching locations most
             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
-
             Tolerance may be a scalar value, which applies the same tolerance
             to all values, or list-like, which applies variable tolerance per
             element. List-like includes list, tuple, array, Series, and must be
             the same size as the index and its dtype must exactly match the
-            index's type.
+            index's type. (Default value = None)
+        self: FrameOrSeries :
+            
+        method: Optional[str] :
+             (Default value = None)
+        copy: bool_t :
+             (Default value = True)
 
         Returns
         -------
@@ -3801,12 +4281,10 @@
         DataFrame.set_index : Set row labels.
         DataFrame.reset_index : Remove row labels or move them to new columns.
         DataFrame.reindex : Change to new indices or expand indices.
-
         Notes
         -----
         Same as calling
         ``.reindex(index=other.index, columns=other.columns,...)``.
-
         Examples
         --------
         >>> df1 = pd.DataFrame([[24.3, 75.7, 'high'],
@@ -3817,27 +4295,27 @@
         ...                             'windspeed'],
         ...                    index=pd.date_range(start='2014-02-12',
         ...                                        end='2014-02-15', freq='D'))
-
+        
         >>> df1
                     temp_celsius  temp_fahrenheit windspeed
         2014-02-12          24.3             75.7      high
         2014-02-13          31.0             87.8      high
         2014-02-14          22.0             71.6    medium
         2014-02-15          35.0             95.0    medium
-
+        
         >>> df2 = pd.DataFrame([[28, 'low'],
         ...                     [30, 'low'],
         ...                     [35.1, 'medium']],
         ...                    columns=['temp_celsius', 'windspeed'],
         ...                    index=pd.DatetimeIndex(['2014-02-12', '2014-02-13',
         ...                                            '2014-02-15']))
-
+        
         >>> df2
                     temp_celsius windspeed
         2014-02-12          28.0       low
         2014-02-13          30.0       low
         2014-02-15          35.1    medium
-
+        
         >>> df2.reindex_like(df1)
                     temp_celsius  temp_fahrenheit windspeed
         2014-02-12          28.0              NaN       low
@@ -3865,6 +4343,29 @@
         inplace: bool_t = False,
         errors: str = "raise",
     ):
+        """
+
+        Parameters
+        ----------
+        labels :
+             (Default value = None)
+        axis :
+             (Default value = 0)
+        index :
+             (Default value = None)
+        columns :
+             (Default value = None)
+        level :
+             (Default value = None)
+        inplace: bool_t :
+             (Default value = False)
+        errors: str :
+             (Default value = "raise")
+
+        Returns
+        -------
+
+        """
 
         inplace = validate_bool_kwarg(inplace, "inplace")
 
@@ -3894,19 +4395,26 @@
     def _drop_axis(
         self: FrameOrSeries, labels, axis, level=None, errors: str = "raise"
     ) -> FrameOrSeries:
-        """
-        Drop labels from specified axis. Used in the ``drop`` method
+        """Drop labels from specified axis. Used in the ``drop`` method
         internally.
 
         Parameters
         ----------
-        labels : single label or list-like
-        axis : int or axis name
-        level : int or level name, default None
-            For MultiIndex
-        errors : {'ignore', 'raise'}, default 'raise'
-            If 'ignore', suppress error and existing labels are dropped.
-
+        self: FrameOrSeries :
+            
+        labels :
+            
+        axis :
+            
+        level :
+             (Default value = None)
+        errors: str :
+             (Default value = "raise")
+
+        Returns
+        -------
+
+        
         """
         axis = self._get_axis_number(axis)
         axis_name = self._get_axis_name(axis)
@@ -3947,14 +4455,19 @@
         return result
 
     def _update_inplace(self, result, verify_is_copy: bool_t = True) -> None:
-        """
-        Replace self internals with result.
-
-        Parameters
-        ----------
-        result : same type as self
-        verify_is_copy : bool, default True
-            Provide is_copy checks.
+        """Replace self internals with result.
+
+        Parameters
+        ----------
+        result :
+            
+        verify_is_copy: bool_t :
+             (Default value = True)
+
+        Returns
+        -------
+
+        
         """
         # NOTE: This does *not* call __finalize__ and that's an explicit
         # decision that we may revisit in the future.
@@ -3964,9 +4477,8 @@
         self._maybe_update_cacher(verify_is_copy=verify_is_copy)
 
     def add_prefix(self: FrameOrSeries, prefix: str) -> FrameOrSeries:
-        """
-        Prefix labels with string `prefix`.
-
+        """Prefix labels with string `prefix`.
+        
         For Series, the row labels are prefixed.
         For DataFrame, the column labels are prefixed.
 
@@ -3974,6 +4486,10 @@
         ----------
         prefix : str
             The string to add before each label.
+        self: FrameOrSeries :
+            
+        prefix: str :
+            
 
         Returns
         -------
@@ -3984,7 +4500,6 @@
         --------
         Series.add_suffix: Suffix row labels with string `suffix`.
         DataFrame.add_suffix: Suffix column labels with string `suffix`.
-
         Examples
         --------
         >>> s = pd.Series([1, 2, 3, 4])
@@ -3994,14 +4509,14 @@
         2    3
         3    4
         dtype: int64
-
+        
         >>> s.add_prefix('item_')
         item_0    1
         item_1    2
         item_2    3
         item_3    4
         dtype: int64
-
+        
         >>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})
         >>> df
            A  B
@@ -4009,7 +4524,7 @@
         1  2  4
         2  3  5
         3  4  6
-
+        
         >>> df.add_prefix('col_')
              col_A  col_B
         0       1       3
@@ -4023,9 +4538,8 @@
         return self.rename(**mapper)  # type: ignore
 
     def add_suffix(self: FrameOrSeries, suffix: str) -> FrameOrSeries:
-        """
-        Suffix labels with string `suffix`.
-
+        """Suffix labels with string `suffix`.
+        
         For Series, the row labels are suffixed.
         For DataFrame, the column labels are suffixed.
 
@@ -4033,6 +4547,10 @@
         ----------
         suffix : str
             The string to add after each label.
+        self: FrameOrSeries :
+            
+        suffix: str :
+            
 
         Returns
         -------
@@ -4043,7 +4561,6 @@
         --------
         Series.add_prefix: Prefix row labels with string `prefix`.
         DataFrame.add_prefix: Prefix column labels with string `prefix`.
-
         Examples
         --------
         >>> s = pd.Series([1, 2, 3, 4])
@@ -4053,14 +4570,14 @@
         2    3
         3    4
         dtype: int64
-
+        
         >>> s.add_suffix('_item')
         0_item    1
         1_item    2
         2_item    3
         3_item    4
         dtype: int64
-
+        
         >>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})
         >>> df
            A  B
@@ -4068,7 +4585,7 @@
         1  2  4
         2  3  5
         3  4  6
-
+        
         >>> df.add_suffix('_col')
              A_col  B_col
         0       1       3
@@ -4091,9 +4608,8 @@
         ignore_index: bool_t = False,
         key: ValueKeyFunc = None,
     ):
-        """
-        Sort by the values along either axis.
-
+        """Sort by the values along either axis.
+        
         Parameters
         ----------%(optional_by)s
         axis : %(axes_single_arg)s, default 0
@@ -4114,9 +4630,9 @@
              end.
         ignore_index : bool, default False
              If True, the resulting axis will be labeled 0, 1, , n - 1.
-
+        
              .. versionadded:: 1.0.0
-
+        
         key : callable, optional
             Apply the key function to the values
             before sorting. This is similar to the `key` argument in the
@@ -4124,8 +4640,25 @@
             this `key` function should be *vectorized*. It should expect a
             ``Series`` and return a Series with the same shape as the input.
             It will be applied to each column in `by` independently.
-
+        
             .. versionadded:: 1.1.0
+
+        Parameters
+        ----------
+        axis :
+             (Default value = 0)
+        ascending :
+             (Default value = True)
+        inplace: bool_t :
+             (Default value = False)
+        kind: str :
+             (Default value = "quicksort")
+        na_position: str :
+             (Default value = "last")
+        ignore_index: bool_t :
+             (Default value = False)
+        key: ValueKeyFunc :
+             (Default value = None)
 
         Returns
         -------
@@ -4136,9 +4669,22 @@
         --------
         DataFrame.sort_index : Sort a DataFrame by the index.
         Series.sort_values : Similar method for a Series.
-
         Examples
         --------
+        
+        Sort by col1
+        
+        
+        Sort by multiple columns
+        
+        
+        Sort Descending
+        
+        
+        Putting NAs first
+        
+        
+        Sorting with a key function
         >>> df = pd.DataFrame({
         ...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],
         ...     'col2': [2, 1, 9, 8, 7, 4],
@@ -4153,9 +4699,7 @@
         3  NaN     8     4    D
         4    D     7     2    e
         5    C     4     3    F
-
-        Sort by col1
-
+        
         >>> df.sort_values(by=['col1'])
           col1  col2  col3 col4
         0    A     2     0    a
@@ -4164,9 +4708,7 @@
         5    C     4     3    F
         4    D     7     2    e
         3  NaN     8     4    D
-
-        Sort by multiple columns
-
+        
         >>> df.sort_values(by=['col1', 'col2'])
           col1  col2  col3 col4
         1    A     1     1    B
@@ -4175,9 +4717,7 @@
         5    C     4     3    F
         4    D     7     2    e
         3  NaN     8     4    D
-
-        Sort Descending
-
+        
         >>> df.sort_values(by='col1', ascending=False)
           col1  col2  col3 col4
         4    D     7     2    e
@@ -4186,9 +4726,7 @@
         0    A     2     0    a
         1    A     1     1    B
         3  NaN     8     4    D
-
-        Putting NAs first
-
+        
         >>> df.sort_values(by='col1', ascending=False, na_position='first')
           col1  col2  col3 col4
         3  NaN     8     4    D
@@ -4197,9 +4735,7 @@
         2    B     9     9    c
         0    A     2     0    a
         1    A     1     1    B
-
-        Sorting with a key function
-
+        
         >>> df.sort_values(by='col4', key=lambda col: col.str.lower())
            col1  col2  col3 col4
         0    A     2     0    a
@@ -4218,31 +4754,30 @@
         optional_axis="",
     )
     def reindex(self: FrameOrSeries, *args, **kwargs) -> FrameOrSeries:
-        """
-        Conform {klass} to new index with optional filling logic.
-
+        """Conform {klass} to new index with optional filling logic.
+        
         Places NA/NaN in locations having no value in the previous index. A new object
         is produced unless the new index is equivalent to the current one and
         ``copy=False``.
 
         Parameters
         ----------
-        {optional_labels}
+        {optional_labels} :
+            
         {axes} : array-like, optional
             New labels / index to conform to, should be specified using
             keywords. Preferably an Index object to avoid duplicating data.
-        {optional_axis}
+        {optional_axis} :
+            
         method : {{None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}}
             Method to use for filling holes in reindexed DataFrame.
             Please note: this is only applicable to DataFrames/Series with a
             monotonically increasing/decreasing index.
-
             * None (default): don't fill gaps
             * pad / ffill: Propagate last valid observation forward to next
-              valid.
+            valid.
             * backfill / bfill: Use next valid observation to fill gap.
             * nearest: Use nearest valid observations to fill gap.
-
         copy : bool, default True
             Return a new object, even if the passed indexes are the same.
         level : int or name
@@ -4257,35 +4792,86 @@
             Maximum distance between original and new labels for inexact
             matches. The values of the index at the matching locations most
             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
-
             Tolerance may be a scalar value, which applies the same tolerance
             to all values, or list-like, which applies variable tolerance per
             element. List-like includes list, tuple, array, Series, and must be
             the same size as the index and its dtype must exactly match the
             index's type.
+        self: FrameOrSeries :
+            
+        *args :
+            
+        **kwargs :
+            
 
         Returns
         -------
         {klass} with changed index.
+            
 
         See Also
         --------
         DataFrame.set_index : Set row labels.
         DataFrame.reset_index : Remove row labels or move them to new columns.
         DataFrame.reindex_like : Change to same indices as other DataFrame.
-
         Examples
         --------
         ``DataFrame.reindex`` supports two calling conventions
-
+        
         * ``(index=index_labels, columns=column_labels, ...)``
         * ``(labels, axis={{'index', 'columns'}}, ...)``
-
+        
         We *highly* recommend using keyword arguments to clarify your
         intent.
-
+        
         Create a dataframe with some fictional data.
-
+        
+        
+        Create a new index and reindex the dataframe. By default
+        values in the new index that do not have corresponding
+        records in the dataframe are assigned ``NaN``.
+        
+        
+        We can fill in the missing values by passing a value to
+        the keyword ``fill_value``. Because the index is not monotonically
+        increasing or decreasing, we cannot use arguments to the keyword
+        ``method`` to fill the ``NaN`` values.
+        
+        
+        
+        We can also reindex the columns.
+        
+        
+        Or we can use "axis-style" keyword arguments
+        
+        
+        To further illustrate the filling functionality in
+        ``reindex``, we will create a dataframe with a
+        monotonically increasing index (for example, a sequence
+        of dates).
+        
+        
+        Suppose we decide to expand the dataframe to cover a wider
+        date range.
+        
+        
+        The index entries that did not have a value in the original data frame
+        (for example, '2009-12-29') are by default filled with ``NaN``.
+        If desired, we can fill in the missing values using one of several
+        options.
+        
+        For example, to back-propagate the last valid value to fill the ``NaN``
+        values, pass ``bfill`` as an argument to the ``method`` keyword.
+        
+        
+        Please note that the ``NaN`` value present in the original dataframe
+        (at index value 2010-01-03) will not be filled by any of the
+        value propagation schemes. This is because filling while reindexing
+        does not look at dataframe values, but only compares the original and
+        desired indexes. If you do want to fill in the ``NaN`` values present
+        in the original dataframe, use the ``fillna()`` method.
+        
+        See the :ref:`user guide <basics.reindexing>` for more.
         >>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']
         >>> df = pd.DataFrame({{'http_status': [200, 200, 404, 404, 301],
         ...                   'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]}},
@@ -4297,11 +4883,7 @@
         Safari             404           0.07
         IE10               404           0.08
         Konqueror          301           1.00
-
-        Create a new index and reindex the dataframe. By default
-        values in the new index that do not have corresponding
-        records in the dataframe are assigned ``NaN``.
-
+        
         >>> new_index = ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',
         ...              'Chrome']
         >>> df.reindex(new_index)
@@ -4311,12 +4893,7 @@
         Comodo Dragon          NaN            NaN
         IE10                 404.0           0.08
         Chrome               200.0           0.02
-
-        We can fill in the missing values by passing a value to
-        the keyword ``fill_value``. Because the index is not monotonically
-        increasing or decreasing, we cannot use arguments to the keyword
-        ``method`` to fill the ``NaN`` values.
-
+        
         >>> df.reindex(new_index, fill_value=0)
                        http_status  response_time
         Safari                 404           0.07
@@ -4324,7 +4901,7 @@
         Comodo Dragon            0           0.00
         IE10                   404           0.08
         Chrome                 200           0.02
-
+        
         >>> df.reindex(new_index, fill_value='missing')
                       http_status response_time
         Safari                404          0.07
@@ -4332,9 +4909,7 @@
         Comodo Dragon     missing       missing
         IE10                  404          0.08
         Chrome                200          0.02
-
-        We can also reindex the columns.
-
+        
         >>> df.reindex(columns=['http_status', 'user_agent'])
                    http_status  user_agent
         Firefox            200         NaN
@@ -4342,9 +4917,7 @@
         Safari             404         NaN
         IE10               404         NaN
         Konqueror          301         NaN
-
-        Or we can use "axis-style" keyword arguments
-
+        
         >>> df.reindex(['http_status', 'user_agent'], axis="columns")
                    http_status  user_agent
         Firefox            200         NaN
@@ -4352,12 +4925,7 @@
         Safari             404         NaN
         IE10               404         NaN
         Konqueror          301         NaN
-
-        To further illustrate the filling functionality in
-        ``reindex``, we will create a dataframe with a
-        monotonically increasing index (for example, a sequence
-        of dates).
-
+        
         >>> date_index = pd.date_range('1/1/2010', periods=6, freq='D')
         >>> df2 = pd.DataFrame({{"prices": [100, 101, np.nan, 100, 89, 88]}},
         ...                    index=date_index)
@@ -4369,10 +4937,7 @@
         2010-01-04   100.0
         2010-01-05    89.0
         2010-01-06    88.0
-
-        Suppose we decide to expand the dataframe to cover a wider
-        date range.
-
+        
         >>> date_index2 = pd.date_range('12/29/2009', periods=10, freq='D')
         >>> df2.reindex(date_index2)
                     prices
@@ -4386,15 +4951,7 @@
         2010-01-05    89.0
         2010-01-06    88.0
         2010-01-07     NaN
-
-        The index entries that did not have a value in the original data frame
-        (for example, '2009-12-29') are by default filled with ``NaN``.
-        If desired, we can fill in the missing values using one of several
-        options.
-
-        For example, to back-propagate the last valid value to fill the ``NaN``
-        values, pass ``bfill`` as an argument to the ``method`` keyword.
-
+        
         >>> df2.reindex(date_index2, method='bfill')
                     prices
         2009-12-29   100.0
@@ -4407,15 +4964,6 @@
         2010-01-05    89.0
         2010-01-06    88.0
         2010-01-07     NaN
-
-        Please note that the ``NaN`` value present in the original dataframe
-        (at index value 2010-01-03) will not be filled by any of the
-        value propagation schemes. This is because filling while reindexing
-        does not look at dataframe values, but only compares the original and
-        desired indexes. If you do want to fill in the ``NaN`` values present
-        in the original dataframe, use the ``fillna()`` method.
-
-        See the :ref:`user guide <basics.reindexing>` for more.
         """
         # TODO: Decide if we care about having different examples for different
         # kinds
@@ -4465,7 +5013,31 @@
     def _reindex_axes(
         self: FrameOrSeries, axes, level, limit, tolerance, method, fill_value, copy
     ) -> FrameOrSeries:
-        """Perform the reindex for all the axes."""
+        """Perform the reindex for all the axes.
+
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
+        axes :
+            
+        level :
+            
+        limit :
+            
+        tolerance :
+            
+        method :
+            
+        fill_value :
+            
+        copy :
+            
+
+        Returns
+        -------
+
+        """
         obj = self
         for a in self._AXIS_ORDERS:
             labels = axes[a]
@@ -4488,7 +5060,21 @@
         return obj
 
     def _needs_reindex_multi(self, axes, method, level) -> bool_t:
-        """Check if we do need a multi reindex."""
+        """Check if we do need a multi reindex.
+
+        Parameters
+        ----------
+        axes :
+            
+        method :
+            
+        level :
+            
+
+        Returns
+        -------
+
+        """
         return (
             (com.count_not_none(*axes.values()) == self._AXIS_LEN)
             and method is None
@@ -4497,6 +5083,21 @@
         )
 
     def _reindex_multi(self, axes, copy, fill_value):
+        """
+
+        Parameters
+        ----------
+        axes :
+            
+        copy :
+            
+        fill_value :
+            
+
+        Returns
+        -------
+
+        """
         raise AbstractMethodError(self)
 
     def _reindex_with_indexers(
@@ -4506,7 +5107,25 @@
         copy: bool_t = False,
         allow_dups: bool_t = False,
     ) -> FrameOrSeries:
-        """allow_dups indicates an internal call here """
+        """allow_dups indicates an internal call here
+
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
+        reindexers :
+            
+        fill_value :
+             (Default value = None)
+        copy: bool_t :
+             (Default value = False)
+        allow_dups: bool_t :
+             (Default value = False)
+
+        Returns
+        -------
+
+        """
         # reindex doing multiple operations on different axes if indicated
         new_data = self._mgr
         for axis in sorted(reindexers.keys()):
@@ -4544,16 +5163,15 @@
         regex: Optional[str] = None,
         axis=None,
     ) -> FrameOrSeries:
-        """
-        Subset the dataframe rows or columns according to the specified index labels.
-
+        """Subset the dataframe rows or columns according to the specified index labels.
+        
         Note that this routine does not filter a dataframe on its
         contents. The filter is applied to the labels of the index.
 
         Parameters
         ----------
         items : list-like
-            Keep labels from axis which are in items.
+            Keep labels from axis which are in items. (Default value = None)
         like : str
             Keep labels from axis for which "like in label == True".
         regex : str (regular expression)
@@ -4562,24 +5180,29 @@
             The axis to filter on, expressed either as an index (int)
             or axis name (str). By default this is the info axis,
             'index' for Series, 'columns' for DataFrame.
+        self: FrameOrSeries :
+            
+        like: Optional[str] :
+             (Default value = None)
+        regex: Optional[str] :
+             (Default value = None)
 
         Returns
         -------
         same type as input object
+            
 
         See Also
         --------
         DataFrame.loc : Access a group of rows and columns
             by label(s) or a boolean array.
-
         Notes
         -----
         The ``items``, ``like``, and ``regex`` parameters are
         enforced to be mutually exclusive.
-
+        
         ``axis`` defaults to the info axis that is used when indexing
         with ``[]``.
-
         Examples
         --------
         >>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),
@@ -4589,19 +5212,19 @@
                 one  two  three
         mouse     1    2      3
         rabbit    4    5      6
-
+        
         >>> # select columns by name
         >>> df.filter(items=['one', 'three'])
                  one  three
         mouse     1      3
         rabbit    4      6
-
+        
         >>> # select columns by regular expression
         >>> df.filter(regex='e$', axis=1)
                  one  three
         mouse     1      3
         rabbit    4      6
-
+        
         >>> # select rows containing 'bbi'
         >>> df.filter(like='bbi', axis=0)
                  one  two  three
@@ -4624,6 +5247,17 @@
         elif like:
 
             def f(x):
+                """
+
+                Parameters
+                ----------
+                x :
+                    
+
+                Returns
+                -------
+
+                """
                 return like in ensure_str(x)
 
             values = labels.map(f)
@@ -4631,6 +5265,17 @@
         elif regex:
 
             def f(x):
+                """
+
+                Parameters
+                ----------
+                x :
+                    
+
+                Returns
+                -------
+
+                """
                 return matcher.search(ensure_str(x)) is not None
 
             matcher = re.compile(regex)
@@ -4640,13 +5285,12 @@
             raise TypeError("Must pass either `items`, `like`, or `regex`")
 
     def head(self: FrameOrSeries, n: int = 5) -> FrameOrSeries:
-        """
-        Return the first `n` rows.
-
+        """Return the first `n` rows.
+        
         This function returns the first `n` rows for the object based
         on position. It is useful for quickly testing if your object
         has the right type of data in it.
-
+        
         For negative values of `n`, this function returns all rows except
         the last `n` rows, equivalent to ``df[:-n]``.
 
@@ -4654,6 +5298,10 @@
         ----------
         n : int, default 5
             Number of rows to select.
+        self: FrameOrSeries :
+            
+        n: int :
+             (Default value = 5)
 
         Returns
         -------
@@ -4663,9 +5311,16 @@
         See Also
         --------
         DataFrame.tail: Returns the last `n` rows.
-
         Examples
         --------
+        
+        Viewing the first 5 lines
+        
+        
+        Viewing the first `n` lines (three in this case)
+        
+        
+        For negative values of `n`
         >>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',
         ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})
         >>> df
@@ -4679,9 +5334,7 @@
         6      shark
         7      whale
         8      zebra
-
-        Viewing the first 5 lines
-
+        
         >>> df.head()
               animal
         0  alligator
@@ -4689,17 +5342,13 @@
         2     falcon
         3       lion
         4     monkey
-
-        Viewing the first `n` lines (three in this case)
-
+        
         >>> df.head(3)
               animal
         0  alligator
         1        bee
         2     falcon
-
-        For negative values of `n`
-
+        
         >>> df.head(-3)
               animal
         0  alligator
@@ -4712,13 +5361,12 @@
         return self.iloc[:n]
 
     def tail(self: FrameOrSeries, n: int = 5) -> FrameOrSeries:
-        """
-        Return the last `n` rows.
-
+        """Return the last `n` rows.
+        
         This function returns last `n` rows from the object based on
         position. It is useful for quickly verifying data, for example,
         after sorting or appending rows.
-
+        
         For negative values of `n`, this function returns all rows except
         the first `n` rows, equivalent to ``df[n:]``.
 
@@ -4726,6 +5374,10 @@
         ----------
         n : int, default 5
             Number of rows to select.
+        self: FrameOrSeries :
+            
+        n: int :
+             (Default value = 5)
 
         Returns
         -------
@@ -4735,9 +5387,16 @@
         See Also
         --------
         DataFrame.head : The first `n` rows of the caller object.
-
         Examples
         --------
+        
+        Viewing the last 5 lines
+        
+        
+        Viewing the last `n` lines (three in this case)
+        
+        
+        For negative values of `n`
         >>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',
         ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})
         >>> df
@@ -4751,9 +5410,7 @@
         6      shark
         7      whale
         8      zebra
-
-        Viewing the last 5 lines
-
+        
         >>> df.tail()
            animal
         4  monkey
@@ -4761,17 +5418,13 @@
         6   shark
         7   whale
         8   zebra
-
-        Viewing the last `n` lines (three in this case)
-
+        
         >>> df.tail(3)
           animal
         6  shark
         7  whale
         8  zebra
-
-        For negative values of `n`
-
+        
         >>> df.tail(-3)
            animal
         3    lion
@@ -4794,9 +5447,8 @@
         random_state=None,
         axis=None,
     ) -> FrameOrSeries:
-        """
-        Return a random sample of items from an axis of object.
-
+        """Return a random sample of items from an axis of object.
+        
         You can use `random_state` for reproducibility.
 
         Parameters
@@ -4805,9 +5457,9 @@
             Number of items from axis to return. Cannot be used with `frac`.
             Default = 1 if `frac` = None.
         frac : float, optional
-            Fraction of axis items to return. Cannot be used with `n`.
+            Fraction of axis items to return. Cannot be used with `n`. (Default value = None)
         replace : bool, default False
-            Allow or disallow sampling of the same row more than once.
+            Allow or disallow sampling of the same row more than once. (Default value = False)
         weights : str or ndarray-like, optional
             Default 'None' results in equal probability weighting.
             If passed a Series, will align with target object on index. Index
@@ -4825,15 +5477,14 @@
             If int, array-like, or BitGenerator (NumPy>=1.17), seed for
             random number generator
             If np.random.RandomState, use as numpy RandomState object.
-
             .. versionchanged:: 1.1.0
-
-                array-like and BitGenerator (for NumPy>=1.17) object now passed to
-                np.random.RandomState() as seed
-
+            array-like and BitGenerator (for NumPy>=1.17) object now passed to
+            np.random.RandomState() as seed (Default value = None)
         axis : {0 or index, 1 or columns, None}, default None
             Axis to sample. Accepts axis number or name. Default is stat axis
             for given data type (0 for Series and DataFrames).
+        self: FrameOrSeries :
+            
 
         Returns
         -------
@@ -4849,13 +5500,26 @@
             Series object.
         numpy.random.choice: Generates a random sample from a given 1-D numpy
             array.
-
         Notes
         -----
         If `frac` > 1, `replacement` should be set to `True`.
-
         Examples
         --------
+        
+        Extract 3 random elements from the ``Series`` ``df['num_legs']``:
+        Note that we use `random_state` to ensure the reproducibility of
+        the examples.
+        
+        
+        A random 50% sample of the ``DataFrame`` with replacement:
+        
+        
+        An upsample sample of the ``DataFrame`` with replacement:
+        Note that `replace` parameter has to be `True` for `frac` parameter > 1.
+        
+        
+        Using a DataFrame column as weights. Rows with larger value in the
+        `num_specimen_seen` column are more likely to be sampled.
         >>> df = pd.DataFrame({'num_legs': [2, 4, 8, 0],
         ...                    'num_wings': [2, 0, 0, 0],
         ...                    'num_specimen_seen': [10, 2, 1, 8]},
@@ -4866,27 +5530,18 @@
         dog            4          0                  2
         spider         8          0                  1
         fish           0          0                  8
-
-        Extract 3 random elements from the ``Series`` ``df['num_legs']``:
-        Note that we use `random_state` to ensure the reproducibility of
-        the examples.
-
+        
         >>> df['num_legs'].sample(n=3, random_state=1)
         fish      0
         spider    8
         falcon    2
         Name: num_legs, dtype: int64
-
-        A random 50% sample of the ``DataFrame`` with replacement:
-
+        
         >>> df.sample(frac=0.5, replace=True, random_state=1)
               num_legs  num_wings  num_specimen_seen
         dog          4          0                  2
         fish         0          0                  8
-
-        An upsample sample of the ``DataFrame`` with replacement:
-        Note that `replace` parameter has to be `True` for `frac` parameter > 1.
-
+        
         >>> df.sample(frac=2, replace=True, random_state=1)
                 num_legs  num_wings  num_specimen_seen
         dog            4          0                  2
@@ -4897,10 +5552,7 @@
         dog            4          0                  2
         fish           0          0                  8
         dog            4          0                  2
-
-        Using a DataFrame column as weights. Rows with larger value in the
-        `num_specimen_seen` column are more likely to be sampled.
-
+        
         >>> df.sample(n=2, weights='num_specimen_seen', random_state=1)
                 num_legs  num_wings  num_specimen_seen
         falcon         2          2                 10
@@ -4995,7 +5647,7 @@
 
     @doc(klass=_shared_doc_kwargs["klass"])
     def pipe(self, func, *args, **kwargs):
-        r"""
+        """r"""
         Apply func(self, \*args, \*\*kwargs).
 
         Parameters
@@ -5010,10 +5662,15 @@
             Positional arguments passed into ``func``.
         kwargs : mapping, optional
             A dictionary of keyword arguments passed into ``func``.
+        *args :
+            
+        **kwargs :
+            
 
         Returns
         -------
         object : the return type of ``func``.
+            
 
         See Also
         --------
@@ -5021,30 +5678,29 @@
         DataFrame.applymap : Apply a function elementwise on a whole DataFrame.
         Series.map : Apply a mapping correspondence on a
             :class:`~pandas.Series`.
-
         Notes
         -----
         Use ``.pipe`` when chaining together functions that expect
         Series, DataFrames or GroupBy objects. Instead of writing
-
+        
+        
+        You can write
+        
+        
+        If you have a function that takes the data as (say) the second
+        argument, pass a tuple indicating which keyword expects the
+        data. For example, suppose ``f`` takes its data as ``arg2``:
         >>> func(g(h(df), arg1=a), arg2=b, arg3=c)  # doctest: +SKIP
-
-        You can write
-
+        
         >>> (df.pipe(h)
         ...    .pipe(g, arg1=a)
         ...    .pipe(func, arg2=b, arg3=c)
         ... )  # doctest: +SKIP
-
-        If you have a function that takes the data as (say) the second
-        argument, pass a tuple indicating which keyword expects the
-        data. For example, suppose ``f`` takes its data as ``arg2``:
-
+        
         >>> (df.pipe(h)
         ...    .pipe(g, arg1=a)
         ...    .pipe((func, 'arg2'), arg1=a, arg3=c)
         ...  )  # doctest: +SKIP
-    """
         return com.pipe(self, func, *args, **kwargs)
 
     _shared_docs["aggregate"] = dedent(
@@ -5180,9 +5836,15 @@
                 object.__setattr__(self, name, value)
 
     def _dir_additions(self):
-        """
-        add the string-like attributes from the info_axis.
+        """add the string-like attributes from the info_axis.
         If info_axis is a MultiIndex, it's first level values are used.
+
+        Parameters
+        ----------
+
+        Returns
+        -------
+
         """
         additions = {
             c
@@ -5195,9 +5857,17 @@
     # Consolidation of internals
 
     def _protect_consolidate(self, f):
-        """
-        Consolidate _mgr -- if the blocks have changed, then clear the
+        """Consolidate _mgr -- if the blocks have changed, then clear the
         cache
+
+        Parameters
+        ----------
+        f :
+            
+
+        Returns
+        -------
+
         """
         blocks_before = len(self._mgr.blocks)
         result = f()
@@ -5209,23 +5879,26 @@
         """Consolidate data in place and return None"""
 
         def f():
+            """ """
             self._mgr = self._mgr.consolidate()
 
         self._protect_consolidate(f)
 
     def _consolidate(self, inplace: bool_t = False):
-        """
-        Compute NDFrame with "consolidated" internals (data of each dtype
+        """Compute NDFrame with "consolidated" internals (data of each dtype
         grouped together in a single ndarray).
 
         Parameters
         ----------
         inplace : bool, default False
             If False return new object, otherwise modify existing object.
-
-        Returns
-        -------
-        consolidated : same type as caller
+        inplace: bool_t :
+             (Default value = False)
+
+        Returns
+        -------
+
+        
         """
         inplace = validate_bool_kwarg(inplace, "inplace")
         if inplace:
@@ -5237,11 +5910,22 @@
 
     @property
     def _is_mixed_type(self) -> bool_t:
+        """ """
         f = lambda: self._mgr.is_mixed_type
         return self._protect_consolidate(f)
 
     def _check_inplace_setting(self, value) -> bool_t:
-        """ check whether we allow in-place setting with this type of value """
+        """check whether we allow in-place setting with this type of value
+
+        Parameters
+        ----------
+        value :
+            
+
+        Returns
+        -------
+
+        """
         if self._is_mixed_type:
             if not self._mgr.is_numeric_mixed_type:
 
@@ -5257,9 +5941,11 @@
         return True
 
     def _get_numeric_data(self):
+        """ """
         return self._constructor(self._mgr.get_numeric_data()).__finalize__(self)
 
     def _get_bool_data(self):
+        """ """
         return self._constructor(self._mgr.get_bool_data()).__finalize__(self)
 
     # ----------------------------------------------------------------------
@@ -5267,15 +5953,17 @@
 
     @property
     def values(self) -> np.ndarray:
-        """
-        Return a Numpy representation of the DataFrame.
-
+        """Return a Numpy representation of the DataFrame.
+        
         .. warning::
-
+        
            We recommend using :meth:`DataFrame.to_numpy` instead.
-
+        
         Only the values in the DataFrame will be returned, the axes labels
         will be removed.
+
+        Parameters
+        ----------
 
         Returns
         -------
@@ -5287,24 +5975,26 @@
         DataFrame.to_numpy : Recommended alternative to this method.
         DataFrame.index : Retrieve the index labels.
         DataFrame.columns : Retrieving the column names.
-
         Notes
         -----
         The dtype will be a lower-common-denominator dtype (implicit
         upcasting); that is to say if the dtypes (even of numeric types)
         are mixed, the one that accommodates all will be chosen. Use this
         with care if you are not dealing with the blocks.
-
+        
         e.g. If the dtypes are float16 and float32, dtype will be upcast to
         float32.  If dtypes are int32 and uint8, dtype will be upcast to
         int32. By :func:`numpy.find_common_type` convention, mixing int64
         and uint64 will result in a float64 dtype.
-
         Examples
         --------
         A DataFrame where all columns are the same type (e.g., int64) results
         in an array of the same type.
-
+        
+        
+        A DataFrame with mixed type columns(e.g., str/object, int64, float32)
+        results in an ndarray of the broadest type that accommodates these
+        mixed types (e.g., object).
         >>> df = pd.DataFrame({'age':    [ 3,  29],
         ...                    'height': [94, 170],
         ...                    'weight': [31, 115]})
@@ -5320,11 +6010,7 @@
         >>> df.values
         array([[  3,  94,  31],
                [ 29, 170, 115]])
-
-        A DataFrame with mixed type columns(e.g., str/object, int64, float32)
-        results in an ndarray of the broadest type that accommodates these
-        mixed types (e.g., object).
-
+        
         >>> df2 = pd.DataFrame([('parrot',   24.0, 'second'),
         ...                     ('lion',     80.5, 1),
         ...                     ('monkey', np.nan, None)],
@@ -5349,13 +6035,15 @@
 
     @property
     def dtypes(self):
-        """
-        Return the dtypes in the DataFrame.
-
+        """Return the dtypes in the DataFrame.
+        
         This returns a Series with the data type of each column.
         The result's index is the original DataFrame's columns. Columns
         with mixed types are stored with the ``object`` dtype. See
         :ref:`the User Guide <basics.dtypes>` for more.
+
+        Parameters
+        ----------
 
         Returns
         -------
@@ -5380,10 +6068,19 @@
 
     def _to_dict_of_blocks(self, copy: bool_t = True):
         """
-        Return a dict of dtype -> Constructor Types that
-        each is a homogeneous dtype.
-
-        Internal ONLY
+
+        Parameters
+        ----------
+        copy: bool_t :
+             (Default value = True)
+
+        Returns
+        -------
+        type
+            each is a homogeneous dtype.
+            
+            Internal ONLY
+
         """
         return {
             k: self._constructor(v).__finalize__(self)
@@ -5393,8 +6090,7 @@
     def astype(
         self: FrameOrSeries, dtype, copy: bool_t = True, errors: str = "raise"
     ) -> FrameOrSeries:
-        """
-        Cast a pandas object to a specified dtype ``dtype``.
+        """Cast a pandas object to a specified dtype ``dtype``.
 
         Parameters
         ----------
@@ -5409,13 +6105,19 @@
             pandas objects).
         errors : {'raise', 'ignore'}, default 'raise'
             Control raising of exceptions on invalid data for provided dtype.
-
             - ``raise`` : allow exceptions to be raised
             - ``ignore`` : suppress exceptions. On error return original object.
+        self: FrameOrSeries :
+            
+        copy: bool_t :
+             (Default value = True)
+        errors: str :
+             (Default value = "raise")
 
         Returns
         -------
         casted : same type as caller
+            
 
         See Also
         --------
@@ -5423,34 +6125,52 @@
         to_timedelta : Convert argument to timedelta.
         to_numeric : Convert argument to a numeric type.
         numpy.ndarray.astype : Cast a numpy array to a specified type.
-
         Examples
         --------
         Create a DataFrame:
-
+        
+        
+        Cast all columns to int32:
+        
+        
+        Cast col1 to int32 using a dictionary:
+        
+        
+        Create a series:
+        
+        
+        Convert to categorical type:
+        
+        
+        Convert to ordered categorical type with custom ordering:
+        
+        
+        Note that using ``copy=False`` and changing data on a new
+        pandas object may propagate changes:
+        
+        
+        Create a series of dates:
+        
+        
+        Datetimes are localized to UTC first before
+        converting to the specified timezone:
         >>> d = {'col1': [1, 2], 'col2': [3, 4]}
         >>> df = pd.DataFrame(data=d)
         >>> df.dtypes
         col1    int64
         col2    int64
         dtype: object
-
-        Cast all columns to int32:
-
+        
         >>> df.astype('int32').dtypes
         col1    int32
         col2    int32
         dtype: object
-
-        Cast col1 to int32 using a dictionary:
-
+        
         >>> df.astype({'col1': 'int32'}).dtypes
         col1    int32
         col2    int64
         dtype: object
-
-        Create a series:
-
+        
         >>> ser = pd.Series([1, 2], dtype='int32')
         >>> ser
         0    1
@@ -5460,17 +6180,13 @@
         0    1
         1    2
         dtype: int64
-
-        Convert to categorical type:
-
+        
         >>> ser.astype('category')
         0    1
         1    2
         dtype: category
         Categories (2, int64): [1, 2]
-
-        Convert to ordered categorical type with custom ordering:
-
+        
         >>> cat_dtype = pd.api.types.CategoricalDtype(
         ...     categories=[2, 1], ordered=True)
         >>> ser.astype(cat_dtype)
@@ -5478,10 +6194,7 @@
         1    2
         dtype: category
         Categories (2, int64): [2 < 1]
-
-        Note that using ``copy=False`` and changing data on a new
-        pandas object may propagate changes:
-
+        
         >>> s1 = pd.Series([1, 2])
         >>> s2 = s1.astype('int64', copy=False)
         >>> s2[0] = 10
@@ -5489,19 +6202,14 @@
         0    10
         1     2
         dtype: int64
-
-        Create a series of dates:
-
+        
         >>> ser_date = pd.Series(pd.date_range('20200101', periods=3))
         >>> ser_date
         0   2020-01-01
         1   2020-01-02
         2   2020-01-03
         dtype: datetime64[ns]
-
-        Datetimes are localized to UTC first before
-        converting to the specified timezone:
-
+        
         >>> ser_date.astype('datetime64[ns, US/Eastern]')
         0   2019-12-31 19:00:00-05:00
         1   2020-01-01 19:00:00-05:00
@@ -5556,14 +6264,13 @@
         return result
 
     def copy(self: FrameOrSeries, deep: bool_t = True) -> FrameOrSeries:
-        """
-        Make a copy of this object's indices and data.
-
+        """Make a copy of this object's indices and data.
+        
         When ``deep=True`` (default), a new object will be created with a
         copy of the calling object's data and indices. Modifications to
         the data or indices of the copy will not be reflected in the
         original object (see notes below).
-
+        
         When ``deep=False``, a new object will be created without copying
         the calling object's data or index (only references to the data
         and index are copied). Any changes to the data of the original
@@ -5574,6 +6281,10 @@
         deep : bool, default True
             Make a deep copy, including a copy of the data and the indices.
             With ``deep=False`` neither the indices nor the data are copied.
+        self: FrameOrSeries :
+            
+        deep: bool_t :
+             (Default value = True)
 
         Returns
         -------
@@ -5586,49 +6297,57 @@
         will not be copied recursively, only the reference to the object.
         This is in contrast to `copy.deepcopy` in the Standard Library,
         which recursively copies object data (see examples below).
-
+        
         While ``Index`` objects are copied when ``deep=True``, the underlying
         numpy array is not copied for performance reasons. Since ``Index`` is
         immutable, the underlying data can be safely shared and a copy
         is not needed.
-
         Examples
         --------
+        
+        
+        **Shallow copy versus default (deep) copy:**
+        
+        
+        Shallow copy shares data and index with original.
+        
+        
+        Deep copy has own copy of data and index.
+        
+        
+        Updates to the data shared by shallow copy and original is reflected
+        in both; deep copy remains unchanged.
+        
+        
+        Note that when copying an object containing Python objects, a deep copy
+        will copy the data, but will not do so recursively. Updating a nested
+        data object will be reflected in the deep copy.
         >>> s = pd.Series([1, 2], index=["a", "b"])
         >>> s
         a    1
         b    2
         dtype: int64
-
+        
         >>> s_copy = s.copy()
         >>> s_copy
         a    1
         b    2
         dtype: int64
-
-        **Shallow copy versus default (deep) copy:**
-
+        
         >>> s = pd.Series([1, 2], index=["a", "b"])
         >>> deep = s.copy()
         >>> shallow = s.copy(deep=False)
-
-        Shallow copy shares data and index with original.
-
+        
         >>> s is shallow
         False
         >>> s.values is shallow.values and s.index is shallow.index
         True
-
-        Deep copy has own copy of data and index.
-
+        
         >>> s is deep
         False
         >>> s.values is deep.values or s.index is deep.index
         False
-
-        Updates to the data shared by shallow copy and original is reflected
-        in both; deep copy remains unchanged.
-
+        
         >>> s[0] = 3
         >>> shallow[1] = 4
         >>> s
@@ -5643,11 +6362,7 @@
         a    1
         b    2
         dtype: int64
-
-        Note that when copying an object containing Python objects, a deep copy
-        will copy the data, but will not do so recursively. Updating a nested
-        data object will be reflected in the deep copy.
-
+        
         >>> s = pd.Series([[1, 2], [3, 4]])
         >>> deep = s.copy()
         >>> s[0][0] = 10
@@ -5683,8 +6398,7 @@
         timedelta: bool_t = False,
         coerce: bool_t = False,
     ) -> FrameOrSeries:
-        """
-        Attempt to infer better dtype for object columns
+        """Attempt to infer better dtype for object columns
 
         Parameters
         ----------
@@ -5698,10 +6412,21 @@
         coerce : bool, default False
             If True, force conversion with unconvertible values converted to
             nulls (NaN or NaT).
-
-        Returns
-        -------
-        converted : same as input object
+        self: FrameOrSeries :
+            
+        datetime: bool_t :
+             (Default value = False)
+        numeric: bool_t :
+             (Default value = False)
+        timedelta: bool_t :
+             (Default value = False)
+        coerce: bool_t :
+             (Default value = False)
+
+        Returns
+        -------
+
+        
         """
         validate_bool_kwarg(datetime, "datetime")
         validate_bool_kwarg(numeric, "numeric")
@@ -5718,17 +6443,22 @@
         ).__finalize__(self)
 
     def infer_objects(self: FrameOrSeries) -> FrameOrSeries:
-        """
-        Attempt to infer better dtypes for object columns.
-
+        """Attempt to infer better dtypes for object columns.
+        
         Attempts soft conversion of object-dtyped
         columns, leaving non-object and unconvertible
         columns unchanged. The inference rules are the
         same as during normal Series/DataFrame construction.
 
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
+
         Returns
         -------
         converted : same type as input object
+            
 
         See Also
         --------
@@ -5736,7 +6466,6 @@
         to_timedelta : Convert argument to timedelta.
         to_numeric : Convert argument to numeric type.
         convert_dtypes : Convert argument to best possible dtype.
-
         Examples
         --------
         >>> df = pd.DataFrame({"A": ["a", 1, 2, 3]})
@@ -5746,11 +6475,11 @@
         1  1
         2  2
         3  3
-
+        
         >>> df.dtypes
         A    object
         dtype: object
-
+        
         >>> df.infer_objects().dtypes
         A    int64
         dtype: object
@@ -5771,9 +6500,8 @@
         convert_integer: bool_t = True,
         convert_boolean: bool_t = True,
     ) -> FrameOrSeries:
-        """
-        Convert columns to best possible dtypes using dtypes supporting ``pd.NA``.
-
+        """Convert columns to best possible dtypes using dtypes supporting ``pd.NA``.
+        
         .. versionadded:: 1.0.0
 
         Parameters
@@ -5786,6 +6514,16 @@
             Whether, if possible, conversion can be done to integer extension types.
         convert_boolean : bool, defaults True
             Whether object dtypes should be converted to ``BooleanDtypes()``.
+        self: FrameOrSeries :
+            
+        infer_objects: bool_t :
+             (Default value = True)
+        convert_string: bool_t :
+             (Default value = True)
+        convert_integer: bool_t :
+             (Default value = True)
+        convert_boolean: bool_t :
+             (Default value = True)
 
         Returns
         -------
@@ -5798,7 +6536,6 @@
         to_datetime : Convert argument to datetime.
         to_timedelta : Convert argument to timedelta.
         to_numeric : Convert argument to a numeric type.
-
         Notes
         -----
         By default, ``convert_dtypes`` will attempt to convert a Series (or each
@@ -5806,22 +6543,34 @@
         ``convert_string``, ``convert_integer``, and ``convert_boolean``, it is
         possible to turn off individual conversions to ``StringDtype``, the integer
         extension types or ``BooleanDtype``, respectively.
-
+        
         For object-dtyped columns, if ``infer_objects`` is ``True``, use the inference
         rules as during normal Series/DataFrame construction.  Then, if possible,
         convert to ``StringDtype``, ``BooleanDtype`` or an appropriate integer extension
         type, otherwise leave as ``object``.
-
+        
         If the dtype is integer, convert to an appropriate integer extension type.
-
+        
         If the dtype is numeric, and consists of all integers, convert to an
         appropriate integer extension type.
-
+        
         In the future, as new dtypes are added that support ``pd.NA``, the results
         of this method will change to support those new dtypes.
-
         Examples
         --------
+        
+        Start with a DataFrame with default dtypes.
+        
+        
+        
+        Convert the DataFrame to use best possible dtypes.
+        
+        
+        
+        Start with a Series of strings and missing data represented by ``np.nan``.
+        
+        
+        Obtain a Series with dtype ``StringDtype``.
         >>> df = pd.DataFrame(
         ...     {
         ...         "a": pd.Series([1, 2, 3], dtype=np.dtype("int32")),
@@ -5832,15 +6581,13 @@
         ...         "f": pd.Series([np.nan, 100.5, 200], dtype=np.dtype("float")),
         ...     }
         ... )
-
-        Start with a DataFrame with default dtypes.
-
+        
         >>> df
            a  b      c    d     e      f
         0  1  x   True    h  10.0    NaN
         1  2  y  False    i   NaN  100.5
         2  3  z    NaN  NaN  20.0  200.0
-
+        
         >>> df.dtypes
         a      int32
         b     object
@@ -5849,16 +6596,14 @@
         e    float64
         f    float64
         dtype: object
-
-        Convert the DataFrame to use best possible dtypes.
-
+        
         >>> dfn = df.convert_dtypes()
         >>> dfn
            a  b      c     d     e      f
         0  1  x   True     h    10    NaN
         1  2  y  False     i  <NA>  100.5
         2  3  z   <NA>  <NA>    20  200.0
-
+        
         >>> dfn.dtypes
         a      Int32
         b     string
@@ -5867,18 +6612,14 @@
         e      Int64
         f    float64
         dtype: object
-
-        Start with a Series of strings and missing data represented by ``np.nan``.
-
+        
         >>> s = pd.Series(["a", "b", np.nan])
         >>> s
         0      a
         1      b
         2    NaN
         dtype: object
-
-        Obtain a Series with dtype ``StringDtype``.
-
+        
         >>> s.convert_dtypes()
         0       a
         1       b
@@ -5912,8 +6653,7 @@
         limit=None,
         downcast=None,
     ) -> Optional[FrameOrSeries]:
-        """
-        Fill NA/NaN values using the specified method.
+        """Fill NA/NaN values using the specified method.
 
         Parameters
         ----------
@@ -5922,13 +6662,13 @@
             dict/Series/DataFrame of values specifying which value to use for
             each index (for a Series) or column (for a DataFrame).  Values not
             in the dict/Series/DataFrame will not be filled. This value cannot
-            be a list.
+            be a list. (Default value = None)
         method : {{'backfill', 'bfill', 'pad', 'ffill', None}}, default None
             Method to use for filling holes in reindexed Series
             pad / ffill: propagate last valid observation forward to next valid
-            backfill / bfill: use next valid observation to fill gap.
+            backfill / bfill: use next valid observation to fill gap. (Default value = None)
         axis : {axes_single_arg}
-            Axis along which to fill missing values.
+            Axis along which to fill missing values. (Default value = None)
         inplace : bool, default False
             If True, fill in-place. Note: this will modify any
             other views on this object (e.g., a no-copy slice for a column in a
@@ -5939,11 +6679,15 @@
             a gap with more than this number of consecutive NaNs, it will only
             be partially filled. If method is not specified, this is the
             maximum number of entries along the entire axis where NaNs will be
-            filled. Must be greater than 0 if not None.
+            filled. Must be greater than 0 if not None. (Default value = None)
         downcast : dict, default is None
             A dict of item->dtype of what to downcast if possible,
             or the string 'infer' which will try to downcast to an appropriate
-            equal type (e.g. float64 to int64 if possible).
+            equal type (e.g. float64 to int64 if possible). (Default value = None)
+        self: FrameOrSeries :
+            
+        inplace: bool_t :
+             (Default value = False)
 
         Returns
         -------
@@ -5955,9 +6699,20 @@
         interpolate : Fill NaN values using interpolation.
         reindex : Conform object to new index.
         asfreq : Convert TimeSeries to specified frequency.
-
         Examples
         --------
+        
+        Replace all NaN elements with 0s.
+        
+        
+        We can also propagate non-null values forward or backward.
+        
+        
+        Replace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1,
+        2, and 3 respectively.
+        
+        
+        Only replace the first NaN element.
         >>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],
         ...                    [3, 4, np.nan, 1],
         ...                    [np.nan, np.nan, np.nan, 5],
@@ -5969,28 +6724,21 @@
         1  3.0  4.0 NaN  1
         2  NaN  NaN NaN  5
         3  NaN  3.0 NaN  4
-
-        Replace all NaN elements with 0s.
-
+        
         >>> df.fillna(0)
             A   B   C   D
         0   0.0 2.0 0.0 0
         1   3.0 4.0 0.0 1
         2   0.0 0.0 0.0 5
         3   0.0 3.0 0.0 4
-
-        We can also propagate non-null values forward or backward.
-
+        
         >>> df.fillna(method='ffill')
             A   B   C   D
         0   NaN 2.0 NaN 0
         1   3.0 4.0 NaN 1
         2   3.0 4.0 NaN 5
         3   3.0 3.0 NaN 4
-
-        Replace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1,
-        2, and 3 respectively.
-
+        
         >>> values = {{'A': 0, 'B': 1, 'C': 2, 'D': 3}}
         >>> df.fillna(value=values)
             A   B   C   D
@@ -5998,9 +6746,7 @@
         1   3.0 4.0 2.0 1
         2   0.0 1.0 2.0 5
         3   0.0 3.0 2.0 4
-
-        Only replace the first NaN element.
-
+        
         >>> df.fillna(value=values, limit=1)
             A   B   C   D
         0   0.0 2.0 2.0 0
@@ -6096,13 +6842,25 @@
         limit=None,
         downcast=None,
     ) -> Optional[FrameOrSeries]:
-        """
-        Synonym for :meth:`DataFrame.fillna` with ``method='ffill'``.
-
-        Returns
-        -------
-        {klass} or None
-            Object with missing values filled or None if ``inplace=True``.
+        """Synonym for :meth:`DataFrame.fillna` with ``method='ffill'``.
+
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
+        axis :
+             (Default value = None)
+        inplace: bool_t :
+             (Default value = False)
+        limit :
+             (Default value = None)
+        downcast :
+             (Default value = None)
+
+        Returns
+        -------
+
+        
         """
         return self.fillna(
             method="ffill", axis=axis, inplace=inplace, limit=limit, downcast=downcast
@@ -6117,13 +6875,25 @@
         limit=None,
         downcast=None,
     ) -> Optional[FrameOrSeries]:
-        """
-        Synonym for :meth:`DataFrame.fillna` with ``method='bfill'``.
-
-        Returns
-        -------
-        {klass} or None
-            Object with missing values filled or None if ``inplace=True``.
+        """Synonym for :meth:`DataFrame.fillna` with ``method='bfill'``.
+
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
+        axis :
+             (Default value = None)
+        inplace: bool_t :
+             (Default value = False)
+        limit :
+             (Default value = None)
+        downcast :
+             (Default value = None)
+
+        Returns
+        -------
+
+        
         """
         return self.fillna(
             method="bfill", axis=axis, inplace=inplace, limit=limit, downcast=downcast
@@ -6141,9 +6911,8 @@
         regex=False,
         method="pad",
     ):
-        """
-        Replace values given in `to_replace` with `value`.
-
+        """Replace values given in `to_replace` with `value`.
+        
         Values of the {klass} are replaced with other values dynamically.
         This differs from updating with ``.loc`` or ``.iloc``, which require
         you to specify a location to update with some value.
@@ -6152,81 +6921,71 @@
         ----------
         to_replace : str, regex, list, dict, Series, int, float, or None
             How to find the values that will be replaced.
-
             * numeric, str or regex:
-
-                - numeric: numeric values equal to `to_replace` will be
-                  replaced with `value`
-                - str: string exactly matching `to_replace` will be replaced
-                  with `value`
-                - regex: regexs matching `to_replace` will be replaced with
-                  `value`
-
+            - numeric: numeric values equal to `to_replace` will be
+            replaced with `value`
+            - str: string exactly matching `to_replace` will be replaced
+            with `value`
+            - regex: regexs matching `to_replace` will be replaced with
+            `value`
             * list of str, regex, or numeric:
-
-                - First, if `to_replace` and `value` are both lists, they
-                  **must** be the same length.
-                - Second, if ``regex=True`` then all of the strings in **both**
-                  lists will be interpreted as regexs otherwise they will match
-                  directly. This doesn't matter much for `value` since there
-                  are only a few possible substitution regexes you can use.
-                - str, regex and numeric rules apply as above.
-
+            - First, if `to_replace` and `value` are both lists, they
+            **must** be the same length.
+            - Second, if ``regex=True`` then all of the strings in **both**
+            lists will be interpreted as regexs otherwise they will match
+            directly. This doesn't matter much for `value` since there
+            are only a few possible substitution regexes you can use.
+            - str, regex and numeric rules apply as above.
             * dict:
-
-                - Dicts can be used to specify different replacement values
-                  for different existing values. For example,
-                  ``{{'a': 'b', 'y': 'z'}}`` replaces the value 'a' with 'b' and
-                  'y' with 'z'. To use a dict in this way the `value`
-                  parameter should be `None`.
-                - For a DataFrame a dict can specify that different values
-                  should be replaced in different columns. For example,
-                  ``{{'a': 1, 'b': 'z'}}`` looks for the value 1 in column 'a'
-                  and the value 'z' in column 'b' and replaces these values
-                  with whatever is specified in `value`. The `value` parameter
-                  should not be ``None`` in this case. You can treat this as a
-                  special case of passing two lists except that you are
-                  specifying the column to search in.
-                - For a DataFrame nested dictionaries, e.g.,
-                  ``{{'a': {{'b': np.nan}}}}``, are read as follows: look in column
-                  'a' for the value 'b' and replace it with NaN. The `value`
-                  parameter should be ``None`` to use a nested dict in this
-                  way. You can nest regular expressions as well. Note that
-                  column names (the top-level dictionary keys in a nested
-                  dictionary) **cannot** be regular expressions.
-
+            - Dicts can be used to specify different replacement values
+            for different existing values. For example,
+            ``{{'a': 'b', 'y': 'z'}}`` replaces the value 'a' with 'b' and
+            'y' with 'z'. To use a dict in this way the `value`
+            parameter should be `None`.
+            - For a DataFrame a dict can specify that different values
+            should be replaced in different columns. For example,
+            ``{{'a': 1, 'b': 'z'}}`` looks for the value 1 in column 'a'
+            and the value 'z' in column 'b' and replaces these values
+            with whatever is specified in `value`. The `value` parameter
+            should not be ``None`` in this case. You can treat this as a
+            special case of passing two lists except that you are
+            specifying the column to search in.
+            - For a DataFrame nested dictionaries, e.g.,
+            ``{{'a': {{'b': np.nan}}}}``, are read as follows: look in column
+            'a' for the value 'b' and replace it with NaN. The `value`
+            parameter should be ``None`` to use a nested dict in this
+            way. You can nest regular expressions as well. Note that
+            column names (the top-level dictionary keys in a nested
+            dictionary) **cannot** be regular expressions.
             * None:
-
-                - This means that the `regex` argument must be a string,
-                  compiled regular expression, or list, dict, ndarray or
-                  Series of such elements. If `value` is also ``None`` then
-                  this **must** be a nested dictionary or Series.
-
-            See the examples section for examples of each of these.
+            - This means that the `regex` argument must be a string,
+            compiled regular expression, or list, dict, ndarray or
+            Series of such elements. If `value` is also ``None`` then
+            this **must** be a nested dictionary or Series.
+            See the examples section for examples of each of these. (Default value = None)
         value : scalar, dict, list, str, regex, default None
             Value to replace any values matching `to_replace` with.
             For a DataFrame a dict of values can be used to specify which
             value to use for each column (columns not in the dict will not be
             filled). Regular expressions, strings and lists or dicts of such
-            objects are also allowed.
+            objects are also allowed. (Default value = None)
         inplace : bool, default False
             If True, in place. Note: this will modify any
             other views on this object (e.g. a column from a DataFrame).
-            Returns the caller if this is True.
+            Returns the caller if this is True. (Default value = False)
         limit : int, default None
-            Maximum size gap to forward or backward fill.
+            Maximum size gap to forward or backward fill. (Default value = None)
         regex : bool or same types as `to_replace`, default False
             Whether to interpret `to_replace` and/or `value` as regular
             expressions. If this is ``True`` then `to_replace` *must* be a
             string. Alternatively, this could be a regular expression or a
             list, dict, or array of regular expressions in which case
-            `to_replace` must be ``None``.
+            `to_replace` must be ``None``. (Default value = False)
         method : {{'pad', 'ffill', 'bfill', `None`}}
             The method to use when for replacement, when `to_replace` is a
             scalar, list or tuple and `value` is ``None``.
-
             .. versionchanged:: 0.23.0
-                Added to DataFrame.
+            Added to DataFrame. (Default value = "pad")
 
         Returns
         -------
@@ -6237,29 +6996,26 @@
         ------
         AssertionError
             * If `regex` is not a ``bool`` and `to_replace` is not
-              ``None``.
-
+            ``None``.
         TypeError
             * If `to_replace` is not a scalar, array-like, ``dict``, or ``None``
             * If `to_replace` is a ``dict`` and `value` is not a ``list``,
-              ``dict``, ``ndarray``, or ``Series``
+            ``dict``, ``ndarray``, or ``Series``
             * If `to_replace` is ``None`` and `regex` is not compilable
-              into a regular expression or is a list, dict, ndarray, or
-              Series.
+            into a regular expression or is a list, dict, ndarray, or
+            Series.
             * When replacing multiple ``bool`` or ``datetime64`` objects and
-              the arguments to `to_replace` does not match the type of the
-              value being replaced
-
+            the arguments to `to_replace` does not match the type of the
+            value being replaced
         ValueError
             * If a ``list`` or an ``ndarray`` is passed to `to_replace` and
-              `value` but they are not the same length.
+            `value` but they are not the same length.
 
         See Also
         --------
         {klass}.fillna : Fill NA values.
         {klass}.where : Replace values based on boolean condition.
         Series.str.replace : Simple string replacement.
-
         Notes
         -----
         * Regex substitution is performed under the hood with ``re.sub``. The
@@ -6274,12 +7030,55 @@
         * When dict is used as the `to_replace` value, it is like
           key(s) in the dict are the to_replace part and
           value(s) in the dict are the value parameter.
-
         Examples
         --------
-
+        
         **Scalar `to_replace` and `value`**
-
+        
+        
+        
+        **List-like `to_replace`**
+        
+        
+        
+        
+        **dict-like `to_replace`**
+        
+        
+        
+        
+        **Regular expression `to_replace`**
+        
+        
+        
+        
+        
+        
+        Note that when replacing multiple ``bool`` or ``datetime64`` objects,
+        the data types in the `to_replace` parameter must match the data
+        type of the value being replaced:
+        
+        
+        This raises a ``TypeError`` because one of the ``dict`` keys is not of
+        the correct type for replacement.
+        
+        Compare the behavior of ``s.replace({{'a': None}})`` and
+        ``s.replace('a', None)`` to understand the peculiarities
+        of the `to_replace` parameter:
+        
+        
+        When one uses a dict as the `to_replace` value, it is like the
+        value(s) in the dict are equal to the `value` parameter.
+        ``s.replace({{'a': None}})`` is equivalent to
+        ``s.replace(to_replace={{'a': None}}, value=None, method=None)``:
+        
+        
+        When ``value=None`` and `to_replace` is a scalar, list or
+        tuple, `replace` uses the method parameter (default 'pad') to do the
+        replacement. So this is why the 'a' values are being replaced by 10
+        in rows 1 and 2 and 'b' in row 4 in this case.
+        The command ``s.replace('a', None)`` is actually equivalent to
+        ``s.replace(to_replace='a', value=None, method='pad')``:
         >>> s = pd.Series([0, 1, 2, 3, 4])
         >>> s.replace(0, 5)
         0    5
@@ -6288,7 +7087,7 @@
         3    3
         4    4
         dtype: int64
-
+        
         >>> df = pd.DataFrame({{'A': [0, 1, 2, 3, 4],
         ...                    'B': [5, 6, 7, 8, 9],
         ...                    'C': ['a', 'b', 'c', 'd', 'e']}})
@@ -6299,9 +7098,7 @@
         2  2  7  c
         3  3  8  d
         4  4  9  e
-
-        **List-like `to_replace`**
-
+        
         >>> df.replace([0, 1, 2, 3], 4)
            A  B  C
         0  4  5  a
@@ -6309,7 +7106,7 @@
         2  4  7  c
         3  4  8  d
         4  4  9  e
-
+        
         >>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])
            A  B  C
         0  4  5  a
@@ -6317,7 +7114,7 @@
         2  2  7  c
         3  1  8  d
         4  4  9  e
-
+        
         >>> s.replace([1, 2], method='bfill')
         0    0
         1    3
@@ -6325,9 +7122,7 @@
         3    3
         4    4
         dtype: int64
-
-        **dict-like `to_replace`**
-
+        
         >>> df.replace({{0: 10, 1: 100}})
              A  B  C
         0   10  5  a
@@ -6335,7 +7130,7 @@
         2    2  7  c
         3    3  8  d
         4    4  9  e
-
+        
         >>> df.replace({{'A': 0, 'B': 5}}, 100)
              A    B  C
         0  100  100  a
@@ -6343,7 +7138,7 @@
         2    2    7  c
         3    3    8  d
         4    4    9  e
-
+        
         >>> df.replace({{'A': {{0: 100, 4: 400}}}})
              A  B  C
         0  100  5  a
@@ -6351,9 +7146,7 @@
         2    2  7  c
         3    3  8  d
         4  400  9  e
-
-        **Regular expression `to_replace`**
-
+        
         >>> df = pd.DataFrame({{'A': ['bat', 'foo', 'bait'],
         ...                    'B': ['abc', 'bar', 'xyz']}})
         >>> df.replace(to_replace=r'^ba.$', value='new', regex=True)
@@ -6361,56 +7154,40 @@
         0   new  abc
         1   foo  new
         2  bait  xyz
-
+        
         >>> df.replace({{'A': r'^ba.$'}}, {{'A': 'new'}}, regex=True)
               A    B
         0   new  abc
         1   foo  bar
         2  bait  xyz
-
+        
         >>> df.replace(regex=r'^ba.$', value='new')
               A    B
         0   new  abc
         1   foo  new
         2  bait  xyz
-
+        
         >>> df.replace(regex={{r'^ba.$': 'new', 'foo': 'xyz'}})
               A    B
         0   new  abc
         1   xyz  new
         2  bait  xyz
-
+        
         >>> df.replace(regex=[r'^ba.$', 'foo'], value='new')
               A    B
         0   new  abc
         1   new  new
         2  bait  xyz
-
-        Note that when replacing multiple ``bool`` or ``datetime64`` objects,
-        the data types in the `to_replace` parameter must match the data
-        type of the value being replaced:
-
+        
         >>> df = pd.DataFrame({{'A': [True, False, True],
         ...                    'B': [False, True, False]}})
         >>> df.replace({{'a string': 'new value', True: False}})  # raises
         Traceback (most recent call last):
             ...
         TypeError: Cannot compare types 'ndarray(dtype=bool)' and 'str'
-
-        This raises a ``TypeError`` because one of the ``dict`` keys is not of
-        the correct type for replacement.
-
-        Compare the behavior of ``s.replace({{'a': None}})`` and
-        ``s.replace('a', None)`` to understand the peculiarities
-        of the `to_replace` parameter:
-
+        
         >>> s = pd.Series([10, 'a', 'a', 'b', 'a'])
-
-        When one uses a dict as the `to_replace` value, it is like the
-        value(s) in the dict are equal to the `value` parameter.
-        ``s.replace({{'a': None}})`` is equivalent to
-        ``s.replace(to_replace={{'a': None}}, value=None, method=None)``:
-
+        
         >>> s.replace({{'a': None}})
         0      10
         1    None
@@ -6418,14 +7195,7 @@
         3       b
         4    None
         dtype: object
-
-        When ``value=None`` and `to_replace` is a scalar, list or
-        tuple, `replace` uses the method parameter (default 'pad') to do the
-        replacement. So this is why the 'a' values are being replaced by 10
-        in rows 1 and 2 and 'b' in row 4 in this case.
-        The command ``s.replace('a', None)`` is actually equivalent to
-        ``s.replace(to_replace='a', value=None, method='pad')``:
-
+        
         >>> s.replace('a', None)
         0    10
         1    10
@@ -6433,7 +7203,7 @@
         3     b
         4     b
         dtype: object
-    """
+        """
         if not (
             is_scalar(to_replace)
             or is_re_compilable(to_replace)
@@ -6604,34 +7374,32 @@
         downcast: Optional[str] = None,
         **kwargs,
     ) -> Optional[FrameOrSeries]:
-        """
-        Please note that only ``method='linear'`` is supported for
+        """Please note that only ``method='linear'`` is supported for
         DataFrame/Series with a MultiIndex.
 
         Parameters
         ----------
         method : str, default 'linear'
             Interpolation technique to use. One of:
-
             * 'linear': Ignore the index and treat the values as equally
-              spaced. This is the only method supported on MultiIndexes.
+            spaced. This is the only method supported on MultiIndexes.
             * 'time': Works on daily and higher resolution data to interpolate
-              given length of interval.
+            given length of interval.
             * 'index', 'values': use the actual numerical values of the index.
             * 'pad': Fill in NaNs using existing values.
             * 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'spline',
-              'barycentric', 'polynomial': Passed to
-              `scipy.interpolate.interp1d`. These methods use the numerical
-              values of the index.  Both 'polynomial' and 'spline' require that
-              you also specify an `order` (int), e.g.
-              ``df.interpolate(method='polynomial', order=5)``.
+            'barycentric', 'polynomial': Passed to
+            `scipy.interpolate.interp1d`. These methods use the numerical
+            values of the index.  Both 'polynomial' and 'spline' require that
+            you also specify an `order` (int), e.g.
+            ``df.interpolate(method='polynomial', order=5)``.
             * 'krogh', 'piecewise_polynomial', 'spline', 'pchip', 'akima',
-              'cubicspline': Wrappers around the SciPy interpolation methods of
-              similar names. See `Notes`.
+            'cubicspline': Wrappers around the SciPy interpolation methods of
+            similar names. See `Notes`.
             * 'from_derivatives': Refers to
-              `scipy.interpolate.BPoly.from_derivatives` which
-              replaces 'piecewise_polynomial' interpolation method in
-              scipy 0.18.
+            `scipy.interpolate.BPoly.from_derivatives` which
+            replaces 'piecewise_polynomial' interpolation method in
+            scipy 0.18.
         axis : {{0 or 'index', 1 or 'columns', None}}, default None
             Axis to interpolate along.
         limit : int, optional
@@ -6641,37 +7409,46 @@
             Update the data in place if possible.
         limit_direction : {{'forward', 'backward', 'both'}}, Optional
             Consecutive NaNs will be filled in this direction.
-
             If limit is specified:
-                * If 'method' is 'pad' or 'ffill', 'limit_direction' must be 'forward'.
-                * If 'method' is 'backfill' or 'bfill', 'limit_direction' must be
-                  'backwards'.
-
+            * If 'method' is 'pad' or 'ffill', 'limit_direction' must be 'forward'.
+            * If 'method' is 'backfill' or 'bfill', 'limit_direction' must be
+            'backwards'.
             If 'limit' is not specified:
-                * If 'method' is 'backfill' or 'bfill', the default is 'backward'
-                * else the default is 'forward'
-
+            * If 'method' is 'backfill' or 'bfill', the default is 'backward'
+            * else the default is 'forward'
             .. versionchanged:: 1.1.0
-                raises ValueError if `limit_direction` is 'forward' or 'both' and
-                    method is 'backfill' or 'bfill'.
-                raises ValueError if `limit_direction` is 'backward' or 'both' and
-                    method is 'pad' or 'ffill'.
-
+            raises ValueError if `limit_direction` is 'forward' or 'both' and
+            method is 'backfill' or 'bfill'.
+            raises ValueError if `limit_direction` is 'backward' or 'both' and
+            method is 'pad' or 'ffill'.
         limit_area : {{`None`, 'inside', 'outside'}}, default None
             If limit is specified, consecutive NaNs will be filled with this
             restriction.
-
             * ``None``: No fill restriction.
             * 'inside': Only fill NaNs surrounded by valid values
-              (interpolate).
+            (interpolate).
             * 'outside': Only fill NaNs outside valid values (extrapolate).
-
             .. versionadded:: 0.23.0
-
         downcast : optional, 'infer' or None, defaults to None
             Downcast dtypes if possible.
-        **kwargs
+        **kwargs :
             Keyword arguments to pass on to the interpolating function.
+        self: FrameOrSeries :
+            
+        method: str :
+             (Default value = "linear")
+        axis: Axis :
+             (Default value = 0)
+        limit: Optional[int] :
+             (Default value = None)
+        inplace: bool_t :
+             (Default value = False)
+        limit_direction: Optional[str] :
+             (Default value = None)
+        limit_area: Optional[str] :
+             (Default value = None)
+        downcast: Optional[str] :
+             (Default value = None)
 
         Returns
         -------
@@ -6692,7 +7469,6 @@
         scipy.interpolate.PchipInterpolator : PCHIP 1-d monotonic cubic
             interpolation.
         scipy.interpolate.CubicSpline : Cubic spline data interpolator.
-
         Notes
         -----
         The 'krogh', 'piecewise_polynomial', 'spline', 'pchip' and 'akima'
@@ -6703,12 +7479,31 @@
         <https://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation>`__
         and `SciPy tutorial
         <https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html>`__.
-
         Examples
         --------
         Filling in ``NaN`` in a :class:`~pandas.Series` via linear
         interpolation.
-
+        
+        
+        Filling in ``NaN`` in a Series by padding, but filling at most two
+        consecutive ``NaN`` at a time.
+        
+        
+        Filling in ``NaN`` in a Series via polynomial interpolation or splines:
+        Both 'polynomial' and 'spline' methods require that you also specify
+        an ``order`` (int).
+        
+        
+        Fill the DataFrame forward (that is, going down) along each column
+        using linear interpolation.
+        
+        Note how the last entry in column 'a' is interpolated differently,
+        because there is no entry after it to use for interpolation.
+        Note how the first entry in column 'b' remains ``NaN``, because there
+        is no entry before it to use for interpolation.
+        
+        
+        Using polynomial interpolation.
         >>> s = pd.Series([0, 1, np.nan, 3])
         >>> s
         0    0.0
@@ -6722,10 +7517,7 @@
         2    2.0
         3    3.0
         dtype: float64
-
-        Filling in ``NaN`` in a Series by padding, but filling at most two
-        consecutive ``NaN`` at a time.
-
+        
         >>> s = pd.Series([np.nan, "single_one", np.nan,
         ...                "fill_two_more", np.nan, np.nan, np.nan,
         ...                4.71, np.nan])
@@ -6751,11 +7543,7 @@
         7             4.71
         8             4.71
         dtype: object
-
-        Filling in ``NaN`` in a Series via polynomial interpolation or splines:
-        Both 'polynomial' and 'spline' methods require that you also specify
-        an ``order`` (int).
-
+        
         >>> s = pd.Series([0, 2, np.nan, 8])
         >>> s.interpolate(method='polynomial', order=2)
         0    0.000000
@@ -6763,15 +7551,7 @@
         2    4.666667
         3    8.000000
         dtype: float64
-
-        Fill the DataFrame forward (that is, going down) along each column
-        using linear interpolation.
-
-        Note how the last entry in column 'a' is interpolated differently,
-        because there is no entry after it to use for interpolation.
-        Note how the first entry in column 'b' remains ``NaN``, because there
-        is no entry before it to use for interpolation.
-
+        
         >>> df = pd.DataFrame([(0.0, np.nan, -1.0, 1.0),
         ...                    (np.nan, 2.0, np.nan, np.nan),
         ...                    (2.0, 3.0, np.nan, 9.0),
@@ -6789,9 +7569,7 @@
         1  1.0  2.0 -2.0   5.0
         2  2.0  3.0 -3.0   9.0
         3  2.0  4.0 -4.0  16.0
-
-        Using polynomial interpolation.
-
+        
         >>> df['d'].interpolate(method='polynomial', order=2)
         0     1.0
         1     4.0
@@ -6891,14 +7669,13 @@
     # Timeseries methods Methods
 
     def asof(self, where, subset=None):
-        """
-        Return the last row(s) without any NaNs before `where`.
-
+        """Return the last row(s) without any NaNs before `where`.
+        
         The last row (for each element in `where`, if list) without any
         NaN is taken.
         In case of a :class:`~pandas.DataFrame`, the last row without NaN
         considering only the subset of columns (if not `None`)
-
+        
         If there is no good value, NaN is returned for a Series or
         a Series of NaN values for a DataFrame
 
@@ -6908,34 +7685,44 @@
             Date(s) before which the last row(s) are returned.
         subset : str or array-like of str, default `None`
             For DataFrame, if not `None`, only use these columns to
-            check for NaNs.
+            check for NaNs. (Default value = None)
 
         Returns
         -------
         scalar, Series, or DataFrame
-
             The return can be:
-
             * scalar : when `self` is a Series and `where` is a scalar
             * Series: when `self` is a Series and `where` is an array-like,
-              or when `self` is a DataFrame and `where` is a scalar
+            or when `self` is a DataFrame and `where` is a scalar
             * DataFrame : when `self` is a DataFrame and `where` is an
-              array-like
-
+            array-like
             Return scalar, Series, or DataFrame.
 
         See Also
         --------
         merge_asof : Perform an asof merge. Similar to left join.
-
         Notes
         -----
         Dates are assumed to be sorted. Raises if this is not the case.
-
         Examples
         --------
         A Series and a scalar `where`.
-
+        
+        
+        
+        For a sequence `where`, a Series is returned. The first value is
+        NaN, because the first element of `where` is before the first
+        index value.
+        
+        
+        Missing values are not considered. The following is ``2.0``, not
+        NaN, even though NaN is at the index location for ``30``.
+        
+        
+        Take all columns into consideration
+        
+        
+        Take a single column into consideration
         >>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])
         >>> s
         10    1.0
@@ -6943,27 +7730,18 @@
         30    NaN
         40    4.0
         dtype: float64
-
+        
         >>> s.asof(20)
         2.0
-
-        For a sequence `where`, a Series is returned. The first value is
-        NaN, because the first element of `where` is before the first
-        index value.
-
+        
         >>> s.asof([5, 20])
         5     NaN
         20    2.0
         dtype: float64
-
-        Missing values are not considered. The following is ``2.0``, not
-        NaN, even though NaN is at the index location for ``30``.
-
+        
         >>> s.asof(30)
         2.0
-
-        Take all columns into consideration
-
+        
         >>> df = pd.DataFrame({'a': [10, 20, 30, 40, 50],
         ...                    'b': [None, None, None, None, 500]},
         ...                   index=pd.DatetimeIndex(['2018-02-27 09:01:00',
@@ -6976,9 +7754,7 @@
                               a   b
         2018-02-27 09:03:30 NaN NaN
         2018-02-27 09:04:30 NaN NaN
-
-        Take a single column into consideration
-
+        
         >>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',
         ...                           '2018-02-27 09:04:30']),
         ...         subset=['a'])
@@ -7059,9 +7835,8 @@
 
     @doc(klass=_shared_doc_kwargs["klass"])
     def isna(self: FrameOrSeries) -> FrameOrSeries:
-        """
-        Detect missing values.
-
+        """Detect missing values.
+        
         Return a boolean same-sized object indicating if the values are NA.
         NA values, such as None or :attr:`numpy.NaN`, gets mapped to True
         values.
@@ -7069,6 +7844,11 @@
         strings ``''`` or :attr:`numpy.inf` are not considered NA values
         (unless you set ``pandas.options.mode.use_inf_as_na = True``).
 
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
+
         Returns
         -------
         {klass}
@@ -7081,11 +7861,13 @@
         {klass}.notna : Boolean inverse of isna.
         {klass}.dropna : Omit axes labels with missing values.
         isna : Top-level isna.
-
         Examples
         --------
         Show which entries in a DataFrame are NA.
-
+        
+        
+        
+        Show which entries in a Series are NA.
         >>> df = pd.DataFrame({{'age': [5, 6, np.NaN],
         ...                    'born': [pd.NaT, pd.Timestamp('1939-05-27'),
         ...                             pd.Timestamp('1940-04-25')],
@@ -7096,22 +7878,20 @@
         0  5.0        NaT  Alfred       None
         1  6.0 1939-05-27  Batman  Batmobile
         2  NaN 1940-04-25              Joker
-
+        
         >>> df.isna()
              age   born   name    toy
         0  False   True  False   True
         1  False  False  False  False
         2   True  False  False  False
-
-        Show which entries in a Series are NA.
-
+        
         >>> ser = pd.Series([5, 6, np.NaN])
         >>> ser
         0    5.0
         1    6.0
         2    NaN
         dtype: float64
-
+        
         >>> ser.isna()
         0    False
         1    False
@@ -7122,13 +7902,23 @@
 
     @doc(isna, klass=_shared_doc_kwargs["klass"])
     def isnull(self: FrameOrSeries) -> FrameOrSeries:
+        """
+
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
+
+        Returns
+        -------
+
+        """
         return isna(self).__finalize__(self, method="isnull")
 
     @doc(klass=_shared_doc_kwargs["klass"])
     def notna(self: FrameOrSeries) -> FrameOrSeries:
-        """
-        Detect existing (non-missing) values.
-
+        """Detect existing (non-missing) values.
+        
         Return a boolean same-sized object indicating if the values are not NA.
         Non-missing values get mapped to True. Characters such as empty
         strings ``''`` or :attr:`numpy.inf` are not considered NA values
@@ -7136,6 +7926,11 @@
         NA values, such as None or :attr:`numpy.NaN`, get mapped to False
         values.
 
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
+
         Returns
         -------
         {klass}
@@ -7148,11 +7943,13 @@
         {klass}.isna : Boolean inverse of notna.
         {klass}.dropna : Omit axes labels with missing values.
         notna : Top-level notna.
-
         Examples
         --------
         Show which entries in a DataFrame are not NA.
-
+        
+        
+        
+        Show which entries in a Series are not NA.
         >>> df = pd.DataFrame({{'age': [5, 6, np.NaN],
         ...                    'born': [pd.NaT, pd.Timestamp('1939-05-27'),
         ...                             pd.Timestamp('1940-04-25')],
@@ -7163,22 +7960,20 @@
         0  5.0        NaT  Alfred       None
         1  6.0 1939-05-27  Batman  Batmobile
         2  NaN 1940-04-25              Joker
-
+        
         >>> df.notna()
              age   born  name    toy
         0   True  False  True  False
         1   True   True  True   True
         2  False   True  True   True
-
-        Show which entries in a Series are not NA.
-
+        
         >>> ser = pd.Series([5, 6, np.NaN])
         >>> ser
         0    5.0
         1    6.0
         2    NaN
         dtype: float64
-
+        
         >>> ser.notna()
         0     True
         1     True
@@ -7189,9 +7984,35 @@
 
     @doc(notna, klass=_shared_doc_kwargs["klass"])
     def notnull(self: FrameOrSeries) -> FrameOrSeries:
+        """
+
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
+
+        Returns
+        -------
+
+        """
         return notna(self).__finalize__(self, method="notnull")
 
     def _clip_with_scalar(self, lower, upper, inplace: bool_t = False):
+        """
+
+        Parameters
+        ----------
+        lower :
+            
+        upper :
+            
+        inplace: bool_t :
+             (Default value = False)
+
+        Returns
+        -------
+
+        """
         if (lower is not None and np.any(isna(lower))) or (
             upper is not None and np.any(isna(upper))
         ):
@@ -7217,6 +8038,23 @@
             return result
 
     def _clip_with_one_bound(self, threshold, method, axis, inplace):
+        """
+
+        Parameters
+        ----------
+        threshold :
+            
+        method :
+            
+        axis :
+            
+        inplace :
+            
+
+        Returns
+        -------
+
+        """
 
         if axis is not None:
             axis = self._get_axis_number(axis)
@@ -7248,9 +8086,8 @@
         *args,
         **kwargs,
     ) -> FrameOrSeries:
-        """
-        Trim values at input threshold(s).
-
+        """Trim values at input threshold(s).
+        
         Assigns values outside boundary to boundary values. Thresholds
         can be singular values or array like, and in the latter case
         the clipping is performed element-wise in the specified axis.
@@ -7259,17 +8096,25 @@
         ----------
         lower : float or array_like, default None
             Minimum threshold value. All values below this
-            threshold will be set to it.
+            threshold will be set to it. (Default value = None)
         upper : float or array_like, default None
             Maximum threshold value. All values above this
-            threshold will be set to it.
+            threshold will be set to it. (Default value = None)
         axis : int or str axis name, optional
-            Align object with lower and upper along the given axis.
+            Align object with lower and upper along the given axis. (Default value = None)
         inplace : bool, default False
             Whether to perform the operation in place on the data.
-        *args, **kwargs
+        *args, **kwargs :
             Additional keywords have no effect but might be accepted
             for compatibility with numpy.
+        self: FrameOrSeries :
+            
+        inplace: bool_t :
+             (Default value = False)
+        *args :
+            
+        **kwargs :
+            
 
         Returns
         -------
@@ -7282,9 +8127,13 @@
         Series.clip : Trim values at input threshold in series.
         DataFrame.clip : Trim values at input threshold in dataframe.
         numpy.clip : Clip (limit) the values in an array.
-
         Examples
         --------
+        
+        Clips per column using lower and upper thresholds:
+        
+        
+        Clips using specific lower and upper thresholds per column element:
         >>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}
         >>> df = pd.DataFrame(data)
         >>> df
@@ -7294,9 +8143,7 @@
         2      0      6
         3     -1      8
         4      5     -5
-
-        Clips per column using lower and upper thresholds:
-
+        
         >>> df.clip(-4, 6)
            col_0  col_1
         0      6     -2
@@ -7304,9 +8151,7 @@
         2      0      6
         3     -1      6
         4      5     -4
-
-        Clips using specific lower and upper thresholds per column element:
-
+        
         >>> t = pd.Series([2, -4, -1, 6, 3])
         >>> t
         0    2
@@ -7315,7 +8160,7 @@
         3    6
         4    3
         dtype: int64
-
+        
         >>> df.clip(t, t + 4, axis=0)
            col_0  col_1
         0      6      2
@@ -7444,11 +8289,10 @@
         normalize: bool_t = False,
         fill_value=None,
     ) -> FrameOrSeries:
-        """
-        Convert TimeSeries to specified frequency.
-
+        """Convert TimeSeries to specified frequency.
+        
         Optionally provide filling method to pad/backfill missing values.
-
+        
         Returns the original data conformed to a new index with the specified
         frequency. ``resample`` is more appropriate if an operation, such as
         summarization, is necessary to represent the data at the new frequency.
@@ -7460,17 +8304,22 @@
         method : {'backfill'/'bfill', 'pad'/'ffill'}, default None
             Method to use for filling holes in reindexed Series (note this
             does not fill NaNs that already were present):
-
             * 'pad' / 'ffill': propagate last valid observation forward to next
-              valid
-            * 'backfill' / 'bfill': use NEXT valid observation to fill.
+            valid
+            * 'backfill' / 'bfill': use NEXT valid observation to fill. (Default value = None)
         how : {'start', 'end'}, default end
             For PeriodIndex only (see PeriodIndex.asfreq).
         normalize : bool, default False
             Whether to reset output index to midnight.
         fill_value : scalar, optional
             Value to use for missing values, applied during upsampling (note
-            this does not fill NaNs that already were present).
+            this does not fill NaNs that already were present). (Default value = None)
+        self: FrameOrSeries :
+            
+        how: Optional[str] :
+             (Default value = None)
+        normalize: bool_t :
+             (Default value = False)
 
         Returns
         -------
@@ -7480,16 +8329,22 @@
         See Also
         --------
         reindex : Conform DataFrame to new index with optional filling logic.
-
         Notes
         -----
         To learn more about the frequency strings, please see `this link
         <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.
-
         Examples
         --------
         Start by creating a series with 4 one minute timestamps.
-
+        
+        
+        Upsample the series into 30 second bins.
+        
+        
+        Upsample again, providing a ``fill value``.
+        
+        
+        Upsample again, providing a ``method``.
         >>> index = pd.date_range('1/1/2000', periods=4, freq='T')
         >>> series = pd.Series([0.0, None, 2.0, 3.0], index=index)
         >>> df = pd.DataFrame({'s':series})
@@ -7499,9 +8354,7 @@
         2000-01-01 00:01:00    NaN
         2000-01-01 00:02:00    2.0
         2000-01-01 00:03:00    3.0
-
-        Upsample the series into 30 second bins.
-
+        
         >>> df.asfreq(freq='30S')
                                s
         2000-01-01 00:00:00    0.0
@@ -7511,9 +8364,7 @@
         2000-01-01 00:02:00    2.0
         2000-01-01 00:02:30    NaN
         2000-01-01 00:03:00    3.0
-
-        Upsample again, providing a ``fill value``.
-
+        
         >>> df.asfreq(freq='30S', fill_value=9.0)
                                s
         2000-01-01 00:00:00    0.0
@@ -7523,9 +8374,7 @@
         2000-01-01 00:02:00    2.0
         2000-01-01 00:02:30    9.0
         2000-01-01 00:03:00    3.0
-
-        Upsample again, providing a ``method``.
-
+        
         >>> df.asfreq(freq='30S', method='bfill')
                                s
         2000-01-01 00:00:00    0.0
@@ -7550,19 +8399,23 @@
     def at_time(
         self: FrameOrSeries, time, asof: bool_t = False, axis=None
     ) -> FrameOrSeries:
-        """
-        Select values at particular time of day (e.g., 9:30AM).
+        """Select values at particular time of day (e.g., 9:30AM).
 
         Parameters
         ----------
         time : datetime.time or str
+            
         axis : {0 or 'index', 1 or 'columns'}, default 0
-
-            .. versionadded:: 0.24.0
+            .. versionadded:: 0.24.0 (Default value = None)
+        self: FrameOrSeries :
+            
+        asof: bool_t :
+             (Default value = False)
 
         Returns
         -------
         Series or DataFrame
+            
 
         Raises
         ------
@@ -7576,7 +8429,6 @@
         last : Select final periods of time series based on a date offset.
         DatetimeIndex.indexer_at_time : Get just the index locations for
             values at particular time of the day.
-
         Examples
         --------
         >>> i = pd.date_range('2018-04-09', periods=4, freq='12H')
@@ -7587,7 +8439,7 @@
         2018-04-09 12:00:00  2
         2018-04-10 00:00:00  3
         2018-04-10 12:00:00  4
-
+        
         >>> ts.at_time('12:00')
                              A
         2018-04-09 12:00:00  2
@@ -7613,9 +8465,8 @@
         include_end: bool_t = True,
         axis=None,
     ) -> FrameOrSeries:
-        """
-        Select values between particular times of the day (e.g., 9:00-9:30 AM).
-
+        """Select values between particular times of the day (e.g., 9:00-9:30 AM).
+        
         By setting ``start_time`` to be later than ``end_time``,
         you can get the times that are *not* between the two times.
 
@@ -7631,8 +8482,13 @@
             Whether the end time needs to be included in the result.
         axis : {0 or 'index', 1 or 'columns'}, default 0
             Determine range time on index or columns value.
-
-            .. versionadded:: 0.24.0
+            .. versionadded:: 0.24.0 (Default value = None)
+        self: FrameOrSeries :
+            
+        include_start: bool_t :
+             (Default value = True)
+        include_end: bool_t :
+             (Default value = True)
 
         Returns
         -------
@@ -7651,9 +8507,12 @@
         last : Select final periods of time series based on a date offset.
         DatetimeIndex.indexer_between_time : Get just the index locations for
             values between particular times of the day.
-
         Examples
         --------
+        
+        
+        You get the times that are *not* between two times by setting
+        ``start_time`` later than ``end_time``:
         >>> i = pd.date_range('2018-04-09', periods=4, freq='1D20min')
         >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)
         >>> ts
@@ -7662,15 +8521,12 @@
         2018-04-10 00:20:00  2
         2018-04-11 00:40:00  3
         2018-04-12 01:00:00  4
-
+        
         >>> ts.between_time('0:15', '0:45')
                              A
         2018-04-10 00:20:00  2
         2018-04-11 00:40:00  3
-
-        You get the times that are *not* between two times by setting
-        ``start_time`` later than ``end_time``:
-
+        
         >>> ts.between_time('0:45', '0:15')
                              A
         2018-04-09 00:00:00  1
@@ -7704,9 +8560,8 @@
         origin: Union[str, TimestampConvertibleTypes] = "start_day",
         offset: Optional[TimedeltaConvertibleTypes] = None,
     ) -> "Resampler":
-        """
-        Resample time-series data.
-
+        """Resample time-series data.
+        
         Convenience method for frequency conversion and resampling of time
         series. Object must have a datetime-like index (`DatetimeIndex`,
         `PeriodIndex`, or `TimedeltaIndex`), or pass datetime-like values
@@ -7737,64 +8592,140 @@
             By default the input representation is retained.
         loffset : timedelta, default None
             Adjust the resampled time labels.
-
             .. deprecated:: 1.1.0
-                You should add the loffset to the `df.index` after the resample.
-                See below.
-
+            You should add the loffset to the `df.index` after the resample.
+            See below. (Default value = None)
         base : int, default 0
             For frequencies that evenly subdivide 1 day, the "origin" of the
             aggregated intervals. For example, for '5min' frequency, base could
             range from 0 through 4. Defaults to 0.
-
             .. deprecated:: 1.1.0
-                The new arguments that you should use are 'offset' or 'origin'.
-
+            The new arguments that you should use are 'offset' or 'origin'.
         on : str, optional
             For a DataFrame, column to use instead of index for resampling.
-            Column must be datetime-like.
+            Column must be datetime-like. (Default value = None)
         level : str or int, optional
             For a MultiIndex, level (name or number) to use for
-            resampling. `level` must be datetime-like.
+            resampling. `level` must be datetime-like. (Default value = None)
         origin : {'epoch', 'start', 'start_day'}, Timestamp or str, default 'start_day'
             The timestamp on which to adjust the grouping. The timezone of origin
             must match the timezone of the index.
             If a timestamp is not used, these values are also supported:
-
             - 'epoch': `origin` is 1970-01-01
             - 'start': `origin` is the first value of the timeseries
             - 'start_day': `origin` is the first day at midnight of the timeseries
-
             .. versionadded:: 1.1.0
-
         offset : Timedelta or str, default is None
             An offset timedelta added to the origin.
-
             .. versionadded:: 1.1.0
+        closed: Optional[str] :
+             (Default value = None)
+        label: Optional[str] :
+             (Default value = None)
+        convention: str :
+             (Default value = "start")
+        kind: Optional[str] :
+             (Default value = None)
+        base: Optional[int] :
+             (Default value = None)
+        origin: Union[str :
+            
+        TimestampConvertibleTypes] :
+             (Default value = "start_day")
+        offset: Optional[TimedeltaConvertibleTypes] :
+             (Default value = None)
 
         Returns
         -------
         Resampler object
+            
 
         See Also
         --------
         groupby : Group by mapping, function, label, or list of labels.
         Series.resample : Resample a Series.
         DataFrame.resample: Resample a DataFrame.
-
         Notes
         -----
         See the `user guide
         <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling>`_
         for more.
-
+        
         To learn more about the offset strings, please see `this link
         <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects>`__.
-
         Examples
         --------
         Start by creating a series with 9 one minute timestamps.
-
+        
+        
+        Downsample the series into 3 minute bins and sum the values
+        of the timestamps falling into a bin.
+        
+        
+        Downsample the series into 3 minute bins as above, but label each
+        bin using the right edge instead of the left. Please note that the
+        value in the bucket used as the label is not included in the bucket,
+        which it labels. For example, in the original series the
+        bucket ``2000-01-01 00:03:00`` contains the value 3, but the summed
+        value in the resampled bucket with the label ``2000-01-01 00:03:00``
+        does not include 3 (if it did, the summed value would be 6, not 3).
+        To include this value close the right side of the bin interval as
+        illustrated in the example below this one.
+        
+        
+        Downsample the series into 3 minute bins as above, but close the right
+        side of the bin interval.
+        
+        
+        Upsample the series into 30 second bins.
+        
+        
+        Upsample the series into 30 second bins and fill the ``NaN``
+        values using the ``pad`` method.
+        
+        
+        Upsample the series into 30 second bins and fill the
+        ``NaN`` values using the ``bfill`` method.
+        
+        
+        Pass a custom function via ``apply``
+        
+        
+        For a Series with a PeriodIndex, the keyword `convention` can be
+        used to control whether to use the start or end of `rule`.
+        
+        Resample a year by quarter using 'start' `convention`. Values are
+        assigned to the first quarter of the period.
+        
+        
+        Resample quarters by month using 'end' `convention`. Values are
+        assigned to the last month of the period.
+        
+        
+        For DataFrame objects, the keyword `on` can be used to specify the
+        column instead of the index for resampling.
+        
+        
+        For a DataFrame with MultiIndex, the keyword `level` can be used to
+        specify on which level the resampling needs to take place.
+        
+        
+        If you want to adjust the start of the bins based on a fixed timestamp:
+        
+        
+        
+        
+        
+        If you want to adjust the start of the bins with an `offset` Timedelta, the two
+        following lines are equivalent:
+        
+        
+        
+        To replace the use of the deprecated `base` argument, you can now use `offset`,
+        in this example it is equivalent to have `base=2`:
+        
+        
+        To replace the use of the deprecated `loffset` argument:
         >>> index = pd.date_range('1/1/2000', periods=9, freq='T')
         >>> series = pd.Series(range(9), index=index)
         >>> series
@@ -7808,44 +8739,26 @@
         2000-01-01 00:07:00    7
         2000-01-01 00:08:00    8
         Freq: T, dtype: int64
-
-        Downsample the series into 3 minute bins and sum the values
-        of the timestamps falling into a bin.
-
+        
         >>> series.resample('3T').sum()
         2000-01-01 00:00:00     3
         2000-01-01 00:03:00    12
         2000-01-01 00:06:00    21
         Freq: 3T, dtype: int64
-
-        Downsample the series into 3 minute bins as above, but label each
-        bin using the right edge instead of the left. Please note that the
-        value in the bucket used as the label is not included in the bucket,
-        which it labels. For example, in the original series the
-        bucket ``2000-01-01 00:03:00`` contains the value 3, but the summed
-        value in the resampled bucket with the label ``2000-01-01 00:03:00``
-        does not include 3 (if it did, the summed value would be 6, not 3).
-        To include this value close the right side of the bin interval as
-        illustrated in the example below this one.
-
+        
         >>> series.resample('3T', label='right').sum()
         2000-01-01 00:03:00     3
         2000-01-01 00:06:00    12
         2000-01-01 00:09:00    21
         Freq: 3T, dtype: int64
-
-        Downsample the series into 3 minute bins as above, but close the right
-        side of the bin interval.
-
+        
         >>> series.resample('3T', label='right', closed='right').sum()
         2000-01-01 00:00:00     0
         2000-01-01 00:03:00     6
         2000-01-01 00:06:00    15
         2000-01-01 00:09:00    15
         Freq: 3T, dtype: int64
-
-        Upsample the series into 30 second bins.
-
+        
         >>> series.resample('30S').asfreq()[0:5]   # Select first 5 rows
         2000-01-01 00:00:00   0.0
         2000-01-01 00:00:30   NaN
@@ -7853,10 +8766,7 @@
         2000-01-01 00:01:30   NaN
         2000-01-01 00:02:00   2.0
         Freq: 30S, dtype: float64
-
-        Upsample the series into 30 second bins and fill the ``NaN``
-        values using the ``pad`` method.
-
+        
         >>> series.resample('30S').pad()[0:5]
         2000-01-01 00:00:00    0
         2000-01-01 00:00:30    0
@@ -7864,10 +8774,7 @@
         2000-01-01 00:01:30    1
         2000-01-01 00:02:00    2
         Freq: 30S, dtype: int64
-
-        Upsample the series into 30 second bins and fill the
-        ``NaN`` values using the ``bfill`` method.
-
+        
         >>> series.resample('30S').bfill()[0:5]
         2000-01-01 00:00:00    0
         2000-01-01 00:00:30    1
@@ -7875,9 +8782,7 @@
         2000-01-01 00:01:30    2
         2000-01-01 00:02:00    2
         Freq: 30S, dtype: int64
-
-        Pass a custom function via ``apply``
-
+        
         >>> def custom_resampler(array_like):
         ...     return np.sum(array_like) + 5
         ...
@@ -7886,13 +8791,7 @@
         2000-01-01 00:03:00    17
         2000-01-01 00:06:00    26
         Freq: 3T, dtype: int64
-
-        For a Series with a PeriodIndex, the keyword `convention` can be
-        used to control whether to use the start or end of `rule`.
-
-        Resample a year by quarter using 'start' `convention`. Values are
-        assigned to the first quarter of the period.
-
+        
         >>> s = pd.Series([1, 2], index=pd.period_range('2012-01-01',
         ...                                             freq='A',
         ...                                             periods=2))
@@ -7910,10 +8809,7 @@
         2013Q3    NaN
         2013Q4    NaN
         Freq: Q-DEC, dtype: float64
-
-        Resample quarters by month using 'end' `convention`. Values are
-        assigned to the last month of the period.
-
+        
         >>> q = pd.Series([1, 2, 3, 4], index=pd.period_range('2018-01-01',
         ...                                                   freq='Q',
         ...                                                   periods=4))
@@ -7935,10 +8831,7 @@
         2018-11    NaN
         2018-12    4.0
         Freq: M, dtype: float64
-
-        For DataFrame objects, the keyword `on` can be used to specify the
-        column instead of the index for resampling.
-
+        
         >>> d = dict({'price': [10, 11, 9, 13, 14, 18, 17, 19],
         ...           'volume': [50, 60, 40, 100, 50, 100, 40, 50]})
         >>> df = pd.DataFrame(d)
@@ -7960,10 +8853,7 @@
         week_starting
         2018-01-31     10.75    62.5
         2018-02-28     17.00    60.0
-
-        For a DataFrame with MultiIndex, the keyword `level` can be used to
-        specify on which level the resampling needs to take place.
-
+        
         >>> days = pd.date_range('1/1/2000', periods=4, freq='D')
         >>> d2 = dict({'price': [10, 11, 9, 13, 14, 18, 17, 19],
         ...            'volume': [50, 60, 40, 100, 50, 100, 40, 50]})
@@ -7988,9 +8878,7 @@
         2000-01-02     22     140
         2000-01-03     32     150
         2000-01-04     36      90
-
-        If you want to adjust the start of the bins based on a fixed timestamp:
-
+        
         >>> start, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00'
         >>> rng = pd.date_range(start, end, freq='7min')
         >>> ts = pd.Series(np.arange(len(rng)) * 3, index=rng)
@@ -8005,7 +8893,7 @@
         2000-10-02 00:19:00    21
         2000-10-02 00:26:00    24
         Freq: 7T, dtype: int64
-
+        
         >>> ts.resample('17min').sum()
         2000-10-01 23:14:00     0
         2000-10-01 23:31:00     9
@@ -8013,7 +8901,7 @@
         2000-10-02 00:05:00    54
         2000-10-02 00:22:00    24
         Freq: 17T, dtype: int64
-
+        
         >>> ts.resample('17min', origin='epoch').sum()
         2000-10-01 23:18:00     0
         2000-10-01 23:35:00    18
@@ -8021,34 +8909,28 @@
         2000-10-02 00:09:00    39
         2000-10-02 00:26:00    24
         Freq: 17T, dtype: int64
-
+        
         >>> ts.resample('17min', origin='2000-01-01').sum()
         2000-10-01 23:24:00     3
         2000-10-01 23:41:00    15
         2000-10-01 23:58:00    45
         2000-10-02 00:15:00    45
         Freq: 17T, dtype: int64
-
-        If you want to adjust the start of the bins with an `offset` Timedelta, the two
-        following lines are equivalent:
-
+        
         >>> ts.resample('17min', origin='start').sum()
         2000-10-01 23:30:00     9
         2000-10-01 23:47:00    21
         2000-10-02 00:04:00    54
         2000-10-02 00:21:00    24
         Freq: 17T, dtype: int64
-
+        
         >>> ts.resample('17min', offset='23h30min').sum()
         2000-10-01 23:30:00     9
         2000-10-01 23:47:00    21
         2000-10-02 00:04:00    54
         2000-10-02 00:21:00    24
         Freq: 17T, dtype: int64
-
-        To replace the use of the deprecated `base` argument, you can now use `offset`,
-        in this example it is equivalent to have `base=2`:
-
+        
         >>> ts.resample('17min', offset='2min').sum()
         2000-10-01 23:16:00     0
         2000-10-01 23:33:00     9
@@ -8056,9 +8938,7 @@
         2000-10-02 00:07:00    39
         2000-10-02 00:24:00    24
         Freq: 17T, dtype: int64
-
-        To replace the use of the deprecated `loffset` argument:
-
+        
         >>> from pandas.tseries.frequencies import to_offset
         >>> loffset = '19min'
         >>> ts_out = ts.resample('17min').sum()
@@ -8091,9 +8971,8 @@
         )
 
     def first(self: FrameOrSeries, offset) -> FrameOrSeries:
-        """
-        Select initial periods of time series data based on a date offset.
-
+        """Select initial periods of time series data based on a date offset.
+        
         When having a DataFrame with dates as index, this function can
         select the first few rows based on a date offset.
 
@@ -8102,6 +8981,8 @@
         offset : str, DateOffset or dateutil.relativedelta
             The offset length of the data that will be selected. For instance,
             '1M' will display all the rows having their index within the first month.
+        self: FrameOrSeries :
+            
 
         Returns
         -------
@@ -8118,9 +8999,15 @@
         last : Select final periods of time series based on a date offset.
         at_time : Select values at a particular time of the day.
         between_time : Select values between particular times of the day.
-
         Examples
         --------
+        
+        Get the rows for the first 3 days:
+        
+        
+        Notice the data for 3 first calendar days were returned, not the first
+        3 days observed in the dataset, and therefore data for 2018-04-13 was
+        not returned.
         >>> i = pd.date_range('2018-04-09', periods=4, freq='2D')
         >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)
         >>> ts
@@ -8129,17 +9016,11 @@
         2018-04-11  2
         2018-04-13  3
         2018-04-15  4
-
-        Get the rows for the first 3 days:
-
+        
         >>> ts.first('3D')
                     A
         2018-04-09  1
         2018-04-11  2
-
-        Notice the data for 3 first calendar days were returned, not the first
-        3 days observed in the dataset, and therefore data for 2018-04-13 was
-        not returned.
         """
         if not isinstance(self.index, DatetimeIndex):
             raise TypeError("'first' only supports a DatetimeIndex index")
@@ -8159,9 +9040,8 @@
         return self.loc[:end]
 
     def last(self: FrameOrSeries, offset) -> FrameOrSeries:
-        """
-        Select final periods of time series data based on a date offset.
-
+        """Select final periods of time series data based on a date offset.
+        
         When having a DataFrame with dates as index, this function can
         select the last few rows based on a date offset.
 
@@ -8170,6 +9050,8 @@
         offset : str, DateOffset, dateutil.relativedelta
             The offset length of the data that will be selected. For instance,
             '3D' will display all the rows having their index within the last 3 days.
+        self: FrameOrSeries :
+            
 
         Returns
         -------
@@ -8186,9 +9068,15 @@
         first : Select initial periods of time series based on a date offset.
         at_time : Select values at a particular time of the day.
         between_time : Select values between particular times of the day.
-
         Examples
         --------
+        
+        Get the rows for the last 3 days:
+        
+        
+        Notice the data for 3 last calendar days were returned, not the last
+        3 observed days in the dataset, and therefore data for 2018-04-11 was
+        not returned.
         >>> i = pd.date_range('2018-04-09', periods=4, freq='2D')
         >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)
         >>> ts
@@ -8197,17 +9085,11 @@
         2018-04-11  2
         2018-04-13  3
         2018-04-15  4
-
-        Get the rows for the last 3 days:
-
+        
         >>> ts.last('3D')
                     A
         2018-04-13  3
         2018-04-15  4
-
-        Notice the data for 3 last calendar days were returned, not the last
-        3 observed days in the dataset, and therefore data for 2018-04-11 was
-        not returned.
         """
         if not isinstance(self.index, DatetimeIndex):
             raise TypeError("'last' only supports a DatetimeIndex index")
@@ -8230,39 +9112,46 @@
         ascending: bool_t = True,
         pct: bool_t = False,
     ) -> FrameOrSeries:
-        """
-        Compute numerical data ranks (1 through n) along axis.
-
+        """Compute numerical data ranks (1 through n) along axis.
+        
         By default, equal values are assigned a rank that is the average of the
         ranks of those values.
 
         Parameters
         ----------
         axis : {0 or 'index', 1 or 'columns'}, default 0
-            Index to direct ranking.
+            Index to direct ranking. (Default value = 0)
         method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'
             How to rank the group of records that have the same value (i.e. ties):
-
             * average: average rank of the group
             * min: lowest rank in the group
             * max: highest rank in the group
             * first: ranks assigned in order they appear in the array
             * dense: like 'min', but rank always increases by 1 between groups.
-
         numeric_only : bool, optional
             For DataFrame objects, rank only numeric columns if set to True.
         na_option : {'keep', 'top', 'bottom'}, default 'keep'
             How to rank NaN values:
-
             * keep: assign NaN rank to NaN values
             * top: assign smallest rank to NaN values if ascending
             * bottom: assign highest rank to NaN values if ascending.
-
         ascending : bool, default True
             Whether or not the elements should be ranked in ascending order.
         pct : bool, default False
             Whether or not to display the returned rankings in percentile
             form.
+        self: FrameOrSeries :
+            
+        method: str :
+             (Default value = "average")
+        numeric_only: Optional[bool_t] :
+             (Default value = None)
+        na_option: str :
+             (Default value = "keep")
+        ascending: bool_t :
+             (Default value = True)
+        pct: bool_t :
+             (Default value = False)
 
         Returns
         -------
@@ -8272,9 +9161,21 @@
         See Also
         --------
         core.groupby.GroupBy.rank : Rank of values within each group.
-
         Examples
         --------
+        
+        The following example shows how the method behaves with the above
+        parameters:
+        
+        * default_rank: this is the default behaviour obtained without using
+          any parameter.
+        * max_rank: setting ``method = 'max'`` the records that have the
+          same values are ranked using the highest rank (e.g.: since 'cat'
+          and 'dog' are both in the 2nd and 3rd position, rank 3 is assigned.)
+        * NA_bottom: choosing ``na_option = 'bottom'``, if there are records
+          with NaN values they are placed at the bottom of the ranking.
+        * pct_rank: when setting ``pct = True``, the ranking is expressed as
+          percentile rank.
         >>> df = pd.DataFrame(data={'Animal': ['cat', 'penguin', 'dog',
         ...                                    'spider', 'snake'],
         ...                         'Number_legs': [4, 2, 4, 8, np.nan]})
@@ -8285,20 +9186,7 @@
         2      dog          4.0
         3   spider          8.0
         4    snake          NaN
-
-        The following example shows how the method behaves with the above
-        parameters:
-
-        * default_rank: this is the default behaviour obtained without using
-          any parameter.
-        * max_rank: setting ``method = 'max'`` the records that have the
-          same values are ranked using the highest rank (e.g.: since 'cat'
-          and 'dog' are both in the 2nd and 3rd position, rank 3 is assigned.)
-        * NA_bottom: choosing ``na_option = 'bottom'``, if there are records
-          with NaN values they are placed at the bottom of the ranking.
-        * pct_rank: when setting ``pct = True``, the ranking is expressed as
-          percentile rank.
-
+        
         >>> df['default_rank'] = df['Number_legs'].rank()
         >>> df['max_rank'] = df['Number_legs'].rank(method='max')
         >>> df['NA_bottom'] = df['Number_legs'].rank(na_option='bottom')
@@ -8318,6 +9206,17 @@
             raise ValueError(msg)
 
         def ranker(data):
+            """
+
+            Parameters
+            ----------
+            data :
+                
+
+            Returns
+            -------
+
+            """
             ranks = algos.rank(
                 data.values,
                 axis=axis,
@@ -8381,6 +9280,23 @@
         keep_shape: bool_t = False,
         keep_equal: bool_t = False,
     ):
+        """
+
+        Parameters
+        ----------
+        other :
+            
+        align_axis: Axis :
+             (Default value = 1)
+        keep_shape: bool_t :
+             (Default value = False)
+        keep_equal: bool_t :
+             (Default value = False)
+
+        Returns
+        -------
+
+        """
         from pandas.core.reshape.concat import concat
 
         if type(self) is not type(other):
@@ -8456,49 +9372,48 @@
         fill_axis=0,
         broadcast_axis=None,
     ):
-        """
-        Align two objects on their axes with the specified join method.
-
+        """Align two objects on their axes with the specified join method.
+        
         Join method is specified for each axis Index.
 
         Parameters
         ----------
         other : DataFrame or Series
+            
         join : {{'outer', 'inner', 'left', 'right'}}, default 'outer'
+             (Default value = "outer")
         axis : allowed axis of the other object, default None
-            Align on index (0), columns (1), or both (None).
+            Align on index (0), columns (1), or both (None). (Default value = None)
         level : int or level name, default None
             Broadcast across a level, matching Index values on the
-            passed MultiIndex level.
+            passed MultiIndex level. (Default value = None)
         copy : bool, default True
             Always returns new objects. If copy=False and no reindexing is
-            required then original objects are returned.
+            required then original objects are returned. (Default value = True)
         fill_value : scalar, default np.NaN
             Value to use for missing values. Defaults to NaN, but can be any
             "compatible" value.
         method : {{'backfill', 'bfill', 'pad', 'ffill', None}}, default None
             Method to use for filling holes in reindexed Series:
-
             - pad / ffill: propagate last valid observation forward to next valid.
-            - backfill / bfill: use NEXT valid observation to fill gap.
-
+            - backfill / bfill: use NEXT valid observation to fill gap. (Default value = None)
         limit : int, default None
             If method is specified, this is the maximum number of consecutive
             NaN values to forward/backward fill. In other words, if there is
             a gap with more than this number of consecutive NaNs, it will only
             be partially filled. If method is not specified, this is the
             maximum number of entries along the entire axis where NaNs will be
-            filled. Must be greater than 0 if not None.
+            filled. Must be greater than 0 if not None. (Default value = None)
         fill_axis : {axes_single_arg}, default 0
-            Filling axis, method and limit.
+            Filling axis, method and limit. (Default value = 0)
         broadcast_axis : {axes_single_arg}, default None
             Broadcast values along this axis, if aligning two objects of
-            different dimensions.
-
-        Returns
-        -------
-        (left, right) : ({klass}, type of other)
-            Aligned objects.
+            different dimensions. (Default value = None)
+
+        Returns
+        -------
+
+        
         """
 
         method = missing.clean_fill_method(method)
@@ -8582,6 +9497,33 @@
         limit=None,
         fill_axis=0,
     ):
+        """
+
+        Parameters
+        ----------
+        other :
+            
+        join :
+             (Default value = "outer")
+        axis :
+             (Default value = None)
+        level :
+             (Default value = None)
+        copy: bool_t :
+             (Default value = True)
+        fill_value :
+             (Default value = None)
+        method :
+             (Default value = None)
+        limit :
+             (Default value = None)
+        fill_axis :
+             (Default value = 0)
+
+        Returns
+        -------
+
+        """
         # defaults
         join_index, join_columns = None, None
         ilidx, iridx = None, None
@@ -8647,6 +9589,33 @@
         limit=None,
         fill_axis=0,
     ):
+        """
+
+        Parameters
+        ----------
+        other :
+            
+        join :
+             (Default value = "outer")
+        axis :
+             (Default value = None)
+        level :
+             (Default value = None)
+        copy: bool_t :
+             (Default value = True)
+        fill_value :
+             (Default value = None)
+        method :
+             (Default value = None)
+        limit :
+             (Default value = None)
+        fill_axis :
+             (Default value = 0)
+
+        Returns
+        -------
+
+        """
 
         is_series = isinstance(self, ABCSeries)
 
@@ -8732,9 +9701,29 @@
         errors="raise",
         try_cast=False,
     ):
-        """
-        Equivalent to public method `where`, except that `other` is not
+        """Equivalent to public method `where`, except that `other` is not
         applied as a function even if callable. Used in __setitem__.
+
+        Parameters
+        ----------
+        cond :
+            
+        other :
+             (Default value = np.nan)
+        inplace :
+             (Default value = False)
+        axis :
+             (Default value = None)
+        level :
+             (Default value = None)
+        errors :
+             (Default value = "raise")
+        try_cast :
+             (Default value = False)
+
+        Returns
+        -------
+
         """
         inplace = validate_bool_kwarg(inplace, "inplace")
 
@@ -8886,8 +9875,7 @@
         errors="raise",
         try_cast=False,
     ):
-        """
-        Replace values where the condition is {cond_rev}.
+        """Replace values where the condition is {cond_rev}.
 
         Parameters
         ----------
@@ -8902,46 +9890,43 @@
             corresponding value from `other`.
             If other is callable, it is computed on the {klass} and
             should return scalar or {klass}. The callable must not
-            change input {klass} (though pandas doesn't check it).
+            change input {klass} (though pandas doesn't check it). (Default value = np.nan)
         inplace : bool, default False
-            Whether to perform the operation in place on the data.
+            Whether to perform the operation in place on the data. (Default value = False)
         axis : int, default None
-            Alignment axis if needed.
+            Alignment axis if needed. (Default value = None)
         level : int, default None
-            Alignment level if needed.
+            Alignment level if needed. (Default value = None)
         errors : str, {{'raise', 'ignore'}}, default 'raise'
             Note that currently this parameter won't affect
             the results and will always coerce to a suitable dtype.
-
             - 'raise' : allow exceptions to be raised.
-            - 'ignore' : suppress exceptions. On error return original object.
-
+            - 'ignore' : suppress exceptions. On error return original object. (Default value = "raise")
         try_cast : bool, default False
-            Try to cast the result back to the input type (if possible).
+            Try to cast the result back to the input type (if possible). (Default value = False)
 
         Returns
         -------
         Same type as caller
+            
 
         See Also
         --------
         :func:`DataFrame.{name_other}` : Return an object of same shape as
             self.
-
         Notes
         -----
         The {name} method is an application of the if-then idiom. For each
         element in the calling DataFrame, if ``cond`` is ``{cond}`` the
         element is used; otherwise the corresponding element from the DataFrame
         ``other`` is used.
-
+        
         The signature for :func:`DataFrame.where` differs from
         :func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to
         ``np.where(m, df1, df2)``.
-
+        
         For further details and examples see the ``{name}`` documentation in
         :ref:`indexing <indexing.where_mask>`.
-
         Examples
         --------
         >>> s = pd.Series(range(5))
@@ -8952,7 +9937,7 @@
         3    3.0
         4    4.0
         dtype: float64
-
+        
         >>> s.mask(s > 0)
         0    0.0
         1    NaN
@@ -8960,7 +9945,7 @@
         3    NaN
         4    NaN
         dtype: float64
-
+        
         >>> s.where(s > 1, 10)
         0    10
         1    10
@@ -8968,7 +9953,7 @@
         3    3
         4    4
         dtype: int64
-
+        
         >>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])
         >>> df
            A  B
@@ -9023,6 +10008,29 @@
         errors="raise",
         try_cast=False,
     ):
+        """
+
+        Parameters
+        ----------
+        cond :
+            
+        other :
+             (Default value = np.nan)
+        inplace :
+             (Default value = False)
+        axis :
+             (Default value = None)
+        level :
+             (Default value = None)
+        errors :
+             (Default value = "raise")
+        try_cast :
+             (Default value = False)
+
+        Returns
+        -------
+
+        """
 
         inplace = validate_bool_kwarg(inplace, "inplace")
         cond = com.apply_if_callable(cond, self)
@@ -9045,9 +10053,8 @@
     def shift(
         self: FrameOrSeries, periods=1, freq=None, axis=0, fill_value=None
     ) -> FrameOrSeries:
-        """
-        Shift index by desired number of periods with an optional time `freq`.
-
+        """Shift index by desired number of periods with an optional time `freq`.
+        
         When `freq` is not passed, shift the index without realigning the data.
         If `freq` is passed (in this case, the index must be date or datetime,
         or it will raise a `NotImplementedError`), the index will be
@@ -9058,7 +10065,7 @@
         Parameters
         ----------
         periods : int
-            Number of periods to shift. Can be positive or negative.
+            Number of periods to shift. Can be positive or negative. (Default value = 1)
         freq : DateOffset, tseries.offsets, timedelta, or str, optional
             Offset to use from the tseries module or time rule (e.g. 'EOM').
             If `freq` is specified then the index values are shifted but the
@@ -9066,17 +10073,18 @@
             extend the index when shifting and preserve the original data.
             If `freq` is specified as "infer" then it will be inferred from
             the freq or inferred_freq attributes of the index. If neither of
-            those attributes exist, a ValueError is thrown
+            those attributes exist, a ValueError is thrown (Default value = None)
         axis : {{0 or 'index', 1 or 'columns', None}}, default None
-            Shift direction.
+            Shift direction. (Default value = 0)
         fill_value : object, optional
             The scalar value to use for newly introduced missing values.
             the default depends on the dtype of `self`.
             For numeric data, ``np.nan`` is used.
             For datetime, timedelta, or period data, etc. :attr:`NaT` is used.
             For extension dtypes, ``self.dtype.na_value`` is used.
-
             .. versionchanged:: 1.1.0
+        self: FrameOrSeries :
+            
 
         Returns
         -------
@@ -9090,7 +10098,6 @@
         PeriodIndex.shift : Shift values of PeriodIndex.
         tshift : Shift the time index, using the index's frequency if
             available.
-
         Examples
         --------
         >>> df = pd.DataFrame({{"Col1": [10, 20, 15, 30, 45],
@@ -9104,7 +10111,7 @@
         2020-01-03    15    18    22
         2020-01-04    30    33    37
         2020-01-05    45    48    52
-
+        
         >>> df.shift(periods=3)
                     Col1  Col2  Col3
         2020-01-01   NaN   NaN   NaN
@@ -9112,7 +10119,7 @@
         2020-01-03   NaN   NaN   NaN
         2020-01-04  10.0  13.0  17.0
         2020-01-05  20.0  23.0  27.0
-
+        
         >>> df.shift(periods=1, axis="columns")
                     Col1  Col2  Col3
         2020-01-01   NaN  10.0  13.0
@@ -9120,7 +10127,7 @@
         2020-01-03   NaN  15.0  18.0
         2020-01-04   NaN  30.0  33.0
         2020-01-05   NaN  45.0  48.0
-
+        
         >>> df.shift(periods=3, fill_value=0)
                     Col1  Col2  Col3
         2020-01-01     0     0     0
@@ -9128,7 +10135,7 @@
         2020-01-03     0     0     0
         2020-01-04    10    13    17
         2020-01-05    20    23    27
-
+        
         >>> df.shift(periods=3, freq="D")
                     Col1  Col2  Col3
         2020-01-04    10    13    17
@@ -9136,7 +10143,7 @@
         2020-01-06    15    18    22
         2020-01-07    30    33    37
         2020-01-08    45    48    52
-
+        
         >>> df.shift(periods=3, freq="infer")
                     Col1  Col2  Col3
         2020-01-04    10    13    17
@@ -9188,9 +10195,8 @@
         return result.__finalize__(self, method="shift")
 
     def slice_shift(self: FrameOrSeries, periods: int = 1, axis=0) -> FrameOrSeries:
-        """
-        Equivalent to `shift` without copying data.
-
+        """Equivalent to `shift` without copying data.
+        
         The shifted data will not include the dropped periods and the
         shifted axis will be smaller than the original.
 
@@ -9198,10 +10204,17 @@
         ----------
         periods : int
             Number of periods to move, can be positive or negative.
+        self: FrameOrSeries :
+            
+        periods: int :
+             (Default value = 1)
+        axis :
+             (Default value = 0)
 
         Returns
         -------
         shifted : same type as caller
+            
 
         Notes
         -----
@@ -9227,9 +10240,8 @@
     def tshift(
         self: FrameOrSeries, periods: int = 1, freq=None, axis: Axis = 0
     ) -> FrameOrSeries:
-        """
-        Shift the time index, using the index's frequency if available.
-
+        """Shift the time index, using the index's frequency if available.
+        
         .. deprecated:: 1.1.0
             Use `shift` instead.
 
@@ -9239,13 +10251,20 @@
             Number of periods to move, can be positive or negative.
         freq : DateOffset, timedelta, or str, default None
             Increment to use from the tseries module
-            or time rule expressed as a string (e.g. 'EOM').
+            or time rule expressed as a string (e.g. 'EOM'). (Default value = None)
         axis : {0 or index, 1 or columns, None}, default 0
             Corresponds to the axis that contains the Index.
+        self: FrameOrSeries :
+            
+        periods: int :
+             (Default value = 1)
+        axis: Axis :
+             (Default value = 0)
 
         Returns
         -------
         shifted : Series/DataFrame
+            
 
         Notes
         -----
@@ -9270,22 +10289,25 @@
     def truncate(
         self: FrameOrSeries, before=None, after=None, axis=None, copy: bool_t = True
     ) -> FrameOrSeries:
-        """
-        Truncate a Series or DataFrame before and after some index value.
-
+        """Truncate a Series or DataFrame before and after some index value.
+        
         This is a useful shorthand for boolean indexing based on index
         values above or below certain thresholds.
 
         Parameters
         ----------
         before : date, str, int
-            Truncate all rows before this index value.
+            Truncate all rows before this index value. (Default value = None)
         after : date, str, int
-            Truncate all rows after this index value.
+            Truncate all rows after this index value. (Default value = None)
         axis : {0 or 'index', 1 or 'columns'}, optional
             Axis to truncate. Truncates the index (rows) by default.
         copy : bool, default is True,
             Return a copy of the truncated section.
+        self: FrameOrSeries :
+            
+        copy: bool_t :
+             (Default value = True)
 
         Returns
         -------
@@ -9296,15 +10318,34 @@
         --------
         DataFrame.loc : Select a subset of a DataFrame by label.
         DataFrame.iloc : Select a subset of a DataFrame by position.
-
         Notes
         -----
         If the index being truncated contains only datetime values,
         `before` and `after` may be specified as strings instead of
         Timestamps.
-
         Examples
         --------
+        
+        
+        The columns of a DataFrame can be truncated.
+        
+        
+        For Series, only rows can be truncated.
+        
+        
+        The index values in ``truncate`` can be datetimes or string
+        dates.
+        
+        
+        
+        Because the index is a DatetimeIndex containing only dates, we can
+        specify `before` and `after` as strings. They will be coerced to
+        Timestamps before truncation.
+        
+        
+        Note that ``truncate`` assumes a 0 value for any unspecified time
+        component (midnight). This differs from partial string slicing, which
+        returns any partially matching dates.
         >>> df = pd.DataFrame({'A': ['a', 'b', 'c', 'd', 'e'],
         ...                    'B': ['f', 'g', 'h', 'i', 'j'],
         ...                    'C': ['k', 'l', 'm', 'n', 'o']},
@@ -9316,15 +10357,13 @@
         3  c  h  m
         4  d  i  n
         5  e  j  o
-
+        
         >>> df.truncate(before=2, after=4)
            A  B  C
         2  b  g  l
         3  c  h  m
         4  d  i  n
-
-        The columns of a DataFrame can be truncated.
-
+        
         >>> df.truncate(before="A", after="B", axis="columns")
            A  B
         1  a  f
@@ -9332,18 +10371,13 @@
         3  c  h
         4  d  i
         5  e  j
-
-        For Series, only rows can be truncated.
-
+        
         >>> df['A'].truncate(before=2, after=4)
         2    b
         3    c
         4    d
         Name: A, dtype: object
-
-        The index values in ``truncate`` can be datetimes or string
-        dates.
-
+        
         >>> dates = pd.date_range('2016-01-01', '2016-02-01', freq='s')
         >>> df = pd.DataFrame(index=dates, data={'A': 1})
         >>> df.tail()
@@ -9353,7 +10387,7 @@
         2016-01-31 23:59:58  1
         2016-01-31 23:59:59  1
         2016-02-01 00:00:00  1
-
+        
         >>> df.truncate(before=pd.Timestamp('2016-01-05'),
         ...             after=pd.Timestamp('2016-01-10')).tail()
                              A
@@ -9362,11 +10396,7 @@
         2016-01-09 23:59:58  1
         2016-01-09 23:59:59  1
         2016-01-10 00:00:00  1
-
-        Because the index is a DatetimeIndex containing only dates, we can
-        specify `before` and `after` as strings. They will be coerced to
-        Timestamps before truncation.
-
+        
         >>> df.truncate('2016-01-05', '2016-01-10').tail()
                              A
         2016-01-09 23:59:56  1
@@ -9374,11 +10404,7 @@
         2016-01-09 23:59:58  1
         2016-01-09 23:59:59  1
         2016-01-10 00:00:00  1
-
-        Note that ``truncate`` assumes a 0 value for any unspecified time
-        component (midnight). This differs from partial string slicing, which
-        returns any partially matching dates.
-
+        
         >>> df.loc['2016-01-05':'2016-01-10', :].tail()
                              A
         2016-01-10 23:59:55  1
@@ -9427,33 +10453,48 @@
     def tz_convert(
         self: FrameOrSeries, tz, axis=0, level=None, copy: bool_t = True
     ) -> FrameOrSeries:
-        """
-        Convert tz-aware axis to target time zone.
+        """Convert tz-aware axis to target time zone.
 
         Parameters
         ----------
         tz : str or tzinfo object
+            
         axis : the axis to convert
+             (Default value = 0)
         level : int, str, default None
             If axis is a MultiIndex, convert a specific level. Otherwise
-            must be None.
+            must be None. (Default value = None)
         copy : bool, default True
             Also make a copy of the underlying data.
+        self: FrameOrSeries :
+            
+        copy: bool_t :
+             (Default value = True)
 
         Returns
         -------
         {klass}
             Object with time zone converted axis.
 
-        Raises
-        ------
-        TypeError
-            If the axis is tz-naive.
+        
         """
         axis = self._get_axis_number(axis)
         ax = self._get_axis(axis)
 
         def _tz_convert(ax, tz):
+            """
+
+            Parameters
+            ----------
+            ax :
+                
+            tz :
+                
+
+            Returns
+            -------
+
+            """
             if not hasattr(ax, "tz_convert"):
                 if len(ax) > 0:
                     ax_name = self._get_axis_name(axis)
@@ -9490,19 +10531,20 @@
         ambiguous="raise",
         nonexistent: str = "raise",
     ) -> FrameOrSeries:
-        """
-        Localize tz-naive index of a Series or DataFrame to target time zone.
-
+        """Localize tz-naive index of a Series or DataFrame to target time zone.
+        
         This operation localizes the Index. To localize the values in a
         timezone-naive Series, use :meth:`Series.dt.tz_localize`.
 
         Parameters
         ----------
         tz : str or tzinfo
+            
         axis : the axis to localize
+             (Default value = 0)
         level : int, str, default None
             If axis ia a MultiIndex, localize a specific level. Otherwise
-            must be None.
+            must be None. (Default value = None)
         copy : bool, default True
             Also make a copy of the underlying data.
         ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'
@@ -9512,29 +10554,32 @@
             00:30:00 UTC and at 01:30:00 UTC. In such a situation, the
             `ambiguous` parameter dictates how ambiguous times should be
             handled.
-
             - 'infer' will attempt to infer fall dst-transition hours based on
-              order
+            order
             - bool-ndarray where True signifies a DST time, False designates
-              a non-DST time (note that this flag is only applicable for
-              ambiguous times)
+            a non-DST time (note that this flag is only applicable for
+            ambiguous times)
             - 'NaT' will return NaT where there are ambiguous times
             - 'raise' will raise an AmbiguousTimeError if there are ambiguous
-              times.
+            times. (Default value = "raise")
         nonexistent : str, default 'raise'
             A nonexistent time does not exist in a particular timezone
             where clocks moved forward due to DST. Valid values are:
-
             - 'shift_forward' will shift the nonexistent time forward to the
-              closest existing time
+            closest existing time
             - 'shift_backward' will shift the nonexistent time backward to the
-              closest existing time
+            closest existing time
             - 'NaT' will return NaT where there are nonexistent times
             - timedelta objects will shift nonexistent times by the timedelta
             - 'raise' will raise an NonExistentTimeError if there are
-              nonexistent times.
-
+            nonexistent times.
             .. versionadded:: 0.24.0
+        self: FrameOrSeries :
+            
+        copy: bool_t :
+             (Default value = True)
+        nonexistent: str :
+             (Default value = "raise")
 
         Returns
         -------
@@ -9549,16 +10594,25 @@
         Examples
         --------
         Localize local times:
-
+        
+        
+        Be careful with DST changes. When there is sequential data, pandas
+        can infer the DST time:
+        
+        
+        In some cases, inferring the DST is impossible. In such cases, you can
+        pass an ndarray to the ambiguous parameter to set the DST explicitly
+        
+        
+        If the DST transition causes nonexistent times, you can shift these
+        dates forward or backward with a timedelta object or `'shift_forward'`
+        or `'shift_backward'`.
         >>> s = pd.Series([1],
         ...               index=pd.DatetimeIndex(['2018-09-15 01:30:00']))
         >>> s.tz_localize('CET')
         2018-09-15 01:30:00+02:00    1
         dtype: int64
-
-        Be careful with DST changes. When there is sequential data, pandas
-        can infer the DST time:
-
+        
         >>> s = pd.Series(range(7),
         ...               index=pd.DatetimeIndex(['2018-10-28 01:30:00',
         ...                                       '2018-10-28 02:00:00',
@@ -9576,10 +10630,7 @@
         2018-10-28 03:00:00+01:00    5
         2018-10-28 03:30:00+01:00    6
         dtype: int64
-
-        In some cases, inferring the DST is impossible. In such cases, you can
-        pass an ndarray to the ambiguous parameter to set the DST explicitly
-
+        
         >>> s = pd.Series(range(3),
         ...               index=pd.DatetimeIndex(['2018-10-28 01:20:00',
         ...                                       '2018-10-28 02:36:00',
@@ -9589,11 +10640,7 @@
         2018-10-28 02:36:00+02:00    1
         2018-10-28 03:46:00+01:00    2
         dtype: int64
-
-        If the DST transition causes nonexistent times, you can shift these
-        dates forward or backward with a timedelta object or `'shift_forward'`
-        or `'shift_backward'`.
-
+        
         >>> s = pd.Series(range(2),
         ...               index=pd.DatetimeIndex(['2015-03-29 02:30:00',
         ...                                       '2015-03-29 03:30:00']))
@@ -9624,6 +10671,23 @@
         ax = self._get_axis(axis)
 
         def _tz_localize(ax, tz, ambiguous, nonexistent):
+            """
+
+            Parameters
+            ----------
+            ax :
+                
+            tz :
+                
+            ambiguous :
+                
+            nonexistent :
+                
+
+            Returns
+            -------
+
+            """
             if not hasattr(ax, "tz_localize"):
                 if len(ax) > 0:
                     ax_name = self._get_axis_name(axis)
@@ -9654,10 +10718,14 @@
     # ----------------------------------------------------------------------
     # Numeric Methods
     def abs(self: FrameOrSeries) -> FrameOrSeries:
-        """
-        Return a Series/DataFrame with absolute numeric value of each element.
-
+        """Return a Series/DataFrame with absolute numeric value of each element.
+        
         This function only applies to elements that are all numeric.
+
+        Parameters
+        ----------
+        self: FrameOrSeries :
+            
 
         Returns
         -------
@@ -9667,16 +10735,23 @@
         See Also
         --------
         numpy.absolute : Calculate the absolute value element-wise.
-
         Notes
         -----
         For ``complex`` inputs, ``1.2 + 1j``, the absolute value is
         :math:`\\sqrt{ a^2 + b^2 }`.
-
         Examples
         --------
         Absolute numeric values in a Series.
-
+        
+        
+        Absolute numeric values in a Series with complex numbers.
+        
+        
+        Absolute numeric values in a Series with a Timedelta element.
+        
+        
+        Select rows with data closest to certain value using argsort (from
+        `StackOverflow <https://stackoverflow.com/a/17758115>`__).
         >>> s = pd.Series([-1.10, 2, -3.33, 4])
         >>> s.abs()
         0    1.10
@@ -9684,24 +10759,17 @@
         2    3.33
         3    4.00
         dtype: float64
-
-        Absolute numeric values in a Series with complex numbers.
-
+        
         >>> s = pd.Series([1.2 + 1j])
         >>> s.abs()
         0    1.56205
         dtype: float64
-
-        Absolute numeric values in a Series with a Timedelta element.
-
+        
         >>> s = pd.Series([pd.Timedelta('1 days')])
         >>> s.abs()
         0   1 days
         dtype: timedelta64[ns]
-
-        Select rows with data closest to certain value using argsort (from
-        `StackOverflow <https://stackoverflow.com/a/17758115>`__).
-
+        
         >>> df = pd.DataFrame({
         ...     'a': [4, 5, 6, 7],
         ...     'b': [10, 20, 30, 40],
@@ -9729,13 +10797,12 @@
         exclude=None,
         datetime_is_numeric=False,
     ) -> FrameOrSeries:
-        """
-        Generate descriptive statistics.
-
+        """Generate descriptive statistics.
+        
         Descriptive statistics include those that summarize the central
         tendency, dispersion and shape of a
         dataset's distribution, excluding ``NaN`` values.
-
+        
         Analyzes both numeric and object series, as well
         as ``DataFrame`` column sets of mixed data types. The output
         will vary depending on what is provided. Refer to the notes
@@ -9751,34 +10818,33 @@
         include : 'all', list-like of dtypes or None (default), optional
             A white list of data types to include in the result. Ignored
             for ``Series``. Here are the options:
-
             - 'all' : All columns of the input will be included in the output.
             - A list-like of dtypes : Limits the results to the
-              provided data types.
-              To limit the result to numeric types submit
-              ``numpy.number``. To limit it instead to object columns submit
-              the ``numpy.object`` data type. Strings
-              can also be used in the style of
-              ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To
-              select pandas categorical columns, use ``'category'``
+            provided data types.
+            To limit the result to numeric types submit
+            ``numpy.number``. To limit it instead to object columns submit
+            the ``numpy.object`` data type. Strings
+            can also be used in the style of
+            ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To
+            select pandas categorical columns, use ``'category'``
             - None (default) : The result will include all numeric columns.
         exclude : list-like of dtypes or None (default), optional,
             A black list of data types to omit from the result. Ignored
             for ``Series``. Here are the options:
-
             - A list-like of dtypes : Excludes the provided data types
-              from the result. To exclude numeric types submit
-              ``numpy.number``. To exclude object columns submit the data
-              type ``numpy.object``. Strings can also be used in the style of
-              ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To
-              exclude pandas categorical columns, use ``'category'``
+            from the result. To exclude numeric types submit
+            ``numpy.number``. To exclude object columns submit the data
+            type ``numpy.object``. Strings can also be used in the style of
+            ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To
+            exclude pandas categorical columns, use ``'category'``
             - None (default) : The result will exclude nothing.
         datetime_is_numeric : bool, default False
             Whether to treat datetime dtypes as numeric. This affects statistics
             calculated for the column. For DataFrame input, this also
             controls whether datetime columns are included by default.
-
             .. versionadded:: 1.1.0
+        self: FrameOrSeries :
+            
 
         Returns
         -------
@@ -9794,7 +10860,6 @@
         DataFrame.std: Standard deviation of the observations.
         DataFrame.select_dtypes: Subset of a DataFrame including/excluding
             columns based on their dtype.
-
         Notes
         -----
         For numeric data, the result's index will include ``count``,
@@ -9802,31 +10867,61 @@
         upper percentiles. By default the lower percentile is ``25`` and the
         upper percentile is ``75``. The ``50`` percentile is the
         same as the median.
-
+        
         For object data (e.g. strings or timestamps), the result's index
         will include ``count``, ``unique``, ``top``, and ``freq``. The ``top``
         is the most common value. The ``freq`` is the most common value's
         frequency. Timestamps also include the ``first`` and ``last`` items.
-
+        
         If multiple object values have the highest count, then the
         ``count`` and ``top`` results will be arbitrarily chosen from
         among those with the highest count.
-
+        
         For mixed data types provided via a ``DataFrame``, the default is to
         return only an analysis of numeric columns. If the dataframe consists
         only of object and categorical data without any numeric columns, the
         default is to return an analysis of both the object and categorical
         columns. If ``include='all'`` is provided as an option, the result
         will include a union of attributes of each type.
-
+        
         The `include` and `exclude` parameters can be used to limit
         which columns in a ``DataFrame`` are analyzed for the output.
         The parameters are ignored when analyzing a ``Series``.
-
         Examples
         --------
         Describing a numeric ``Series``.
-
+        
+        
+        Describing a categorical ``Series``.
+        
+        
+        Describing a timestamp ``Series``.
+        
+        
+        Describing a ``DataFrame``. By default only numeric fields
+        are returned.
+        
+        
+        Describing all columns of a ``DataFrame`` regardless of data type.
+        
+        
+        Describing a column from a ``DataFrame`` by accessing it as
+        an attribute.
+        
+        
+        Including only numeric columns in a ``DataFrame`` description.
+        
+        
+        Including only string columns in a ``DataFrame`` description.
+        
+        
+        Including only categorical columns from a ``DataFrame`` description.
+        
+        
+        Excluding numeric columns from a ``DataFrame`` description.
+        
+        
+        Excluding object columns from a ``DataFrame`` description.
         >>> s = pd.Series([1, 2, 3])
         >>> s.describe()
         count    3.0
@@ -9838,9 +10933,7 @@
         75%      2.5
         max      3.0
         dtype: float64
-
-        Describing a categorical ``Series``.
-
+        
         >>> s = pd.Series(['a', 'a', 'b', 'c'])
         >>> s.describe()
         count     4
@@ -9848,9 +10941,7 @@
         top       a
         freq      2
         dtype: object
-
-        Describing a timestamp ``Series``.
-
+        
         >>> s = pd.Series([
         ...   np.datetime64("2000-01-01"),
         ...   np.datetime64("2010-01-01"),
@@ -9865,10 +10956,7 @@
         75%      2010-01-01 00:00:00
         max      2010-01-01 00:00:00
         dtype: object
-
-        Describing a ``DataFrame``. By default only numeric fields
-        are returned.
-
+        
         >>> df = pd.DataFrame({'categorical': pd.Categorical(['d','e','f']),
         ...                    'numeric': [1, 2, 3],
         ...                    'object': ['a', 'b', 'c']
@@ -9883,9 +10971,7 @@
         50%        2.0
         75%        2.5
         max        3.0
-
-        Describing all columns of a ``DataFrame`` regardless of data type.
-
+        
         >>> df.describe(include='all')  # doctest: +SKIP
                categorical  numeric object
         count            3      3.0      3
@@ -9899,10 +10985,7 @@
         50%            NaN      2.0    NaN
         75%            NaN      2.5    NaN
         max            NaN      3.0    NaN
-
-        Describing a column from a ``DataFrame`` by accessing it as
-        an attribute.
-
+        
         >>> df.numeric.describe()
         count    3.0
         mean     2.0
@@ -9913,9 +10996,7 @@
         75%      2.5
         max      3.0
         Name: numeric, dtype: float64
-
-        Including only numeric columns in a ``DataFrame`` description.
-
+        
         >>> df.describe(include=[np.number])
                numeric
         count      3.0
@@ -9926,36 +11007,28 @@
         50%        2.0
         75%        2.5
         max        3.0
-
-        Including only string columns in a ``DataFrame`` description.
-
+        
         >>> df.describe(include=[object])  # doctest: +SKIP
                object
         count       3
         unique      3
         top         a
         freq        1
-
-        Including only categorical columns from a ``DataFrame`` description.
-
+        
         >>> df.describe(include=['category'])
                categorical
         count            3
         unique           3
         top              f
         freq             1
-
-        Excluding numeric columns from a ``DataFrame`` description.
-
+        
         >>> df.describe(exclude=[np.number])  # doctest: +SKIP
                categorical object
         count            3      3
         unique           3      3
         top              f      a
         freq             1      1
-
-        Excluding object columns from a ``DataFrame`` description.
-
+        
         >>> df.describe(exclude=[object])  # doctest: +SKIP
                categorical  numeric
         count            3      3.0
@@ -9996,6 +11069,17 @@
         formatted_percentiles = format_percentiles(percentiles)
 
         def describe_numeric_1d(series):
+            """
+
+            Parameters
+            ----------
+            series :
+                
+
+            Returns
+            -------
+
+            """
             stat_index = (
                 ["count", "mean", "std", "min"] + formatted_percentiles + ["max"]
             )
@@ -10007,6 +11091,17 @@
             return pd.Series(d, index=stat_index, name=series.name)
 
         def describe_categorical_1d(data):
+            """
+
+            Parameters
+            ----------
+            data :
+                
+
+            Returns
+            -------
+
+            """
             names = ["count", "unique"]
             objcounts = data.value_counts()
             count_unique = len(objcounts[objcounts != 0])
@@ -10056,6 +11151,17 @@
             return pd.Series(result, index=names, name=data.name, dtype=dtype)
 
         def describe_timestamp_1d(data):
+            """
+
+            Parameters
+            ----------
+            data :
+                
+
+            Returns
+            -------
+
+            """
             # GH-30164
             stat_index = ["count", "mean", "min"] + formatted_percentiles + ["max"]
             d = (
@@ -10066,6 +11172,17 @@
             return pd.Series(d, index=stat_index, name=data.name)
 
         def describe_1d(data):
+            """
+
+            Parameters
+            ----------
+            data :
+                
+
+            Returns
+            -------
+
+            """
             if is_bool_dtype(data.dtype):
                 return describe_categorical_1d(data)
             elif is_numeric_dtype(data):
@@ -10116,9 +11233,8 @@
         freq=None,
         **kwargs,
     ) -> FrameOrSeries:
-        """
-        Percentage change between the current and a prior element.
-
+        """Percentage change between the current and a prior element.
+        
         Computes the percentage change from the immediately previous row by
         default. This is useful in comparing the percentage of change in a time
         series of elements.
@@ -10126,16 +11242,18 @@
         Parameters
         ----------
         periods : int, default 1
-            Periods to shift for forming percent change.
+            Periods to shift for forming percent change. (Default value = 1)
         fill_method : str, default 'pad'
-            How to handle NAs before computing percent changes.
+            How to handle NAs before computing percent changes. (Default value = "pad")
         limit : int, default None
-            The number of consecutive NAs to fill before stopping.
+            The number of consecutive NAs to fill before stopping. (Default value = None)
         freq : DateOffset, timedelta, or str, optional
-            Increment to use from time series API (e.g. 'M' or BDay()).
-        **kwargs
+            Increment to use from time series API (e.g. 'M' or BDay()). (Default value = None)
+        **kwargs :
             Additional keyword arguments are passed into
             `DataFrame.shift` or `Series.shift`.
+        self: FrameOrSeries :
+            
 
         Returns
         -------
@@ -10148,33 +11266,46 @@
         DataFrame.diff : Compute the difference of two elements in a DataFrame.
         Series.shift : Shift the index by some number of periods.
         DataFrame.shift : Shift the index by some number of periods.
-
         Examples
         --------
         **Series**
-
+        
+        
+        
+        
+        See the percentage change in a Series where filling NAs with last
+        valid observation forward to next valid.
+        
+        
+        
+        **DataFrame**
+        
+        Percentage change in French franc, Deutsche Mark, and Italian lira from
+        1980-01-01 to 1980-03-01.
+        
+        
+        
+        Percentage of change in GOOG and APPL stock volume. Shows computing
+        the percentage change between columns.
         >>> s = pd.Series([90, 91, 85])
         >>> s
         0    90
         1    91
         2    85
         dtype: int64
-
+        
         >>> s.pct_change()
         0         NaN
         1    0.011111
         2   -0.065934
         dtype: float64
-
+        
         >>> s.pct_change(periods=2)
         0         NaN
         1         NaN
         2   -0.055556
         dtype: float64
-
-        See the percentage change in a Series where filling NAs with last
-        valid observation forward to next valid.
-
+        
         >>> s = pd.Series([90, 91, None, 85])
         >>> s
         0    90.0
@@ -10182,19 +11313,14 @@
         2     NaN
         3    85.0
         dtype: float64
-
+        
         >>> s.pct_change(fill_method='ffill')
         0         NaN
         1    0.011111
         2    0.000000
         3   -0.065934
         dtype: float64
-
-        **DataFrame**
-
-        Percentage change in French franc, Deutsche Mark, and Italian lira from
-        1980-01-01 to 1980-03-01.
-
+        
         >>> df = pd.DataFrame({
         ...     'FR': [4.0405, 4.0963, 4.3149],
         ...     'GR': [1.7246, 1.7482, 1.8519],
@@ -10205,16 +11331,13 @@
         1980-01-01  4.0405  1.7246  804.74
         1980-02-01  4.0963  1.7482  810.01
         1980-03-01  4.3149  1.8519  860.13
-
+        
         >>> df.pct_change()
                           FR        GR        IT
         1980-01-01       NaN       NaN       NaN
         1980-02-01  0.013810  0.013684  0.006549
         1980-03-01  0.053365  0.059318  0.061876
-
-        Percentage of change in GOOG and APPL stock volume. Shows computing
-        the percentage change between columns.
-
+        
         >>> df = pd.DataFrame({
         ...     '2016': [1769950, 30586265],
         ...     '2015': [1500923, 40912316],
@@ -10224,7 +11347,7 @@
                   2016      2015      2014
         GOOG   1769950   1500923   1371819
         APPL  30586265  40912316  41403351
-
+        
         >>> df.pct_change(axis='columns')
               2016      2015      2014
         GOOG   NaN -0.151997 -0.086016
@@ -10247,6 +11370,25 @@
         return rs
 
     def _agg_by_level(self, name, axis=0, level=0, skipna=True, **kwargs):
+        """
+
+        Parameters
+        ----------
+        name :
+            
+        axis :
+             (Default value = 0)
+        level :
+             (Default value = 0)
+        skipna :
+             (Default value = True)
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         if axis is None:
             raise ValueError("Must specify 'axis' when aggregating by level.")
         grouped = self.groupby(level=level, axis=axis, sort=False)
@@ -10259,9 +11401,7 @@
 
     @classmethod
     def _add_numeric_operations(cls):
-        """
-        Add the operations to the cls; evaluate the doc strings again
-        """
+        """Add the operations to the cls; evaluate the doc strings again"""
         axis_descr, name1, name2 = _doc_parms(cls)
 
         cls.any = _make_logical_function(
@@ -10299,24 +11439,22 @@
             examples="",
         )
         def mad(self, axis=None, skipna=None, level=None):
-            """
-            {desc}
+            """{desc}
 
             Parameters
             ----------
             axis : {axis_descr}
-                Axis for the function to be applied on.
+                Axis for the function to be applied on. (Default value = None)
             skipna : bool, default None
-                Exclude NA/null values when computing the result.
+                Exclude NA/null values when computing the result. (Default value = None)
             level : int or level name, default None
                 If the axis is a MultiIndex (hierarchical), count along a
-                particular level, collapsing into a {name1}.
+                particular level, collapsing into a {name1}. (Default value = None)
 
             Returns
             -------
-            {name1} or {name2} (if level specified)\
-            {see_also}\
-            {examples}
+
+            
             """
             if skipna is None:
                 skipna = True
@@ -10504,9 +11642,15 @@
 
     @classmethod
     def _add_series_or_dataframe_operations(cls):
-        """
-        Add the series or dataframe only operations to the cls; evaluate
+        """Add the series or dataframe only operations to the cls; evaluate
         the doc strings again.
+
+        Parameters
+        ----------
+
+        Returns
+        -------
+
         """
         from pandas.core.window import (
             Expanding,
@@ -10526,6 +11670,29 @@
             axis=0,
             closed=None,
         ):
+            """
+
+            Parameters
+            ----------
+            window :
+                
+            min_periods :
+                 (Default value = None)
+            center :
+                 (Default value = False)
+            win_type :
+                 (Default value = None)
+            on :
+                 (Default value = None)
+            axis :
+                 (Default value = 0)
+            closed :
+                 (Default value = None)
+
+            Returns
+            -------
+
+            """
             axis = self._get_axis_number(axis)
 
             if win_type is not None:
@@ -10555,6 +11722,21 @@
 
         @doc(Expanding)
         def expanding(self, min_periods=1, center=None, axis=0):
+            """
+
+            Parameters
+            ----------
+            min_periods :
+                 (Default value = 1)
+            center :
+                 (Default value = None)
+            axis :
+                 (Default value = 0)
+
+            Returns
+            -------
+
+            """
             axis = self._get_axis_number(axis)
             if center is not None:
                 warnings.warn(
@@ -10583,6 +11765,33 @@
             axis=0,
             times=None,
         ):
+            """
+
+            Parameters
+            ----------
+            com :
+                 (Default value = None)
+            span :
+                 (Default value = None)
+            halflife :
+                 (Default value = None)
+            alpha :
+                 (Default value = None)
+            min_periods :
+                 (Default value = 0)
+            adjust :
+                 (Default value = True)
+            ignore_na :
+                 (Default value = False)
+            axis :
+                 (Default value = 0)
+            times :
+                 (Default value = None)
+
+            Returns
+            -------
+
+            """
             axis = self._get_axis_number(axis)
             return ExponentialMovingWindow(
                 self,
@@ -10601,9 +11810,8 @@
 
     @doc(klass=_shared_doc_kwargs["klass"], axis="")
     def transform(self, func, *args, **kwargs):
-        """
-        Call ``func`` on self producing a {klass} with transformed values.
-
+        """Call ``func`` on self producing a {klass} with transformed values.
+        
         Produced {klass} will have same axis length as self.
 
         Parameters
@@ -10611,17 +11819,16 @@
         func : function, str, list or dict
             Function to use for transforming the data. If a function, must either
             work when passed a {klass} or when passed to {klass}.apply.
-
             Accepted combinations are:
-
             - function
             - string function name
             - list of functions and/or function names, e.g. ``[np.exp. 'sqrt']``
             - dict of axis labels -> functions, function names or list of such.
-        {axis}
-        *args
+        {axis} :
+            
+        *args :
             Positional arguments to pass to `func`.
-        **kwargs
+        **kwargs :
             Keyword arguments to pass to `func`.
 
         Returns
@@ -10631,15 +11838,18 @@
 
         Raises
         ------
-        ValueError : If the returned {klass} has a different length than self.
+        ValueError
+            
 
         See Also
         --------
         {klass}.agg : Only perform aggregating type operations.
         {klass}.apply : Invoke function on a {klass}.
-
         Examples
         --------
+        
+        Even though the resulting {klass} must have the same length as the
+        input {klass}, it is possible to provide several input functions:
         >>> df = pd.DataFrame({{'A': range(3), 'B': range(1, 4)}})
         >>> df
            A  B
@@ -10651,10 +11861,7 @@
         0  1  2
         1  2  3
         2  3  4
-
-        Even though the resulting {klass} must have the same length as the
-        input {klass}, it is possible to provide several input functions:
-
+        
         >>> s = pd.Series(range(3))
         >>> s
         0    0
@@ -10677,17 +11884,19 @@
     # Misc methods
 
     def _find_valid_index(self, how: str):
-        """
-        Retrieves the index of the first valid value.
+        """Retrieves the index of the first valid value.
 
         Parameters
         ----------
         how : {'first', 'last'}
             Use this parameter to change between the first or last valid index.
-
-        Returns
-        -------
-        idx_first_valid : type of index
+        how: str :
+            
+
+        Returns
+        -------
+
+        
         """
         idxpos = find_valid_index(self._values, how)
         if idxpos is None:
@@ -10696,12 +11905,15 @@
 
     @doc(position="first", klass=_shared_doc_kwargs["klass"])
     def first_valid_index(self):
-        """
-        Return index for {position} non-NA/null value.
+        """Return index for {position} non-NA/null value.
+
+        Parameters
+        ----------
 
         Returns
         -------
         scalar : type of index
+            
 
         Notes
         -----
@@ -10712,11 +11924,12 @@
 
     @doc(first_valid_index, position="last", klass=_shared_doc_kwargs["klass"])
     def last_valid_index(self):
+        """ """
         return self._find_valid_index("last")
 
 
 def _doc_parms(cls):
-    """Return a tuple of the doc parms."""
+    """ """
     axis_descr = (
         f"{{{', '.join(f'{a} ({i})' for i, a in enumerate(cls._AXIS_ORDERS))}}}"
     )
@@ -11386,6 +12599,31 @@
     see_also: str = "",
     examples: str = "",
 ) -> Callable:
+    """
+
+    Parameters
+    ----------
+    name: str :
+        
+    name1: str :
+        
+    name2: str :
+        
+    axis_descr: str :
+        
+    desc: str :
+        
+    func: Callable :
+        
+    see_also: str :
+         (Default value = "")
+    examples: str :
+         (Default value = "")
+
+    Returns
+    -------
+
+    """
     @Substitution(
         desc=desc,
         name1=name1,
@@ -11405,6 +12643,27 @@
         min_count=0,
         **kwargs,
     ):
+        """
+
+        Parameters
+        ----------
+        axis :
+             (Default value = None)
+        skipna :
+             (Default value = None)
+        level :
+             (Default value = None)
+        numeric_only :
+             (Default value = None)
+        min_count :
+             (Default value = 0)
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         if name == "sum":
             nv.validate_sum(tuple(), kwargs)
         elif name == "prod":
@@ -11442,6 +12701,31 @@
     see_also: str = "",
     examples: str = "",
 ) -> Callable:
+    """
+
+    Parameters
+    ----------
+    name: str :
+        
+    name1: str :
+        
+    name2: str :
+        
+    axis_descr: str :
+        
+    desc: str :
+        
+    func: Callable :
+        
+    see_also: str :
+         (Default value = "")
+    examples: str :
+         (Default value = "")
+
+    Returns
+    -------
+
+    """
     @Substitution(
         desc=desc,
         name1=name1,
@@ -11455,6 +12739,25 @@
     def stat_func(
         self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs
     ):
+        """
+
+        Parameters
+        ----------
+        axis :
+             (Default value = None)
+        skipna :
+             (Default value = None)
+        level :
+             (Default value = None)
+        numeric_only :
+             (Default value = None)
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         if name == "median":
             nv.validate_median(tuple(), kwargs)
         else:
@@ -11475,11 +12778,53 @@
 def _make_stat_function_ddof(
     cls, name: str, name1: str, name2: str, axis_descr: str, desc: str, func: Callable
 ) -> Callable:
+    """
+
+    Parameters
+    ----------
+    name: str :
+        
+    name1: str :
+        
+    name2: str :
+        
+    axis_descr: str :
+        
+    desc: str :
+        
+    func: Callable :
+        
+
+    Returns
+    -------
+
+    """
     @Substitution(desc=desc, name1=name1, name2=name2, axis_descr=axis_descr)
     @Appender(_num_ddof_doc)
     def stat_func(
         self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs
     ):
+        """
+
+        Parameters
+        ----------
+        axis :
+             (Default value = None)
+        skipna :
+             (Default value = None)
+        level :
+             (Default value = None)
+        ddof :
+             (Default value = 1)
+        numeric_only :
+             (Default value = None)
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         nv.validate_stat_ddof_func(tuple(), kwargs, fname=name)
         if skipna is None:
             skipna = True
@@ -11507,6 +12852,31 @@
     accum_func_name: str,
     examples: str,
 ) -> Callable:
+    """
+
+    Parameters
+    ----------
+    name: str :
+        
+    name1: str :
+        
+    name2: str :
+        
+    axis_descr: str :
+        
+    desc: str :
+        
+    accum_func: Callable :
+        
+    accum_func_name: str :
+        
+    examples: str :
+        
+
+    Returns
+    -------
+
+    """
     @Substitution(
         desc=desc,
         name1=name1,
@@ -11517,6 +12887,23 @@
     )
     @Appender(_cnum_doc)
     def cum_func(self, axis=None, skipna=True, *args, **kwargs):
+        """
+
+        Parameters
+        ----------
+        axis :
+             (Default value = None)
+        skipna :
+             (Default value = True)
+        *args :
+            
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         skipna = nv.validate_cum_func_with_skipna(skipna, args, kwargs, name)
         if axis is None:
             axis = self._stat_axis_number
@@ -11527,6 +12914,17 @@
             return cum_func(self.T, axis=0, skipna=skipna, *args, **kwargs).T
 
         def block_accum_func(blk_values):
+            """
+
+            Parameters
+            ----------
+            blk_values :
+                
+
+            Returns
+            -------
+
+            """
             values = blk_values.T if hasattr(blk_values, "T") else blk_values
 
             result = nanops.na_accum_func(values, accum_func, skipna=skipna)
@@ -11553,6 +12951,33 @@
     examples: str,
     empty_value: bool,
 ) -> Callable:
+    """
+
+    Parameters
+    ----------
+    name: str :
+        
+    name1: str :
+        
+    name2: str :
+        
+    axis_descr: str :
+        
+    desc: str :
+        
+    func: Callable :
+        
+    see_also: str :
+        
+    examples: str :
+        
+    empty_value: bool :
+        
+
+    Returns
+    -------
+
+    """
     @Substitution(
         desc=desc,
         name1=name1,
@@ -11564,6 +12989,25 @@
     )
     @Appender(_bool_doc)
     def logical_func(self, axis=0, bool_only=None, skipna=True, level=None, **kwargs):
+        """
+
+        Parameters
+        ----------
+        axis :
+             (Default value = 0)
+        bool_only :
+             (Default value = None)
+        skipna :
+             (Default value = True)
+        level :
+             (Default value = None)
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         nv.validate_logical_func(tuple(), kwargs, fname=name)
         if level is not None:
             if bool_only is not None:
