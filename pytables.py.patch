# Patch generated by Pyment v0.3.3

--- a/..//venv/lib/python3.8/site-packages/pandas/io/pytables.py
+++ b/..//venv/lib/python3.8/site-packages/pandas/io/pytables.py
@@ -68,13 +68,34 @@
 
 
 def _ensure_decoded(s):
-    """ if we have bytes, decode them to unicode """
+    """if we have bytes, decode them to unicode
+
+    Parameters
+    ----------
+    s :
+        
+
+    Returns
+    -------
+
+    """
     if isinstance(s, np.bytes_):
         s = s.decode("UTF-8")
     return s
 
 
 def _ensure_encoding(encoding):
+    """
+
+    Parameters
+    ----------
+    encoding :
+        
+
+    Returns
+    -------
+
+    """
     # set the encoding if we need
     if encoding is None:
         encoding = _default_encoding
@@ -83,11 +104,19 @@
 
 
 def _ensure_str(name):
-    """
-    Ensure that an index / column name is a str (python 3); otherwise they
+    """Ensure that an index / column name is a str (python 3); otherwise they
     may be np.string dtype. Non-string dtypes are passed through unchanged.
-
+    
     https://github.com/pandas-dev/pandas/issues/13492
+
+    Parameters
+    ----------
+    name :
+        
+
+    Returns
+    -------
+
     """
     if isinstance(name, str):
         name = str(name)
@@ -98,11 +127,21 @@
 
 
 def _ensure_term(where, scope_level: int):
-    """
-    ensure that the where is a Term or a list of Term
+    """ensure that the where is a Term or a list of Term
     this makes sure that we are capturing the scope of variables
     that are passed
     create the terms here with a frame_level=2 (we are 2 levels down)
+
+    Parameters
+    ----------
+    where :
+        
+    scope_level: int :
+        
+
+    Returns
+    -------
+
     """
     # only consider list/tuple here as an ndarray is automatically a coordinate
     # list
@@ -121,14 +160,17 @@
 
 
 class PossibleDataLossError(Exception):
+    """ """
     pass
 
 
 class ClosedFileError(Exception):
+    """ """
     pass
 
 
 class IncompatibilityWarning(Warning):
+    """ """
     pass
 
 
@@ -140,6 +182,7 @@
 
 
 class AttributeConflictWarning(Warning):
+    """ """
     pass
 
 
@@ -150,6 +193,7 @@
 
 
 class DuplicateWarning(Warning):
+    """ """
     pass
 
 
@@ -194,6 +238,7 @@
 
 
 def _tables():
+    """ """
     global _table_mod
     global _table_file_open_policy_is_strict
     if _table_mod is None:
@@ -234,7 +279,51 @@
     errors: str = "strict",
     encoding: str = "UTF-8",
 ):
-    """ store this object, close it if we opened it """
+    """store this object, close it if we opened it
+
+    Parameters
+    ----------
+    path_or_buf :
+        
+    key: str :
+        
+    value: FrameOrSeries :
+        
+    mode: str :
+         (Default value = "a")
+    complevel: Optional[int] :
+         (Default value = None)
+    complib: Optional[str] :
+         (Default value = None)
+    append: bool :
+         (Default value = False)
+    format: Optional[str] :
+         (Default value = None)
+    index: bool :
+         (Default value = True)
+    min_itemsize: Optional[Union[int :
+        
+    Dict[str :
+        
+    int]]] :
+         (Default value = None)
+    nan_rep :
+         (Default value = None)
+    dropna: Optional[bool] :
+         (Default value = None)
+    data_columns: Optional[Union[bool :
+        
+    List[str]]] :
+         (Default value = None)
+    errors: str :
+         (Default value = "strict")
+    encoding: str :
+         (Default value = "UTF-8")
+
+    Returns
+    -------
+
+    """
     if append:
         f = lambda store: store.append(
             key,
@@ -285,18 +374,17 @@
     chunksize: Optional[int] = None,
     **kwargs,
 ):
-    """
-    Read from the store, close it if we opened it.
-
+    """Read from the store, close it if we opened it.
+    
     Retrieve pandas object stored in file, optionally based on where
     criteria.
-
+    
     .. warning::
-
+    
        Pandas uses PyTables for reading and writing HDF5 files, which allows
        serializing object-dtype data with pickle when using the "fixed" format.
        Loading pickled data received from untrusted sources can be unsafe.
-
+    
        See: https://docs.python.org/3/library/pickle.html for more.
 
     Parameters
@@ -305,18 +393,15 @@
         Any valid string path is acceptable. The string could be a URL. Valid
         URL schemes include http, ftp, s3, and file. For file URLs, a host is
         expected. A local file could be: ``file://localhost/path/to/table.h5``.
-
         If you want to pass in a path object, pandas accepts any
         ``os.PathLike``.
-
         Alternatively, pandas accepts an open :class:`pandas.HDFStore` object.
-
         By file-like object, we refer to objects with a ``read()`` method,
         such as a file handler (e.g. via builtin ``open`` function)
         or ``StringIO``.
     key : object, optional
         The group identifier in the store. Can be omitted if the HDF file
-        contains a single pandas object.
+        contains a single pandas object. (Default value = None)
     mode : {'r', 'r+', 'a'}, default 'r'
         Mode to use when opening the file. Ignored if path_or_buf is a
         :class:`pandas.HDFStore`. Default is 'r'.
@@ -325,19 +410,29 @@
         See the errors argument for :func:`open` for a full list
         of options.
     where : list, optional
-        A list of Term (or convertible) objects.
+        A list of Term (or convertible) objects. (Default value = None)
     start : int, optional
         Row number to start selection.
-    stop  : int, optional
+    stop : int, optional
         Row number to stop selection.
     columns : list, optional
-        A list of columns names to return.
+        A list of columns names to return. (Default value = None)
     iterator : bool, optional
-        Return an iterator object.
+        Return an iterator object. (Default value = False)
     chunksize : int, optional
         Number of rows to include in an iteration when using an iterator.
-    **kwargs
+    **kwargs :
         Additional keyword arguments passed to HDFStore.
+    mode: str :
+         (Default value = "r")
+    errors: str :
+         (Default value = "strict")
+    start: Optional[int] :
+         (Default value = None)
+    stop: Optional[int] :
+         (Default value = None)
+    chunksize: Optional[int] :
+         (Default value = None)
 
     Returns
     -------
@@ -348,7 +443,6 @@
     --------
     DataFrame.to_hdf : Write a HDF file from a DataFrame.
     HDFStore : Low-level access to HDF files.
-
     Examples
     --------
     >>> df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])
@@ -434,7 +528,19 @@
 
 
 def _is_metadata_of(group: "Node", parent_group: "Node") -> bool:
-    """Check if a given group is a metadata group for a given parent_group."""
+    """Check if a given group is a metadata group for a given parent_group.
+
+    Parameters
+    ----------
+    group: "Node" :
+        
+    parent_group: "Node" :
+        
+
+    Returns
+    -------
+
+    """
     if group._v_depth <= parent_group._v_depth:
         return False
 
@@ -448,17 +554,16 @@
 
 
 class HDFStore:
-    """
-    Dict-like IO interface for storing pandas objects in PyTables.
-
+    """Dict-like IO interface for storing pandas objects in PyTables.
+    
     Either Fixed or Table format.
-
+    
     .. warning::
-
+    
        Pandas uses PyTables for reading and writing HDF5 files, which allows
        serializing object-dtype data with pickle when using the "fixed" format.
        Loading pickled data received from untrusted sources can be unsafe.
-
+    
        See: https://docs.python.org/3/library/pickle.html for more.
 
     Parameters
@@ -466,17 +571,16 @@
     path : str
         File path to HDF5 file.
     mode : {'a', 'w', 'r', 'r+'}, default 'a'
-
         ``'r'``
-            Read-only; no data can be modified.
+        Read-only; no data can be modified.
         ``'w'``
-            Write; a new file is created (an existing file with the same
-            name would be deleted).
+        Write; a new file is created (an existing file with the same
+        name would be deleted).
         ``'a'``
-            Append; an existing file is opened for reading and writing,
-            and if the file does not exist it is created.
+        Append; an existing file is opened for reading and writing,
+        and if the file does not exist it is created.
         ``'r+'``
-            It is similar to ``'a'``, but the file must already exist.
+        It is similar to ``'a'``, but the file must already exist.
     complevel : int, 0-9, default None
         Specifies a compression level for data.
         A value of 0 or None disables compression.
@@ -485,28 +589,31 @@
         As of v0.20.2 these additional compressors for Blosc are supported
         (default if no compressor specified: 'blosc:blosclz'):
         {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy',
-         'blosc:zlib', 'blosc:zstd'}.
+        'blosc:zlib', 'blosc:zstd'}.
         Specifying a compression library which is not available issues
         a ValueError.
     fletcher32 : bool, default False
         If applying compression use the fletcher32 checksum.
-    **kwargs
+    **kwargs :
         These parameters will be passed to the PyTables open_file method.
+
+    Returns
+    -------
 
     Examples
     --------
+    
+    **Create or load HDF5 file in-memory**
+    
+    When passing the `driver` option to the PyTables open_file method through
+    **kwargs, the HDF5 file is loaded or created in-memory and will only be
+    written when closed:
     >>> bar = pd.DataFrame(np.random.randn(10, 4))
     >>> store = pd.HDFStore('test.h5')
     >>> store['foo'] = bar   # write to HDF5
     >>> bar = store['foo']   # retrieve
     >>> store.close()
-
-    **Create or load HDF5 file in-memory**
-
-    When passing the `driver` option to the PyTables open_file method through
-    **kwargs, the HDF5 file is loaded or created in-memory and will only be
-    written when closed:
-
+    
     >>> bar = pd.DataFrame(np.random.randn(10, 4))
     >>> store = pd.HDFStore('test.h5', driver='H5FD_CORE')
     >>> store['foo'] = bar
@@ -557,12 +664,13 @@
 
     @property
     def root(self):
-        """ return the root node """
+        """ """
         self._check_if_open()
         return self._handle.root
 
     @property
     def filename(self):
+        """ """
         return self._path
 
     def __getitem__(self, key: str):
@@ -610,26 +718,23 @@
         self.close()
 
     def keys(self, include: str = "pandas") -> List[str]:
-        """
-        Return a list of keys corresponding to objects stored in HDFStore.
-
-        Parameters
-        ----------
-
+        """Return a list of keys corresponding to objects stored in HDFStore.
+
+        Parameters
+        ----------
         include : str, default 'pandas'
-                When kind equals 'pandas' return pandas objects
-                When kind equals 'native' return native HDF5 Table objects
-
-                .. versionadded:: 1.1.0
+            When kind equals 'pandas' return pandas objects
+            When kind equals 'native' return native HDF5 Table objects
+            .. versionadded:: 1.1.0
+        include: str :
+             (Default value = "pandas")
 
         Returns
         -------
         list
             List of ABSOLUTE path-names (e.g. have the leading '/').
 
-        Raises
-        ------
-        raises ValueError if kind has an illegal value
+        
         """
         if include == "pandas":
             return [n._v_pathname for n in self.groups()]
@@ -647,24 +752,26 @@
         return iter(self.keys())
 
     def items(self):
-        """
-        iterate on key->group
-        """
+        """iterate on key->group"""
         for g in self.groups():
             yield g._v_pathname, g
 
     iteritems = items
 
     def open(self, mode: str = "a", **kwargs):
-        """
-        Open the file in the specified mode
-
-        Parameters
-        ----------
-        mode : {'a', 'w', 'r', 'r+'}, default 'a'
-            See HDFStore docstring or tables.open_file for info about modes
-        **kwargs
-            These parameters will be passed to the PyTables open_file method.
+        """Open the file in the specified mode
+
+        Parameters
+        ----------
+        mode: str :
+             (Default value = "a")
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        
         """
         tables = _tables()
 
@@ -730,30 +837,30 @@
             raise
 
     def close(self):
-        """
-        Close the PyTables file handle
-        """
+        """Close the PyTables file handle"""
         if self._handle is not None:
             self._handle.close()
         self._handle = None
 
     @property
     def is_open(self) -> bool:
-        """
-        return a boolean indicating whether the file is open
-        """
+        """ """
         if self._handle is None:
             return False
         return bool(self._handle.isopen)
 
     def flush(self, fsync: bool = False):
-        """
-        Force all buffered modifications to be written to disk.
+        """Force all buffered modifications to be written to disk.
 
         Parameters
         ----------
         fsync : bool (default False)
-          call ``os.fsync()`` on the file handle to force writing to disk.
+            call ``os.fsync()`` on the file handle to force writing to disk.
+        fsync: bool :
+             (Default value = False)
+
+        Returns
+        -------
 
         Notes
         -----
@@ -771,17 +878,19 @@
                     pass
 
     def get(self, key: str):
-        """
-        Retrieve pandas object stored in file.
+        """Retrieve pandas object stored in file.
 
         Parameters
         ----------
         key : str
-
-        Returns
-        -------
-        object
-            Same type as object stored in file.
+            
+        key: str :
+            
+
+        Returns
+        -------
+
+        
         """
         with patch_pickle():
             # GH#31167 Without this patch, pickle doesn't know how to unpickle
@@ -802,40 +911,43 @@
         chunksize=None,
         auto_close: bool = False,
     ):
-        """
-        Retrieve pandas object stored in file, optionally based on where criteria.
-
+        """Retrieve pandas object stored in file, optionally based on where criteria.
+        
         .. warning::
-
+        
            Pandas uses PyTables for reading and writing HDF5 files, which allows
            serializing object-dtype data with pickle when using the "fixed" format.
            Loading pickled data received from untrusted sources can be unsafe.
-
+        
            See: https://docs.python.org/3/library/pickle.html for more.
 
         Parameters
         ----------
         key : str
-                Object being retrieved from file.
+            Object being retrieved from file.
         where : list, default None
-                List of Term (or convertible) objects, optional.
+            List of Term (or convertible) objects, optional. (Default value = None)
         start : int, default None
-                Row number to start selection.
+            Row number to start selection. (Default value = None)
         stop : int, default None
-                Row number to stop selection.
+            Row number to stop selection. (Default value = None)
         columns : list, default None
-                A list of columns that if not None, will limit the return columns.
+            A list of columns that if not None, will limit the return columns. (Default value = None)
         iterator : bool, default False
-                Returns an iterator.
+            Returns an iterator. (Default value = False)
         chunksize : int, default None
-                Number or rows to include in iteration, return an iterator.
+            Number or rows to include in iteration, return an iterator. (Default value = None)
         auto_close : bool, default False
             Should automatically close the store when finished.
-
-        Returns
-        -------
-        object
-            Retrieved object from file.
+        key: str :
+            
+        auto_close: bool :
+             (Default value = False)
+
+        Returns
+        -------
+
+        
         """
         group = self.get_node(key)
         if group is None:
@@ -848,6 +960,21 @@
 
         # function to call on iteration
         def func(_start, _stop, _where):
+            """
+
+            Parameters
+            ----------
+            _start :
+                
+            _stop :
+                
+            _where :
+                
+
+            Returns
+            -------
+
+            """
             return s.read(start=_start, stop=_stop, where=_where, columns=columns)
 
         # create the iterator
@@ -873,24 +1000,31 @@
         start: Optional[int] = None,
         stop: Optional[int] = None,
     ):
-        """
-        return the selection as an Index
-
+        """return the selection as an Index
+        
         .. warning::
-
+        
            Pandas uses PyTables for reading and writing HDF5 files, which allows
            serializing object-dtype data with pickle when using the "fixed" format.
            Loading pickled data received from untrusted sources can be unsafe.
-
+        
            See: https://docs.python.org/3/library/pickle.html for more.
 
-
-        Parameters
-        ----------
-        key : str
-        where : list of Term (or convertible) objects, optional
-        start : integer (defaults to None), row number to start selection
-        stop  : integer (defaults to None), row number to stop selection
+        Parameters
+        ----------
+        key: str :
+            
+        where :
+             (Default value = None)
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        
         """
         where = _ensure_term(where, scope_level=1)
         tbl = self.get_storer(key)
@@ -905,33 +1039,40 @@
         start: Optional[int] = None,
         stop: Optional[int] = None,
     ):
-        """
-        return a single column from the table. This is generally only useful to
+        """return a single column from the table. This is generally only useful to
         select an indexable
-
+        
         .. warning::
-
+        
            Pandas uses PyTables for reading and writing HDF5 files, which allows
            serializing object-dtype data with pickle when using the "fixed" format.
            Loading pickled data received from untrusted sources can be unsafe.
-
+        
            See: https://docs.python.org/3/library/pickle.html for more.
 
         Parameters
         ----------
         key : str
+            
         column : str
             The column of interest.
         start : int or None, default None
+            
         stop : int or None, default None
-
-        Raises
-        ------
-        raises KeyError if the column is not found (or key is not a valid
-            store)
-        raises ValueError if the column can not be extracted individually (it
-            is part of a data block)
-
+            
+        key: str :
+            
+        column: str :
+            
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        
         """
         tbl = self.get_storer(key)
         if not isinstance(tbl, Table):
@@ -950,35 +1091,43 @@
         chunksize=None,
         auto_close: bool = False,
     ):
-        """
-        Retrieve pandas objects from multiple tables.
-
+        """Retrieve pandas objects from multiple tables.
+        
         .. warning::
-
+        
            Pandas uses PyTables for reading and writing HDF5 files, which allows
            serializing object-dtype data with pickle when using the "fixed" format.
            Loading pickled data received from untrusted sources can be unsafe.
-
+        
            See: https://docs.python.org/3/library/pickle.html for more.
 
         Parameters
         ----------
         keys : a list of the tables
+            
         selector : the table to apply the where criteria (defaults to keys[0]
-            if not supplied)
+            if not supplied) (Default value = None)
         columns : the columns I want back
+             (Default value = None)
         start : integer (defaults to None), row number to start selection
-        stop  : integer (defaults to None), row number to stop selection
+             (Default value = None)
+        stop : integer (defaults to None), row number to stop selection
+             (Default value = None)
         iterator : boolean, return an iterator, default False
+             (Default value = False)
         chunksize : nrows to include in iteration, return an iterator
+             (Default value = None)
         auto_close : bool, default False
             Should automatically close the store when finished.
-
-        Raises
-        ------
-        raises KeyError if keys or selector is not found or keys is empty
-        raises TypeError if keys is not a list or tuple
-        raises ValueError if the tables are not ALL THE SAME DIMENSIONS
+        where :
+             (Default value = None)
+        auto_close: bool :
+             (Default value = False)
+
+        Returns
+        -------
+
+        
         """
         # default to single select
         where = _ensure_term(where, scope_level=1)
@@ -1033,6 +1182,21 @@
         axis = list({t.non_index_axes[0][0] for t in _tbls})[0]
 
         def func(_start, _stop, _where):
+            """
+
+            Parameters
+            ----------
+            _start :
+                
+            _stop :
+                
+            _where :
+                
+
+            Returns
+            -------
+
+            """
 
             # retrieve the objs, _where is always passed as a set of
             # coordinates here
@@ -1076,39 +1240,45 @@
         errors: str = "strict",
         track_times: bool = True,
     ):
-        """
-        Store object in HDFStore.
-
-        Parameters
-        ----------
-        key : str
-        value : {Series, DataFrame}
-        format : 'fixed(f)|table(t)', default is 'fixed'
-            Format to use when storing object in HDFStore. Value can be one of:
-
-            ``'fixed'``
-                Fixed format.  Fast writing/reading. Not-appendable, nor searchable.
-            ``'table'``
-                Table format.  Write as a PyTables Table structure which may perform
-                worse but allow more flexible operations like searching / selecting
-                subsets of the data.
-        append   : bool, default False
-            This will force Table format, append the input data to the
-            existing.
-        data_columns : list, default None
-            List of columns to create as data columns, or True to
-            use all columns. See `here
-            <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#query-via-data-columns>`__.
-        encoding : str, default None
-            Provide an encoding for strings.
-        dropna   : bool, default False, do not write an ALL nan row to
-            The store settable by the option 'io.hdf.dropna_table'.
-        track_times : bool, default True
-            Parameter is propagated to 'create_table' method of 'PyTables'.
-            If set to False it enables to have the same h5 files (same hashes)
-            independent on creation time.
-
-            .. versionadded:: 1.1.0
+        """Store object in HDFStore.
+
+        Parameters
+        ----------
+        key: str :
+            
+        value: FrameOrSeries :
+            
+        format :
+             (Default value = None)
+        index :
+             (Default value = True)
+        append :
+             (Default value = False)
+        complib :
+             (Default value = None)
+        complevel: Optional[int] :
+             (Default value = None)
+        min_itemsize: Optional[Union[int :
+            
+        Dict[str :
+            
+        int]]] :
+             (Default value = None)
+        nan_rep :
+             (Default value = None)
+        data_columns: Optional[List[str]] :
+             (Default value = None)
+        encoding :
+             (Default value = None)
+        errors: str :
+             (Default value = "strict")
+        track_times: bool :
+             (Default value = True)
+
+        Returns
+        -------
+
+        
         """
         if format is None:
             format = get_option("io.hdf.default_format") or "fixed"
@@ -1130,25 +1300,27 @@
         )
 
     def remove(self, key: str, where=None, start=None, stop=None):
-        """
-        Remove pandas object partially by specifying the where condition
+        """Remove pandas object partially by specifying the where condition
 
         Parameters
         ----------
         key : string
             Node to remove or delete rows from
         where : list of Term (or convertible) objects, optional
+             (Default value = None)
         start : integer (defaults to None), row number to start selection
-        stop  : integer (defaults to None), row number to stop selection
+             (Default value = None)
+        stop : integer (defaults to None), row number to stop selection
+             (Default value = None)
+        key: str :
+            
 
         Returns
         -------
         number of rows removed (or None if not a Table)
-
-        Raises
-        ------
-        raises KeyError if key is not a valid store
-
+            
+
+        
         """
         where = _ensure_term(where, scope_level=1)
         try:
@@ -1206,36 +1378,70 @@
         encoding=None,
         errors: str = "strict",
     ):
-        """
-        Append to Table in file. Node must already exist and be Table
+        """Append to Table in file. Node must already exist and be Table
         format.
 
         Parameters
         ----------
         key : str
+            
         value : {Series, DataFrame}
+            
         format : 'table' is the default
             Format to use when storing object in HDFStore.  Value can be one of:
-
             ``'table'``
-                Table format. Write as a PyTables Table structure which may perform
-                worse but allow more flexible operations like searching / selecting
-                subsets of the data.
-        append       : bool, default True
-            Append the input data to the existing.
+            Table format. Write as a PyTables Table structure which may perform
+            worse but allow more flexible operations like searching / selecting
+            subsets of the data. (Default value = None)
+        append : bool, default True
+            Append the input data to the existing. (Default value = True)
         data_columns : list of columns, or True, default None
             List of columns to create as indexed data columns for on-disk
             queries, or True to use all columns. By default only the axes
             of the object are indexed. See `here
             <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#query-via-data-columns>`__.
         min_itemsize : dict of columns that specify minimum str sizes
-        nan_rep      : str to use as str nan representation
-        chunksize    : size to chunk the writing
+            
+        nan_rep : str to use as str nan representation
+             (Default value = None)
+        chunksize : size to chunk the writing
+             (Default value = None)
         expectedrows : expected TOTAL row size of this table
-        encoding     : default None, provide an encoding for str
+             (Default value = None)
+        encoding : default None, provide an encoding for str
+             (Default value = None)
         dropna : bool, default False
             Do not write an ALL nan row to the store settable
             by the option 'io.hdf.dropna_table'.
+        key: str :
+            
+        value: FrameOrSeries :
+            
+        axes :
+             (Default value = None)
+        index :
+             (Default value = True)
+        complib :
+             (Default value = None)
+        complevel: Optional[int] :
+             (Default value = None)
+        columns :
+             (Default value = None)
+        min_itemsize: Optional[Union[int :
+            
+        Dict[str :
+            
+        int]]] :
+             (Default value = None)
+        dropna: Optional[bool] :
+             (Default value = None)
+        data_columns: Optional[List[str]] :
+             (Default value = None)
+        errors: str :
+             (Default value = "strict")
+
+        Returns
+        -------
 
         Notes
         -----
@@ -1281,26 +1487,34 @@
         dropna=False,
         **kwargs,
     ):
-        """
-        Append to multiple tables
+        """Append to multiple tables
 
         Parameters
         ----------
         d : a dict of table_name to table_columns, None is acceptable as the
             values of one node (this will get all the remaining columns)
         value : a pandas object
+            
         selector : a string that designates the indexable table; all of its
             columns will be designed as data_columns, unless data_columns is
             passed, in which case these are used
         data_columns : list of columns to create as data columns, or True to
-            use all columns
+            use all columns (Default value = None)
         dropna : if evaluates to True, drop rows from all tables if any single
-                 row in each table has all NaN. Default False.
+            row in each table has all NaN. Default False.
+        d: Dict :
+            
+        axes :
+             (Default value = None)
+        **kwargs :
+            
+
+        Returns
+        -------
 
         Notes
         -----
         axes parameter is currently not accepted
-
         """
         if axes is not None:
             raise TypeError(
@@ -1375,28 +1589,33 @@
         optlevel: Optional[int] = None,
         kind: Optional[str] = None,
     ):
-        """
-        Create a pytables index on the table.
+        """Create a pytables index on the table.
 
         Parameters
         ----------
         key : str
+            
         columns : None, bool, or listlike[str]
             Indicate which columns to create an index on.
-
             * False : Do not create any indexes.
             * True : Create indexes on all columns.
             * None : Create indexes on all columns.
-            * listlike : Create indexes on the given columns.
-
+            * listlike : Create indexes on the given columns. (Default value = None)
         optlevel : int or None, default None
             Optimization level, if None, pytables defaults to 6.
         kind : str or None, default None
             Kind of index, if None, pytables defaults to "medium".
-
-        Raises
-        ------
-        TypeError: raises if the node is not a table
+        key: str :
+            
+        optlevel: Optional[int] :
+             (Default value = None)
+        kind: Optional[str] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        
         """
         # version requirements
         _tables()
@@ -1409,15 +1628,17 @@
         s.create_index(columns=columns, optlevel=optlevel, kind=kind)
 
     def groups(self):
-        """
-        Return a list of all the top-level nodes.
-
+        """Return a list of all the top-level nodes.
+        
         Each node returned is not a pandas storage object.
 
-        Returns
-        -------
-        list
-            List of objects.
+        Parameters
+        ----------
+
+        Returns
+        -------
+
+        
         """
         _tables()
         self._check_if_open()
@@ -1435,33 +1656,28 @@
         ]
 
     def walk(self, where="/"):
-        """
-        Walk the pytables group hierarchy for pandas objects.
-
+        """Walk the pytables group hierarchy for pandas objects.
+        
         This generator will yield the group path, subgroups and pandas object
         names for each group.
-
+        
         Any non-pandas PyTables objects that are not a group will be ignored.
-
+        
         The `where` group itself is listed first (preorder), then each of its
         child groups (following an alphanumerical order) is also traversed,
         following the same procedure.
-
+        
         .. versionadded:: 0.24.0
 
         Parameters
         ----------
-        where : str, default "/"
-            Group where to start walking.
-
-        Yields
-        ------
-        path : str
-            Full path to a group (without trailing '/').
-        groups : list
-            Names (strings) of the groups contained in `path`.
-        leaves : list
-            Names (strings) of the pandas objects contained in `path`.
+        where :
+             (Default value = "/")
+
+        Returns
+        -------
+
+        
         """
         _tables()
         self._check_if_open()
@@ -1482,7 +1698,19 @@
             yield (g._v_pathname.rstrip("/"), groups, leaves)
 
     def get_node(self, key: str) -> Optional["Node"]:
-        """ return the node with the key or None if it does not exist """
+        """
+
+        Parameters
+        ----------
+        key: str :
+            
+
+        Returns
+        -------
+        type
+            
+
+        """
         self._check_if_open()
         if not key.startswith("/"):
             key = "/" + key
@@ -1498,7 +1726,19 @@
         return node
 
     def get_storer(self, key: str) -> Union["GenericFixed", "Table"]:
-        """ return the storer object for a key, raise if not in the file """
+        """
+
+        Parameters
+        ----------
+        key: str :
+            
+
+        Returns
+        -------
+        type
+            
+
+        """
         group = self.get_node(key)
         if group is None:
             raise KeyError(f"No object named {key} in the file")
@@ -1518,21 +1758,35 @@
         fletcher32: bool = False,
         overwrite=True,
     ):
-        """
-        Copy the existing store to a new file, updating in place.
-
-        Parameters
-        ----------
-        propindexes: bool, default True
+        """Copy the existing store to a new file, updating in place.
+
+        Parameters
+        ----------
+        propindexes : bool, default True
             Restore indexes in copied file.
-        keys       : list of keys to include in the copy (defaults to all)
-        overwrite  : overwrite (remove and replace) existing nodes in the
+        keys : list of keys to include in the copy (defaults to all)
+             (Default value = None)
+        overwrite : overwrite (remove and replace) existing nodes in the
             new store (default is True)
-        mode, complib, complevel, fletcher32 same as in HDFStore.__init__
-
-        Returns
-        -------
-        open file handle of the new store
+        mode, complib, complevel, fletcher32 same as in HDFStore.__init__ :
+            
+        file :
+            
+        mode :
+             (Default value = "w")
+        propindexes: bool :
+             (Default value = True)
+        complib :
+             (Default value = None)
+        complevel: Optional[int] :
+             (Default value = None)
+        fletcher32: bool :
+             (Default value = False)
+
+        Returns
+        -------
+
+        
         """
         new_store = HDFStore(
             file, mode=mode, complib=complib, complevel=complevel, fletcher32=fletcher32
@@ -1568,13 +1822,7 @@
         return new_store
 
     def info(self) -> str:
-        """
-        Print detailed information on the store.
-
-        Returns
-        -------
-        str
-        """
+        """Print detailed information on the store."""
         path = pprint_thing(self._path)
         output = f"{type(self)}\nFile path: {path}\n"
 
@@ -1610,11 +1858,22 @@
     # private methods
 
     def _check_if_open(self):
+        """ """
         if not self.is_open:
             raise ClosedFileError(f"{self._path} file is not open!")
 
     def _validate_format(self, format: str) -> str:
-        """ validate / deprecate formats """
+        """validate / deprecate formats
+
+        Parameters
+        ----------
+        format: str :
+            
+
+        Returns
+        -------
+
+        """
         # validate
         try:
             format = _FORMAT_MAP[format.lower()]
@@ -1631,13 +1890,44 @@
         encoding: str = "UTF-8",
         errors: str = "strict",
     ) -> Union["GenericFixed", "Table"]:
-        """ return a suitable class to operate """
+        """
+
+        Parameters
+        ----------
+        group :
+            
+        format :
+             (Default value = None)
+        value: Optional[FrameOrSeries] :
+             (Default value = None)
+        encoding: str :
+             (Default value = "UTF-8")
+        errors: str :
+             (Default value = "strict")
+
+        Returns
+        -------
+        type
+            
+
+        """
         cls: Union[Type["GenericFixed"], Type["Table"]]
 
         if value is not None and not isinstance(value, (Series, DataFrame)):
             raise TypeError("value must be None, Series, or DataFrame")
 
         def error(t):
+            """
+
+            Parameters
+            ----------
+            t :
+                
+
+            Returns
+            -------
+
+            """
             # return instead of raising so mypy can tell where we are raising
             return TypeError(
                 f"cannot properly create the storer for: [{t}] [group->"
@@ -1737,6 +2027,55 @@
         errors: str = "strict",
         track_times: bool = True,
     ):
+        """
+
+        Parameters
+        ----------
+        key: str :
+            
+        value: FrameOrSeries :
+            
+        format :
+            
+        axes :
+             (Default value = None)
+        index :
+             (Default value = True)
+        append :
+             (Default value = False)
+        complib :
+             (Default value = None)
+        complevel: Optional[int] :
+             (Default value = None)
+        fletcher32 :
+             (Default value = None)
+        min_itemsize: Optional[Union[int :
+            
+        Dict[str :
+            
+        int]]] :
+             (Default value = None)
+        chunksize :
+             (Default value = None)
+        expectedrows :
+             (Default value = None)
+        dropna :
+             (Default value = False)
+        nan_rep :
+             (Default value = None)
+        data_columns :
+             (Default value = None)
+        encoding :
+             (Default value = None)
+        errors: str :
+             (Default value = "strict")
+        track_times: bool :
+             (Default value = True)
+
+        Returns
+        -------
+
+        """
         group = self.get_node(key)
 
         # we make this assertion for mypy; the get_node call will already
@@ -1805,30 +2144,24 @@
             s.create_index(columns=index)
 
     def _read_group(self, group: "Node"):
+        """
+
+        Parameters
+        ----------
+        group: "Node" :
+            
+
+        Returns
+        -------
+
+        """
         s = self._create_storer(group)
         s.infer_axes()
         return s.read()
 
 
 class TableIterator:
-    """
-    Define the iteration interface on a table
-
-    Parameters
-    ----------
-    store : HDFStore
-    s     : the referred storer
-    func  : the function to execute the query
-    where : the where of the query
-    nrows : the rows to iterate on
-    start : the passed start value (default is None)
-    stop  : the passed stop value (default is None)
-    iterator : bool, default False
-        Whether to use the default iterator.
-    chunksize : the passed chunking value (default is 100000)
-    auto_close : bool, default False
-        Whether to automatically close the store at the end of iteration.
-    """
+    """Define the iteration interface on a table"""
 
     chunksize: Optional[int]
     store: HDFStore
@@ -1893,10 +2226,22 @@
         self.close()
 
     def close(self):
+        """ """
         if self.auto_close:
             self.store.close()
 
     def get_result(self, coordinates: bool = False):
+        """
+
+        Parameters
+        ----------
+        coordinates: bool :
+             (Default value = False)
+
+        Returns
+        -------
+
+        """
 
         #  return the actual iterator
         if self.chunksize is not None:
@@ -1924,18 +2269,7 @@
 
 
 class IndexCol:
-    """
-    an index column description class
-
-    Parameters
-    ----------
-    axis   : axis which I reference
-    values : the ndarray like converted values
-    kind   : a string description of this type
-    typ    : the pytables type
-    pos    : the position in the pytables
-
-    """
+    """an index column description class"""
 
     is_an_indexable = True
     is_data_indexable = True
@@ -1990,15 +2324,27 @@
 
     @property
     def itemsize(self) -> int:
+        """ """
         # Assumes self.typ has already been initialized
         return self.typ.itemsize
 
     @property
     def kind_attr(self) -> str:
+        """ """
         return f"{self.name}_kind"
 
     def set_pos(self, pos: int):
-        """ set the position of this column in the Table """
+        """set the position of this column in the Table
+
+        Parameters
+        ----------
+        pos: int :
+            
+
+        Returns
+        -------
+
+        """
         self.pos = pos
         if pos is not None and self.typ is not None:
             self.typ._v_pos = pos
@@ -2026,15 +2372,29 @@
 
     @property
     def is_indexed(self) -> bool:
-        """ return whether I am an indexed column """
+        """ """
         if not hasattr(self.table, "cols"):
             # e.g. if infer hasn't been called yet, self.table will be None.
             return False
         return getattr(self.table.cols, self.cname).is_indexed
 
     def convert(self, values: np.ndarray, nan_rep, encoding: str, errors: str):
-        """
-        Convert the data from this selection to the appropriate pandas type.
+        """Convert the data from this selection to the appropriate pandas type.
+
+        Parameters
+        ----------
+        values: np.ndarray :
+            
+        nan_rep :
+            
+        encoding: str :
+            
+        errors: str :
+            
+
+        Returns
+        -------
+
         """
         assert isinstance(values, np.ndarray), type(values)
 
@@ -2065,35 +2425,45 @@
         return new_pd_index, new_pd_index
 
     def take_data(self):
-        """ return the values"""
+        """ """
         return self.values
 
     @property
     def attrs(self):
+        """ """
         return self.table._v_attrs
 
     @property
     def description(self):
+        """ """
         return self.table.description
 
     @property
     def col(self):
-        """ return my current col description """
+        """ """
         return getattr(self.description, self.cname, None)
 
     @property
     def cvalues(self):
-        """ return my cython values """
+        """ """
         return self.values
 
     def __iter__(self):
         return iter(self.values)
 
     def maybe_set_size(self, min_itemsize=None):
-        """
-        maybe set a string col itemsize:
+        """maybe set a string col itemsize:
             min_itemsize can be an integer or a dict with this columns name
             with an integer size
+
+        Parameters
+        ----------
+        min_itemsize :
+             (Default value = None)
+
+        Returns
+        -------
+
         """
         if _ensure_decoded(self.kind) == "string":
 
@@ -2104,9 +2474,23 @@
                 self.typ = _tables().StringCol(itemsize=min_itemsize, pos=self.pos)
 
     def validate_names(self):
+        """ """
         pass
 
     def validate_and_set(self, handler: "AppendableTable", append: bool):
+        """
+
+        Parameters
+        ----------
+        handler: "AppendableTable" :
+            
+        append: bool :
+            
+
+        Returns
+        -------
+
+        """
         self.table = handler.table
         self.validate_col()
         self.validate_attr(append)
@@ -2115,7 +2499,17 @@
         self.set_attr()
 
     def validate_col(self, itemsize=None):
-        """ validate this column: return the compared against itemsize """
+        """validate this column: return the compared against itemsize
+
+        Parameters
+        ----------
+        itemsize :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         # validate this column for string truncation (or reset to the max size)
         if _ensure_decoded(self.kind) == "string":
             c = self.col
@@ -2134,6 +2528,17 @@
         return None
 
     def validate_attr(self, append: bool):
+        """
+
+        Parameters
+        ----------
+        append: bool :
+            
+
+        Returns
+        -------
+
+        """
         # check for backwards incompatibility
         if append:
             existing_kind = getattr(self.attrs, self.kind_attr, None)
@@ -2143,9 +2548,17 @@
                 )
 
     def update_info(self, info):
-        """
-        set/update the info for this indexable with the key/value
+        """set/update the info for this indexable with the key/value
         if there is a conflict raise/warn as needed
+
+        Parameters
+        ----------
+        info :
+            
+
+        Returns
+        -------
+
         """
         for key in self._info_fields:
 
@@ -2175,17 +2588,37 @@
                     idx[key] = value
 
     def set_info(self, info):
-        """ set my state from the passed info """
+        """set my state from the passed info
+
+        Parameters
+        ----------
+        info :
+            
+
+        Returns
+        -------
+
+        """
         idx = info.get(self.name)
         if idx is not None:
             self.__dict__.update(idx)
 
     def set_attr(self):
-        """ set the kind for this column """
+        """set the kind for this column"""
         setattr(self.attrs, self.kind_attr, self.kind)
 
     def validate_metadata(self, handler: "AppendableTable"):
-        """ validate that kind=category does not change the categories """
+        """validate that kind=category does not change the categories
+
+        Parameters
+        ----------
+        handler: "AppendableTable" :
+            
+
+        Returns
+        -------
+
+        """
         if self.meta == "category":
             new_metadata = self.metadata
             cur_metadata = handler.read_metadata(self.cname)
@@ -2200,28 +2633,47 @@
                 )
 
     def write_metadata(self, handler: "AppendableTable"):
-        """ set the meta data """
+        """set the meta data
+
+        Parameters
+        ----------
+        handler: "AppendableTable" :
+            
+
+        Returns
+        -------
+
+        """
         if self.metadata is not None:
             handler.write_metadata(self.cname, self.metadata)
 
 
 class GenericIndexCol(IndexCol):
-    """ an index which is not represented in the data of the table """
+    """an index which is not represented in the data of the table"""
 
     @property
     def is_indexed(self) -> bool:
+        """ """
         return False
 
     def convert(self, values: np.ndarray, nan_rep, encoding: str, errors: str):
-        """
-        Convert the data from this selection to the appropriate pandas type.
-
-        Parameters
-        ----------
-        values : np.ndarray
-        nan_rep : str
-        encoding : str
-        errors : str
+        """Convert the data from this selection to the appropriate pandas type.
+
+        Parameters
+        ----------
+        values: np.ndarray :
+            
+        nan_rep :
+            
+        encoding: str :
+            
+        errors: str :
+            
+
+        Returns
+        -------
+
+        
         """
         assert isinstance(values, np.ndarray), type(values)
 
@@ -2229,21 +2681,12 @@
         return values, values
 
     def set_attr(self):
+        """ """
         pass
 
 
 class DataCol(IndexCol):
-    """
-    a data holding column, by definition this is not indexable
-
-    Parameters
-    ----------
-    data   : the actual data
-    cname  : the column name in the table to hold the data (typically
-                values)
-    meta   : a string description of the metadata
-    metadata : the actual metadata
-    """
+    """a data holding column, by definition this is not indexable"""
 
     is_an_indexable = False
     is_data_indexable = False
@@ -2283,10 +2726,12 @@
 
     @property
     def dtype_attr(self) -> str:
+        """ """
         return f"{self.name}_dtype"
 
     @property
     def meta_attr(self) -> str:
+        """ """
         return f"{self.name}_meta"
 
     def __repr__(self) -> str:
@@ -2310,6 +2755,17 @@
         )
 
     def set_data(self, data: ArrayLike):
+        """
+
+        Parameters
+        ----------
+        data: ArrayLike :
+            
+
+        Returns
+        -------
+
+        """
         assert data is not None
         assert self.dtype is None
 
@@ -2320,13 +2776,21 @@
         self.kind = _dtype_to_kind(dtype_name)
 
     def take_data(self):
-        """ return the data """
+        """ """
         return self.data
 
     @classmethod
     def _get_atom(cls, values: ArrayLike) -> "Col":
-        """
-        Get an appropriately typed and shaped pytables.Col object for values.
+        """Get an appropriately typed and shaped pytables.Col object for values.
+
+        Parameters
+        ----------
+        values: ArrayLike :
+            
+
+        Returns
+        -------
+
         """
         dtype = values.dtype
         itemsize = dtype.itemsize  # type: ignore
@@ -2357,11 +2821,36 @@
 
     @classmethod
     def get_atom_string(cls, shape, itemsize):
+        """
+
+        Parameters
+        ----------
+        shape :
+            
+        itemsize :
+            
+
+        Returns
+        -------
+
+        """
         return _tables().StringCol(itemsize=itemsize, shape=shape[0])
 
     @classmethod
     def get_atom_coltype(cls, kind: str) -> Type["Col"]:
-        """ return the PyTables column class for this column """
+        """
+
+        Parameters
+        ----------
+        kind: str :
+            
+
+        Returns
+        -------
+        type
+            
+
+        """
         if kind.startswith("uint"):
             k4 = kind[4:]
             col_name = f"UInt{k4}Col"
@@ -2376,27 +2865,73 @@
 
     @classmethod
     def get_atom_data(cls, shape, kind: str) -> "Col":
+        """
+
+        Parameters
+        ----------
+        shape :
+            
+        kind: str :
+            
+
+        Returns
+        -------
+
+        """
         return cls.get_atom_coltype(kind=kind)(shape=shape[0])
 
     @classmethod
     def get_atom_datetime64(cls, shape):
+        """
+
+        Parameters
+        ----------
+        shape :
+            
+
+        Returns
+        -------
+
+        """
         return _tables().Int64Col(shape=shape[0])
 
     @classmethod
     def get_atom_timedelta64(cls, shape):
+        """
+
+        Parameters
+        ----------
+        shape :
+            
+
+        Returns
+        -------
+
+        """
         return _tables().Int64Col(shape=shape[0])
 
     @property
     def shape(self):
+        """ """
         return getattr(self.data, "shape", None)
 
     @property
     def cvalues(self):
-        """ return my cython values """
+        """ """
         return self.data
 
     def validate_attr(self, append):
-        """validate that we have the same order as the existing & same dtype"""
+        """validate that we have the same order as the existing & same dtype
+
+        Parameters
+        ----------
+        append :
+            
+
+        Returns
+        -------
+
+        """
         if append:
             existing_fields = getattr(self.attrs, self.kind_attr, None)
             if existing_fields is not None and existing_fields != list(self.values):
@@ -2409,20 +2944,29 @@
                 )
 
     def convert(self, values: np.ndarray, nan_rep, encoding: str, errors: str):
-        """
-        Convert the data from this selection to the appropriate pandas type.
+        """Convert the data from this selection to the appropriate pandas type.
 
         Parameters
         ----------
         values : np.ndarray
+            
         nan_rep :
+            
         encoding : str
+            
         errors : str
-
-        Returns
-        -------
-        index : listlike to become an Index
-        data : ndarraylike to become a column
+            
+        values: np.ndarray :
+            
+        encoding: str :
+            
+        errors: str :
+            
+
+        Returns
+        -------
+
+        
         """
         assert isinstance(values, np.ndarray), type(values)
 
@@ -2512,7 +3056,7 @@
         return self.values, converted
 
     def set_attr(self):
-        """ set the data for this column """
+        """set the data for this column"""
         setattr(self.attrs, self.kind_attr, self.values)
         setattr(self.attrs, self.meta_attr, self.meta)
         assert self.dtype is not None
@@ -2520,49 +3064,99 @@
 
 
 class DataIndexableCol(DataCol):
-    """ represent a data column that can be indexed """
+    """represent a data column that can be indexed"""
 
     is_data_indexable = True
 
     def validate_names(self):
+        """ """
         if not Index(self.values).is_object():
             # TODO: should the message here be more specifically non-str?
             raise ValueError("cannot have non-object label DataIndexableCol")
 
     @classmethod
     def get_atom_string(cls, shape, itemsize):
+        """
+
+        Parameters
+        ----------
+        shape :
+            
+        itemsize :
+            
+
+        Returns
+        -------
+
+        """
         return _tables().StringCol(itemsize=itemsize)
 
     @classmethod
     def get_atom_data(cls, shape, kind: str) -> "Col":
+        """
+
+        Parameters
+        ----------
+        shape :
+            
+        kind: str :
+            
+
+        Returns
+        -------
+
+        """
         return cls.get_atom_coltype(kind=kind)()
 
     @classmethod
     def get_atom_datetime64(cls, shape):
+        """
+
+        Parameters
+        ----------
+        shape :
+            
+
+        Returns
+        -------
+
+        """
         return _tables().Int64Col()
 
     @classmethod
     def get_atom_timedelta64(cls, shape):
+        """
+
+        Parameters
+        ----------
+        shape :
+            
+
+        Returns
+        -------
+
+        """
         return _tables().Int64Col()
 
 
 class GenericDataIndexableCol(DataIndexableCol):
-    """ represent a generic pytables data column """
+    """represent a generic pytables data column"""
 
     pass
 
 
 class Fixed:
-    """
-    represent an object in my store
+    """represent an object in my store
     facilitate read/write of various types of objects
     this is an abstract base class
 
     Parameters
     ----------
-    parent : HDFStore
-    group : Node
-        The group node where the table resides.
+
+    Returns
+    -------
+
+    
     """
 
     pandas_kind: str
@@ -2592,11 +3186,12 @@
 
     @property
     def is_old_version(self) -> bool:
+        """ """
         return self.version[0] <= 0 and self.version[1] <= 10 and self.version[2] < 1
 
     @property
     def version(self) -> Tuple[int, int, int]:
-        """ compute and set our version """
+        """compute and set our version"""
         version = _ensure_decoded(getattr(self.group._v_attrs, "pandas_version", None))
         try:
             version = tuple(int(x) for x in version.split("."))
@@ -2608,6 +3203,7 @@
 
     @property
     def pandas_type(self):
+        """ """
         return _ensure_decoded(getattr(self.group._v_attrs, "pandas_type", None))
 
     def __repr__(self) -> str:
@@ -2622,78 +3218,105 @@
         return self.pandas_type
 
     def set_object_info(self):
-        """ set my pandas type & version """
+        """set my pandas type & version"""
         self.attrs.pandas_type = str(self.pandas_kind)
         self.attrs.pandas_version = str(_version)
 
     def copy(self):
+        """ """
         new_self = copy.copy(self)
         return new_self
 
     @property
     def shape(self):
+        """ """
         return self.nrows
 
     @property
     def pathname(self):
+        """ """
         return self.group._v_pathname
 
     @property
     def _handle(self):
+        """ """
         return self.parent._handle
 
     @property
     def _filters(self):
+        """ """
         return self.parent._filters
 
     @property
     def _complevel(self) -> int:
+        """ """
         return self.parent._complevel
 
     @property
     def _fletcher32(self) -> bool:
+        """ """
         return self.parent._fletcher32
 
     @property
     def attrs(self):
+        """ """
         return self.group._v_attrs
 
     def set_attrs(self):
-        """ set our object attributes """
+        """set our object attributes"""
         pass
 
     def get_attrs(self):
-        """ get our object attributes """
+        """get our object attributes"""
         pass
 
     @property
     def storable(self):
-        """ return my storable """
+        """ """
         return self.group
 
     @property
     def is_exists(self) -> bool:
+        """ """
         return False
 
     @property
     def nrows(self):
+        """ """
         return getattr(self.storable, "nrows", None)
 
     def validate(self, other):
-        """ validate against an existing storable """
+        """validate against an existing storable
+
+        Parameters
+        ----------
+        other :
+            
+
+        Returns
+        -------
+
+        """
         if other is None:
             return
         return True
 
     def validate_version(self, where=None):
-        """ are we trying to operate on an old version? """
+        """are we trying to operate on an old version?
+
+        Parameters
+        ----------
+        where :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         return True
 
     def infer_axes(self):
-        """
-        infer the axes of my storer
-        return a boolean indicating if we have a valid storer or not
-        """
+        """infer the axes of my storer"""
         s = self.storable
         if s is None:
             return False
@@ -2707,11 +3330,39 @@
         start: Optional[int] = None,
         stop: Optional[int] = None,
     ):
+        """
+
+        Parameters
+        ----------
+        where :
+             (Default value = None)
+        columns :
+             (Default value = None)
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         raise NotImplementedError(
             "cannot read on an abstract storer: subclasses should implement"
         )
 
     def write(self, **kwargs):
+        """
+
+        Parameters
+        ----------
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         raise NotImplementedError(
             "cannot write on an abstract storer: subclasses should implement"
         )
@@ -2719,9 +3370,21 @@
     def delete(
         self, where=None, start: Optional[int] = None, stop: Optional[int] = None
     ):
-        """
-        support fully deleting the node in its entirety (only) - where
+        """support fully deleting the node in its entirety (only) - where
         specification must be None
+
+        Parameters
+        ----------
+        where :
+             (Default value = None)
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
         """
         if com.all_none(where, start, stop):
             self._handle.remove_node(self.group, recursive=True)
@@ -2731,7 +3394,7 @@
 
 
 class GenericFixed(Fixed):
-    """ a generified fixed version """
+    """a generified fixed version"""
 
     _index_type_map = {DatetimeIndex: "datetime", PeriodIndex: "period"}
     _reverse_index_map = {v: k for k, v in _index_type_map.items()}
@@ -2739,18 +3402,56 @@
 
     # indexer helpers
     def _class_to_alias(self, cls) -> str:
+        """ """
         return self._index_type_map.get(cls, "")
 
     def _alias_to_class(self, alias):
+        """
+
+        Parameters
+        ----------
+        alias :
+            
+
+        Returns
+        -------
+
+        """
         if isinstance(alias, type):  # pragma: no cover
             # compat: for a short period of time master stored types
             return alias
         return self._reverse_index_map.get(alias, Index)
 
     def _get_index_factory(self, klass):
+        """
+
+        Parameters
+        ----------
+        klass :
+            
+
+        Returns
+        -------
+
+        """
         if klass == DatetimeIndex:
 
             def f(values, freq=None, tz=None):
+                """
+
+                Parameters
+                ----------
+                values :
+                    
+                freq :
+                     (Default value = None)
+                tz :
+                     (Default value = None)
+
+                Returns
+                -------
+
+                """
                 # data are already in UTC, localize and convert if tz present
                 dta = DatetimeArray._simple_new(values.values, freq=freq)
                 result = DatetimeIndex._simple_new(dta, name=None)
@@ -2762,6 +3463,21 @@
         elif klass == PeriodIndex:
 
             def f(values, freq=None, tz=None):
+                """
+
+                Parameters
+                ----------
+                values :
+                    
+                freq :
+                     (Default value = None)
+                tz :
+                     (Default value = None)
+
+                Returns
+                -------
+
+                """
                 parr = PeriodArray._simple_new(values, freq=freq)
                 return PeriodIndex._simple_new(parr, name=None)
 
@@ -2771,7 +3487,17 @@
 
     def validate_read(self, columns, where):
         """
-        raise if any keywords are passed which are not-None
+
+        Parameters
+        ----------
+        columns :
+            
+        where :
+            
+
+        Returns
+        -------
+
         """
         if columns is not None:
             raise TypeError(
@@ -2786,27 +3512,55 @@
 
     @property
     def is_exists(self) -> bool:
+        """ """
         return True
 
     def set_attrs(self):
-        """ set our object attributes """
+        """set our object attributes"""
         self.attrs.encoding = self.encoding
         self.attrs.errors = self.errors
 
     def get_attrs(self):
-        """ retrieve our attributes """
+        """retrieve our attributes"""
         self.encoding = _ensure_encoding(getattr(self.attrs, "encoding", None))
         self.errors = _ensure_decoded(getattr(self.attrs, "errors", "strict"))
         for n in self.attributes:
             setattr(self, n, _ensure_decoded(getattr(self.attrs, n, None)))
 
     def write(self, obj, **kwargs):
+        """
+
+        Parameters
+        ----------
+        obj :
+            
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         self.set_attrs()
 
     def read_array(
         self, key: str, start: Optional[int] = None, stop: Optional[int] = None
     ):
-        """ read an array for the specified node (off of group """
+        """read an array for the specified node (off of group
+
+        Parameters
+        ----------
+        key: str :
+            
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         import tables
 
         node = getattr(self.group, key)
@@ -2843,6 +3597,21 @@
     def read_index(
         self, key: str, start: Optional[int] = None, stop: Optional[int] = None
     ) -> Index:
+        """
+
+        Parameters
+        ----------
+        key: str :
+            
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         variety = _ensure_decoded(getattr(self.attrs, f"{key}_variety"))
 
         if variety == "multi":
@@ -2855,6 +3624,19 @@
             raise TypeError(f"unrecognized index variety: {variety}")
 
     def write_index(self, key: str, index: Index):
+        """
+
+        Parameters
+        ----------
+        key: str :
+            
+        index: Index :
+            
+
+        Returns
+        -------
+
+        """
         if isinstance(index, MultiIndex):
             setattr(self.attrs, f"{key}_variety", "multi")
             self.write_multi_index(key, index)
@@ -2878,6 +3660,19 @@
                 node._v_attrs.tz = _get_tz(index.tz)
 
     def write_multi_index(self, key: str, index: MultiIndex):
+        """
+
+        Parameters
+        ----------
+        key: str :
+            
+        index: MultiIndex :
+            
+
+        Returns
+        -------
+
+        """
         setattr(self.attrs, f"{key}_nlevels", index.nlevels)
 
         for i, (lev, level_codes, name) in enumerate(
@@ -2905,6 +3700,21 @@
     def read_multi_index(
         self, key: str, start: Optional[int] = None, stop: Optional[int] = None
     ) -> MultiIndex:
+        """
+
+        Parameters
+        ----------
+        key: str :
+            
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         nlevels = getattr(self.attrs, f"{key}_nlevels")
 
         levels = []
@@ -2928,6 +3738,21 @@
     def read_index_node(
         self, node: "Node", start: Optional[int] = None, stop: Optional[int] = None
     ) -> Index:
+        """
+
+        Parameters
+        ----------
+        node: "Node" :
+            
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         data = node[start:stop]
         # If the index was an empty array write_array_empty() will
         # have written a sentinel. Here we replace it with the original.
@@ -2978,7 +3803,19 @@
         return index
 
     def write_array_empty(self, key: str, value: ArrayLike):
-        """ write a 0-len array """
+        """write a 0-len array
+
+        Parameters
+        ----------
+        key: str :
+            
+        value: ArrayLike :
+            
+
+        Returns
+        -------
+
+        """
         # ugly hack for length 0 axes
         arr = np.empty((1,) * value.ndim)
         self._handle.create_array(self.group, key, arr)
@@ -2987,6 +3824,21 @@
         node._v_attrs.shape = value.shape
 
     def write_array(self, key: str, value: ArrayLike, items: Optional[Index] = None):
+        """
+
+        Parameters
+        ----------
+        key: str :
+            
+        value: ArrayLike :
+            
+        items: Optional[Index] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         # TODO: we only have one test that gets here, the only EA
         #  that gets passed is DatetimeArray, and we never have
         #  both self._filters and EA
@@ -3071,6 +3923,7 @@
 
 
 class SeriesFixed(GenericFixed):
+    """ """
     pandas_kind = "series"
     attributes = ["name"]
 
@@ -3078,6 +3931,7 @@
 
     @property
     def shape(self):
+        """ """
         try:
             return (len(self.group.values),)
         except (TypeError, AttributeError):
@@ -3090,12 +3944,42 @@
         start: Optional[int] = None,
         stop: Optional[int] = None,
     ):
+        """
+
+        Parameters
+        ----------
+        where :
+             (Default value = None)
+        columns :
+             (Default value = None)
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         self.validate_read(columns, where)
         index = self.read_index("index", start=start, stop=stop)
         values = self.read_array("values", start=start, stop=stop)
         return Series(values, index=index, name=self.name)
 
     def write(self, obj, **kwargs):
+        """
+
+        Parameters
+        ----------
+        obj :
+            
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         super().write(obj, **kwargs)
         self.write_index("index", obj.index)
         self.write_array("values", obj.values)
@@ -3103,12 +3987,14 @@
 
 
 class BlockManagerFixed(GenericFixed):
+    """ """
     attributes = ["ndim", "nblocks"]
 
     nblocks: int
 
     @property
     def shape(self):
+        """ """
         try:
             ndim = self.ndim
 
@@ -3141,6 +4027,23 @@
         start: Optional[int] = None,
         stop: Optional[int] = None,
     ):
+        """
+
+        Parameters
+        ----------
+        where :
+             (Default value = None)
+        columns :
+             (Default value = None)
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         # start, stop applied to rows, so 0th axis only
         self.validate_read(columns, where)
         select_axis = self.obj_type()._get_block_manager_axis(0)
@@ -3172,6 +4075,19 @@
         return DataFrame(columns=axes[0], index=axes[1])
 
     def write(self, obj, **kwargs):
+        """
+
+        Parameters
+        ----------
+        obj :
+            
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         super().write(obj, **kwargs)
         data = obj._mgr
         if not data.is_consolidated():
@@ -3193,20 +4109,20 @@
 
 
 class FrameFixed(BlockManagerFixed):
+    """ """
     pandas_kind = "frame"
     obj_type = DataFrame
 
 
 class Table(Fixed):
-    """
-    represent a table:
+    """represent a table:
         facilitate read/write of various types of tables
-
+    
     Attrs in Table Node
     -------------------
     These are attributes that are store in the main table node, they are
     necessary to recreate these tables when read back in.
-
+    
     index_axes    : a list of tuples of the (original indexing axis and
         index column)
     non_index_axes: a list of tuples of the (original index axis and
@@ -3220,6 +4136,13 @@
         objects
     levels        : the names of levels
     metadata      : the names of the metadata columns
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
 
     pandas_kind = "wide_table"
@@ -3258,6 +4181,7 @@
 
     @property
     def table_type_short(self) -> str:
+        """ """
         return self.table_type.split("_")[0]
 
     def __repr__(self) -> str:
@@ -3286,7 +4210,17 @@
         return None
 
     def validate(self, other):
-        """ validate against an existing table """
+        """validate against an existing table
+
+        Parameters
+        ----------
+        other :
+            
+
+        Returns
+        -------
+
+        """
         if other is None:
             return
 
@@ -3322,9 +4256,17 @@
         return isinstance(self.levels, list)
 
     def validate_multiindex(self, obj):
-        """
-        validate that we can store the multi-index; reset and return the
+        """validate that we can store the multi-index; reset and return the
         new object
+
+        Parameters
+        ----------
+        obj :
+            
+
+        Returns
+        -------
+
         """
         levels = [
             l if l is not None else f"level_{i}" for i, l in enumerate(obj.index.names)
@@ -3338,47 +4280,52 @@
 
     @property
     def nrows_expected(self) -> int:
-        """ based on our axes, compute the expected nrows """
+        """based on our axes, compute the expected nrows"""
         return np.prod([i.cvalues.shape[0] for i in self.index_axes])
 
     @property
     def is_exists(self) -> bool:
-        """ has this table been created """
+        """has this table been created"""
         return "table" in self.group
 
     @property
     def storable(self):
+        """ """
         return getattr(self.group, "table", None)
 
     @property
     def table(self):
-        """ return the table group (this is my storable) """
+        """ """
         return self.storable
 
     @property
     def dtype(self):
+        """ """
         return self.table.dtype
 
     @property
     def description(self):
+        """ """
         return self.table.description
 
     @property
     def axes(self):
+        """ """
         return itertools.chain(self.index_axes, self.values_axes)
 
     @property
     def ncols(self) -> int:
-        """ the number of total columns in the values axes """
+        """the number of total columns in the values axes"""
         return sum(len(a.values) for a in self.values_axes)
 
     @property
     def is_transposed(self) -> bool:
+        """ """
         return False
 
     @property
     def data_orientation(self):
-        """return a tuple of my permutated axes, non_indexable at the front"""
+        """ """
         return tuple(
             itertools.chain(
                 [int(a[0]) for a in self.non_index_axes],
@@ -3387,7 +4334,7 @@
         )
 
     def queryables(self) -> Dict[str, Any]:
-        """ return a dict of the kinds allowable columns for this object """
+        """ """
         # mypy doesn't recognize DataFrame._AXIS_NAMES, so we re-write it here
         axis_names = {0: "index", 1: "columns"}
 
@@ -3403,27 +4350,45 @@
         #  List[Tuple[Any, None]]; expected List[Tuple[str, IndexCol]]
 
     def index_cols(self):
-        """ return a list of my index cols """
+        """ """
         # Note: each `i.cname` below is assured to be a str.
         return [(i.axis, i.cname) for i in self.index_axes]
 
     def values_cols(self) -> List[str]:
-        """ return a list of my values cols """
+        """ """
         return [i.cname for i in self.values_axes]
 
     def _get_metadata_path(self, key: str) -> str:
-        """ return the metadata pathname for this key """
+        """
+
+        Parameters
+        ----------
+        key: str :
+            
+
+        Returns
+        -------
+        type
+            
+
+        """
         group = self.group._v_pathname
         return f"{group}/meta/{key}/meta"
 
     def write_metadata(self, key: str, values: np.ndarray):
-        """
-        Write out a metadata array to the key as a fixed-format Series.
-
-        Parameters
-        ----------
-        key : str
-        values : ndarray
+        """Write out a metadata array to the key as a fixed-format Series.
+
+        Parameters
+        ----------
+        key: str :
+            
+        values: np.ndarray :
+            
+
+        Returns
+        -------
+
+        
         """
         values = Series(values)
         self.parent.put(
@@ -3436,13 +4401,25 @@
         )
 
     def read_metadata(self, key: str):
-        """ return the meta data array for this key """
+        """
+
+        Parameters
+        ----------
+        key: str :
+            
+
+        Returns
+        -------
+        type
+            
+
+        """
         if getattr(getattr(self.group, "meta", None), key, None) is not None:
             return self.parent.select(self._get_metadata_path(key))
         return None
 
     def set_attrs(self):
-        """ set our table type & indexables """
+        """set our table type & indexables"""
         self.attrs.table_type = str(self.table_type)
         self.attrs.index_cols = self.index_cols()
         self.attrs.values_cols = self.values_cols()
@@ -3455,7 +4432,7 @@
         self.attrs.info = self.info
 
     def get_attrs(self):
-        """ retrieve our attributes """
+        """retrieve our attributes"""
         self.non_index_axes = getattr(self.attrs, "non_index_axes", None) or []
         self.data_columns = getattr(self.attrs, "data_columns", None) or []
         self.info = getattr(self.attrs, "info", None) or dict()
@@ -3467,16 +4444,34 @@
         self.values_axes = [a for a in self.indexables if not a.is_an_indexable]
 
     def validate_version(self, where=None):
-        """ are we trying to operate on an old version? """
+        """are we trying to operate on an old version?
+
+        Parameters
+        ----------
+        where :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         if where is not None:
             if self.version[0] <= 0 and self.version[1] <= 10 and self.version[2] < 1:
                 ws = incompatibility_doc % ".".join([str(x) for x in self.version])
                 warnings.warn(ws, IncompatibilityWarning)
 
     def validate_min_itemsize(self, min_itemsize):
-        """
-        validate the min_itemsize doesn't contain items that are not in the
+        """validate the min_itemsize doesn't contain items that are not in the
         axes this needs data_columns to be defined
+
+        Parameters
+        ----------
+        min_itemsize :
+            
+
+        Returns
+        -------
+
         """
         if min_itemsize is None:
             return
@@ -3497,7 +4492,7 @@
 
     @cache_readonly
     def indexables(self):
-        """ create/cache the indexables if they don't exist """
+        """create/cache the indexables if they don't exist"""
         _indexables = []
 
         desc = self.description
@@ -3531,6 +4526,19 @@
         base_pos = len(_indexables)
 
         def f(i, c):
+            """
+
+            Parameters
+            ----------
+            i :
+                
+            c :
+                
+
+            Returns
+            -------
+
+            """
             assert isinstance(c, str)
             klass = DataCol
             if c in dc:
@@ -3570,27 +4578,30 @@
         return _indexables
 
     def create_index(self, columns=None, optlevel=None, kind: Optional[str] = None):
-        """
-        Create a pytables index on the specified columns.
+        """Create a pytables index on the specified columns.
 
         Parameters
         ----------
         columns : None, bool, or listlike[str]
             Indicate which columns to create an index on.
-
             * False : Do not create any indexes.
             * True : Create indexes on all columns.
             * None : Create indexes on all columns.
-            * listlike : Create indexes on the given columns.
-
+            * listlike : Create indexes on the given columns. (Default value = None)
         optlevel : int or None, default None
             Optimization level, if None, pytables defaults to 6.
         kind : str or None, default None
             Kind of index, if None, pytables defaults to "medium".
+        kind: Optional[str] :
+             (Default value = None)
+
+        Returns
+        -------
 
         Raises
         ------
         TypeError if trying to create an index on a complex-type column.
+            
 
         Notes
         -----
@@ -3656,18 +4667,25 @@
     def _read_axes(
         self, where, start: Optional[int] = None, stop: Optional[int] = None
     ) -> List[Tuple[ArrayLike, ArrayLike]]:
-        """
-        Create the axes sniffed from the table.
+        """Create the axes sniffed from the table.
 
         Parameters
         ----------
         where : ???
+            
         start : int or None, default None
+            
         stop : int or None, default None
-
-        Returns
-        -------
-        List[Tuple[index_values, column_values]]
+            
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        
         """
         # create the selection
         selection = Selection(self, where=where, start=start, stop=stop)
@@ -3689,13 +4707,39 @@
 
     @classmethod
     def get_object(cls, obj, transposed: bool):
-        """ return the data for this obj """
+        """
+
+        Parameters
+        ----------
+        obj :
+            
+        transposed: bool :
+            
+
+        Returns
+        -------
+        type
+            
+
+        """
         return obj
 
     def validate_data_columns(self, data_columns, min_itemsize, non_index_axes):
-        """
-        take the input data_columns and min_itemize and create a data
+        """take the input data_columns and min_itemize and create a data
         columns spec
+
+        Parameters
+        ----------
+        data_columns :
+            
+        min_itemsize :
+            
+        non_index_axes :
+            
+
+        Returns
+        -------
+
         """
         if not len(non_index_axes):
             return []
@@ -3740,28 +4784,27 @@
         data_columns=None,
         min_itemsize=None,
     ):
-        """
-        Create and return the axes.
-
-        Parameters
-        ----------
-        axes: list or None
-            The names or numbers of the axes to create.
-        obj : DataFrame
-            The object to create axes on.
-        validate: bool, default True
-            Whether to validate the obj against an existing object already written.
+        """Create and return the axes.
+
+        Parameters
+        ----------
+        axes :
+            
+        obj: DataFrame :
+            
+        validate: bool :
+             (Default value = True)
         nan_rep :
-            A value to use for string column nan_rep.
-        data_columns : List[str], True, or None, default None
-            Specify the columns that we want to create to allow indexing on.
-
-            * True : Use all available columns.
-            * None : Use no columns.
-            * List[str] : Use the specified columns.
-
-        min_itemsize: Dict[str, int] or None, default None
-            The min itemsize for a column in bytes.
+             (Default value = None)
+        data_columns :
+             (Default value = None)
+        min_itemsize :
+             (Default value = None)
+
+        Returns
+        -------
+
+        
         """
         if not isinstance(obj, DataFrame):
             group = self.group._v_name
@@ -3850,6 +4893,19 @@
             obj = _reindex_axis(obj, a[0], a[1])
 
         def get_blk_items(mgr, blocks):
+            """
+
+            Parameters
+            ----------
+            mgr :
+                
+            blocks :
+                
+
+            Returns
+            -------
+
+            """
             return [mgr.items.take(blk.mgr_locs) for blk in blocks]
 
         transposed = new_index.axis == 1
@@ -3969,9 +5025,41 @@
     def _get_blocks_and_items(
         block_obj, table_exists, new_non_index_axes, values_axes, data_columns
     ):
+        """
+
+        Parameters
+        ----------
+        block_obj :
+            
+        table_exists :
+            
+        new_non_index_axes :
+            
+        values_axes :
+            
+        data_columns :
+            
+
+        Returns
+        -------
+
+        """
         # Helper to clarify non-state-altering parts of _create_axes
 
         def get_blk_items(mgr, blocks):
+            """
+
+            Parameters
+            ----------
+            mgr :
+                
+            blocks :
+                
+
+            Returns
+            -------
+
+            """
             return [mgr.items.take(blk.mgr_locs) for blk in blocks]
 
         blocks = block_obj._mgr.blocks
@@ -4015,7 +5103,21 @@
         return blocks, blk_items
 
     def process_axes(self, obj, selection: "Selection", columns=None):
-        """ process axes filters """
+        """process axes filters
+
+        Parameters
+        ----------
+        obj :
+            
+        selection: "Selection" :
+            
+        columns :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         # make a copy to avoid side effects
         if columns is not None:
             columns = list(columns)
@@ -4036,6 +5138,19 @@
             for field, op, filt in selection.filter.format():
 
                 def process_filter(field, filt):
+                    """
+
+                    Parameters
+                    ----------
+                    field :
+                        
+                    filt :
+                        
+
+                    Returns
+                    -------
+
+                    """
 
                     for axis_name in obj._AXIS_ORDERS:
                         axis_number = obj._get_axis_number(axis_name)
@@ -4079,7 +5194,23 @@
         fletcher32: bool,
         expectedrows: Optional[int],
     ) -> Dict[str, Any]:
-        """ create the description of the table from the axes & values """
+        """create the description of the table from the axes & values
+
+        Parameters
+        ----------
+        complib :
+            
+        complevel: Optional[int] :
+            
+        fletcher32: bool :
+            
+        expectedrows: Optional[int] :
+            
+
+        Returns
+        -------
+
+        """
         # provided expected rows if its passed
         if expectedrows is None:
             expectedrows = max(self.nrows_expected, 10000)
@@ -4106,9 +5237,21 @@
     def read_coordinates(
         self, where=None, start: Optional[int] = None, stop: Optional[int] = None,
     ):
-        """
-        select coordinates (row numbers) from a table; return the
+        """select coordinates (row numbers) from a table; return the
         coordinates object
+
+        Parameters
+        ----------
+        where :
+             (Default value = None)
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
         """
         # validate the version
         self.validate_version(where)
@@ -4137,8 +5280,23 @@
         stop: Optional[int] = None,
     ):
         """
-        return a single column from the table, generally only indexables
-        are interesting
+
+        Parameters
+        ----------
+        column: str :
+            
+        where :
+             (Default value = None)
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+        type
+            are interesting
+
         """
         # validate the version
         self.validate_version()
@@ -4175,10 +5333,16 @@
 
 
 class WORMTable(Table):
-    """
-    a write-once read-many table: this format DOES NOT ALLOW appending to a
+    """a write-once read-many table: this format DOES NOT ALLOW appending to a
     table. writing is a one-time operation the data are stored in a format
     that allows for searching the data on disk
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
 
     table_type = "worm"
@@ -4190,22 +5354,44 @@
         start: Optional[int] = None,
         stop: Optional[int] = None,
     ):
-        """
-        read the indices and the indexing array, calculate offset rows and return
+        """read the indices and the indexing array, calculate offset rows and return
+
+        Parameters
+        ----------
+        where :
+             (Default value = None)
+        columns :
+             (Default value = None)
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
         """
         raise NotImplementedError("WORMTable needs to implement read")
 
     def write(self, **kwargs):
-        """
-        write in a format that we can search later on (but cannot append
+        """write in a format that we can search later on (but cannot append
         to): write out the indices and the values using _write_array
         (e.g. a CArray) create an indexing table so that we can search
+
+        Parameters
+        ----------
+        **kwargs :
+            
+
+        Returns
+        -------
+
         """
         raise NotImplementedError("WORMTable needs to implement write")
 
 
 class AppendableTable(Table):
-    """ support the new appendable table formats """
+    """support the new appendable table formats"""
 
     table_type = "appendable"
 
@@ -4225,6 +5411,41 @@
         data_columns=None,
         track_times=True,
     ):
+        """
+
+        Parameters
+        ----------
+        obj :
+            
+        axes :
+             (Default value = None)
+        append :
+             (Default value = False)
+        complib :
+             (Default value = None)
+        complevel :
+             (Default value = None)
+        fletcher32 :
+             (Default value = None)
+        min_itemsize :
+             (Default value = None)
+        chunksize :
+             (Default value = None)
+        expectedrows :
+             (Default value = None)
+        dropna :
+             (Default value = False)
+        nan_rep :
+             (Default value = None)
+        data_columns :
+             (Default value = None)
+        track_times :
+             (Default value = True)
+
+        Returns
+        -------
+
+        """
         if not append and self.is_exists:
             self._handle.remove_node(self.group, "table")
 
@@ -4270,8 +5491,18 @@
         table.write_data(chunksize, dropna=dropna)
 
     def write_data(self, chunksize: Optional[int], dropna: bool = False):
-        """
-        we form the data into a 2-d including indexes,values,mask write chunk-by-chunk
+        """we form the data into a 2-d including indexes,values,mask write chunk-by-chunk
+
+        Parameters
+        ----------
+        chunksize: Optional[int] :
+            
+        dropna: bool :
+             (Default value = False)
+
+        Returns
+        -------
+
         """
         names = self.dtype.names
         nrows = self.nrows_expected
@@ -4338,12 +5569,22 @@
         values: List[np.ndarray],
     ):
         """
-        Parameters
-        ----------
-        rows : an empty memory space where we are putting the chunk
-        indexes : an array of the indexes
-        mask : an array of the masks
-        values : an array of the values
+
+        Parameters
+        ----------
+        rows: np.ndarray :
+            
+        indexes: List[np.ndarray] :
+            
+        mask: Optional[np.ndarray] :
+            
+        values: List[np.ndarray] :
+            
+
+        Returns
+        -------
+
+        
         """
         # 0 len
         for v in values:
@@ -4377,6 +5618,21 @@
     def delete(
         self, where=None, start: Optional[int] = None, stop: Optional[int] = None,
     ):
+        """
+
+        Parameters
+        ----------
+        where :
+             (Default value = None)
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
 
         # delete all rows (and return the nrows)
         if where is None or not len(where):
@@ -4438,7 +5694,7 @@
 
 
 class AppendableFrameTable(AppendableTable):
-    """ support the new appendable table formats """
+    """support the new appendable table formats"""
 
     pandas_kind = "frame_table"
     table_type = "appendable_frame"
@@ -4447,11 +5703,24 @@
 
     @property
     def is_transposed(self) -> bool:
+        """ """
         return self.index_axes[0].axis == 1
 
     @classmethod
     def get_object(cls, obj, transposed: bool):
-        """ these are written transposed """
+        """these are written transposed
+
+        Parameters
+        ----------
+        obj :
+            
+        transposed: bool :
+            
+
+        Returns
+        -------
+
+        """
         if transposed:
             obj = obj.T
         return obj
@@ -4463,6 +5732,23 @@
         start: Optional[int] = None,
         stop: Optional[int] = None,
     ):
+        """
+
+        Parameters
+        ----------
+        where :
+             (Default value = None)
+        columns :
+             (Default value = None)
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
 
         # validate the version
         self.validate_version(where)
@@ -4538,7 +5824,7 @@
 
 
 class AppendableSeriesTable(AppendableFrameTable):
-    """ support the new appendable table formats """
+    """support the new appendable table formats"""
 
     pandas_kind = "series_table"
     table_type = "appendable_series"
@@ -4547,14 +5833,42 @@
 
     @property
     def is_transposed(self) -> bool:
+        """ """
         return False
 
     @classmethod
     def get_object(cls, obj, transposed: bool):
+        """
+
+        Parameters
+        ----------
+        obj :
+            
+        transposed: bool :
+            
+
+        Returns
+        -------
+
+        """
         return obj
 
     def write(self, obj, data_columns=None, **kwargs):
-        """ we are going to write this as a frame table """
+        """we are going to write this as a frame table
+
+        Parameters
+        ----------
+        obj :
+            
+        data_columns :
+             (Default value = None)
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         if not isinstance(obj, DataFrame):
             name = obj.name or "values"
             obj = obj.to_frame(name)
@@ -4567,6 +5881,23 @@
         start: Optional[int] = None,
         stop: Optional[int] = None,
     ) -> Series:
+        """
+
+        Parameters
+        ----------
+        where :
+             (Default value = None)
+        columns :
+             (Default value = None)
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
 
         is_multi_index = self.is_multi_index
         if columns is not None and is_multi_index:
@@ -4587,13 +5918,25 @@
 
 
 class AppendableMultiSeriesTable(AppendableSeriesTable):
-    """ support the new appendable table formats """
+    """support the new appendable table formats"""
 
     pandas_kind = "series_table"
     table_type = "appendable_multiseries"
 
     def write(self, obj, **kwargs):
-        """ we are going to write this as a frame table """
+        """we are going to write this as a frame table
+
+        Parameters
+        ----------
+        obj :
+            
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         name = obj.name or "values"
         obj, self.levels = self.validate_multiindex(obj)
         cols = list(self.levels)
@@ -4603,7 +5946,7 @@
 
 
 class GenericTable(AppendableFrameTable):
-    """ a table that read/writes the generic pytables table format """
+    """a table that read/writes the generic pytables table format"""
 
     pandas_kind = "frame_table"
     table_type = "generic_table"
@@ -4612,14 +5955,16 @@
 
     @property
     def pandas_type(self) -> str:
+        """ """
         return self.pandas_kind
 
     @property
     def storable(self):
+        """ """
         return getattr(self.group, "table", None) or self.group
 
     def get_attrs(self):
-        """ retrieve our attributes """
+        """retrieve our attributes"""
         self.non_index_axes = []
         self.nan_rep = None
         self.levels = []
@@ -4630,7 +5975,7 @@
 
     @cache_readonly
     def indexables(self):
-        """ create the indexables from the table description """
+        """create the indexables from the table description"""
         d = self.description
 
         # TODO: can we get a typ for this?  AFAICT it is the only place
@@ -4664,11 +6009,22 @@
         return _indexables
 
     def write(self, **kwargs):
+        """
+
+        Parameters
+        ----------
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         raise NotImplementedError("cannot write on an generic table")
 
 
 class AppendableMultiFrameTable(AppendableFrameTable):
-    """ a frame with a multi-index """
+    """a frame with a multi-index"""
 
     table_type = "appendable_multiframe"
     obj_type = DataFrame
@@ -4677,9 +6033,25 @@
 
     @property
     def table_type_short(self) -> str:
+        """ """
         return "appendable_multi"
 
     def write(self, obj, data_columns=None, **kwargs):
+        """
+
+        Parameters
+        ----------
+        obj :
+            
+        data_columns :
+             (Default value = None)
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         if data_columns is None:
             data_columns = []
         elif data_columns is True:
@@ -4697,6 +6069,23 @@
         start: Optional[int] = None,
         stop: Optional[int] = None,
     ):
+        """
+
+        Parameters
+        ----------
+        where :
+             (Default value = None)
+        columns :
+             (Default value = None)
+        start: Optional[int] :
+             (Default value = None)
+        stop: Optional[int] :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
 
         df = super().read(where=where, columns=columns, start=start, stop=stop)
         df = df.set_index(self.levels)
@@ -4710,6 +6099,23 @@
 
 
 def _reindex_axis(obj: DataFrame, axis: int, labels: Index, other=None) -> DataFrame:
+    """
+
+    Parameters
+    ----------
+    obj: DataFrame :
+        
+    axis: int :
+        
+    labels: Index :
+        
+    other :
+         (Default value = None)
+
+    Returns
+    -------
+
+    """
     ax = obj._get_axis(axis)
     labels = ensure_index(labels)
 
@@ -4734,7 +6140,17 @@
 
 
 def _get_tz(tz: tzinfo) -> Union[str, tzinfo]:
-    """ for a tz-aware type, return an encoded zone """
+    """for a tz-aware type, return an encoded zone
+
+    Parameters
+    ----------
+    tz: tzinfo :
+        
+
+    Returns
+    -------
+
+    """
     zone = timezones.get_timezone(tz)
     return zone
 
@@ -4744,15 +6160,26 @@
     tz: Optional[Union[str, tzinfo]],
     coerce: bool = False,
 ) -> Union[np.ndarray, DatetimeIndex]:
-    """
-    coerce the values to a DatetimeIndex if tz is set
+    """coerce the values to a DatetimeIndex if tz is set
     preserve the input shape if possible
 
     Parameters
     ----------
-    values : ndarray or Index
-    tz : str or tzinfo
-    coerce : if we do not have a passed timezone, coerce to M8[ns] ndarray
+    values: Union[np.ndarray :
+        
+    Index] :
+        
+    tz: Optional[Union[str :
+        
+    tzinfo]] :
+        
+    coerce: bool :
+         (Default value = False)
+
+    Returns
+    -------
+
+    
     """
     if isinstance(values, DatetimeIndex):
         # If values is tzaware, the tz gets dropped in the values.ravel()
@@ -4773,6 +6200,23 @@
 
 
 def _convert_index(name: str, index: Index, encoding: str, errors: str) -> IndexCol:
+    """
+
+    Parameters
+    ----------
+    name: str :
+        
+    index: Index :
+        
+    encoding: str :
+        
+    errors: str :
+        
+
+    Returns
+    -------
+
+    """
     assert isinstance(name, str)
 
     index_name = index.name
@@ -4834,6 +6278,23 @@
 def _unconvert_index(
     data, kind: str, encoding: str, errors: str
 ) -> Union[np.ndarray, Index]:
+    """
+
+    Parameters
+    ----------
+    data :
+        
+    kind: str :
+        
+    encoding: str :
+        
+    errors: str :
+        
+
+    Returns
+    -------
+
+    """
     index: Union[Index, np.ndarray]
 
     if kind == "datetime64":
@@ -4861,6 +6322,29 @@
 def _maybe_convert_for_string_atom(
     name: str, block, existing_col, min_itemsize, nan_rep, encoding, errors
 ):
+    """
+
+    Parameters
+    ----------
+    name: str :
+        
+    block :
+        
+    existing_col :
+        
+    min_itemsize :
+        
+    nan_rep :
+        
+    encoding :
+        
+    errors :
+        
+
+    Returns
+    -------
+
+    """
 
     if not block.is_object:
         return block.values
@@ -4925,19 +6409,27 @@
 
 
 def _convert_string_array(data: np.ndarray, encoding: str, errors: str) -> np.ndarray:
-    """
-    Take a string-like that is object dtype and coerce to a fixed size string type.
+    """Take a string-like that is object dtype and coerce to a fixed size string type.
 
     Parameters
     ----------
     data : np.ndarray[object]
+        
     encoding : str
+        
     errors : str
         Handler for encoding errors.
+    data: np.ndarray :
+        
+    encoding: str :
+        
+    errors: str :
+        
 
     Returns
     -------
-    np.ndarray[fixed-length-string]
+
+    
     """
     # encode if needed
     if len(data):
@@ -4958,21 +6450,29 @@
 def _unconvert_string_array(
     data: np.ndarray, nan_rep, encoding: str, errors: str
 ) -> np.ndarray:
-    """
-    Inverse of _convert_string_array.
+    """Inverse of _convert_string_array.
 
     Parameters
     ----------
     data : np.ndarray[fixed-length-string]
+        
     nan_rep : the storage repr of NaN
+        
     encoding : str
+        
     errors : str
         Handler for encoding errors.
+    data: np.ndarray :
+        
+    encoding: str :
+        
+    errors: str :
+        
 
     Returns
     -------
-    np.ndarray[object]
-        Decoded data.
+
+    
     """
     shape = data.shape
     data = np.asarray(data.ravel(), dtype=object)
@@ -4995,6 +6495,23 @@
 
 
 def _maybe_convert(values: np.ndarray, val_kind: str, encoding: str, errors: str):
+    """
+
+    Parameters
+    ----------
+    values: np.ndarray :
+        
+    val_kind: str :
+        
+    encoding: str :
+        
+    errors: str :
+        
+
+    Returns
+    -------
+
+    """
     assert isinstance(val_kind, str), type(val_kind)
     if _need_convert(val_kind):
         conv = _get_converter(val_kind, encoding, errors)
@@ -5003,6 +6520,21 @@
 
 
 def _get_converter(kind: str, encoding: str, errors: str):
+    """
+
+    Parameters
+    ----------
+    kind: str :
+        
+    encoding: str :
+        
+    errors: str :
+        
+
+    Returns
+    -------
+
+    """
     if kind == "datetime64":
         return lambda x: np.asarray(x, dtype="M8[ns]")
     elif kind == "string":
@@ -5014,24 +6546,39 @@
 
 
 def _need_convert(kind: str) -> bool:
+    """
+
+    Parameters
+    ----------
+    kind: str :
+        
+
+    Returns
+    -------
+
+    """
     if kind in ("datetime64", "string"):
         return True
     return False
 
 
 def _maybe_adjust_name(name: str, version) -> str:
-    """
-    Prior to 0.10.1, we named values blocks like: values_block_0 an the
+    """Prior to 0.10.1, we named values blocks like: values_block_0 an the
     name values_0, adjust the given name if necessary.
 
     Parameters
     ----------
     name : str
+        
     version : Tuple[int, int, int]
+        
+    name: str :
+        
 
     Returns
     -------
-    str
+
+    
     """
     try:
         if version[0] == 0 and version[1] <= 10 and version[2] == 0:
@@ -5045,8 +6592,16 @@
 
 
 def _dtype_to_kind(dtype_str: str) -> str:
-    """
-    Find the "kind" string describing the given dtype name.
+    """Find the "kind" string describing the given dtype name.
+
+    Parameters
+    ----------
+    dtype_str: str :
+        
+
+    Returns
+    -------
+
     """
     dtype_str = _ensure_decoded(dtype_str)
 
@@ -5078,8 +6633,16 @@
 
 
 def _get_data_and_dtype_name(data: ArrayLike):
-    """
-    Convert the passed data into a storable form and a dtype string.
+    """Convert the passed data into a storable form and a dtype string.
+
+    Parameters
+    ----------
+    data: ArrayLike :
+        
+
+    Returns
+    -------
+
     """
     if isinstance(data, Categorical):
         data = data.codes
@@ -5100,16 +6663,7 @@
 
 
 class Selection:
-    """
-    Carries out a selection operation on a tables.Table object.
-
-    Parameters
-    ----------
-    table : a Table object
-    where : list of Terms (or convertible to)
-    start, stop: indices to start and/or stop selection
-
-    """
+    """Carries out a selection operation on a tables.Table object."""
 
     def __init__(
         self,
@@ -5162,7 +6716,17 @@
                 self.condition, self.filter = self.terms.evaluate()
 
     def generate(self, where):
-        """ where can be a : dict,list,tuple,string """
+        """where can be a : dict,list,tuple,string
+
+        Parameters
+        ----------
+        where :
+            
+
+        Returns
+        -------
+
+        """
         if where is None:
             return None
 
@@ -5184,9 +6748,7 @@
             ) from err
 
     def select(self):
-        """
-        generate the selection
-        """
+        """generate the selection"""
         if self.condition is not None:
             return self.table.table.read_where(
                 self.condition.format(), start=self.start, stop=self.stop
@@ -5196,9 +6758,7 @@
         return self.table.table.read(start=self.start, stop=self.stop)
 
     def select_coords(self):
-        """
-        generate the selection
-        """
+        """generate the selection"""
         start, stop = self.start, self.stop
         nrows = self.table.nrows
         if start is None:
