# Patch generated by Pyment v0.3.3

--- a/..//venv/lib/python3.8/site-packages/bleach/sanitizer.py
+++ b/..//venv/lib/python3.8/site-packages/bleach/sanitizer.py
@@ -57,30 +57,35 @@
 
 class Cleaner(object):
     """Cleaner for cleaning HTML fragments of malicious content
-
+    
     This cleaner is a security-focused function whose sole purpose is to remove
     malicious content from a string such that it can be displayed as content in
     a web page.
-
+    
     To use::
-
+    
         from bleach.sanitizer import Cleaner
-
+    
         cleaner = Cleaner()
-
+    
         for text in all_the_yucky_things:
             sanitized = cleaner.clean(text)
-
+    
     .. Note::
-
+    
        This cleaner is not designed to use to transform content to be used in
        non-web-page contexts.
-
+    
     .. Warning::
-
+    
        This cleaner is not thread-safe--the html parser has internal state.
        Create a separate cleaner per thread!
 
+    Parameters
+    ----------
+
+    Returns
+    -------
 
     """
 
@@ -152,12 +157,23 @@
 
     def clean(self, text):
         """Cleans text and returns sanitized result as unicode
-
+        
         :arg str text: text to be cleaned
 
-        :returns: sanitized text as unicode
-
-        :raises TypeError: if ``text`` is not a text type
+        Parameters
+        ----------
+        text :
+            
+
+        Returns
+        -------
+        type
+            sanitized text as unicode
+
+        Raises
+        ------
+        TypeError
+            if ``text`` is not a text type
 
         """
         if not isinstance(text, six.string_types):
@@ -196,11 +212,19 @@
 
 def attribute_filter_factory(attributes):
     """Generates attribute filter function for the given attributes value
-
+    
     The attributes value can take one of several shapes. This returns a filter
     function appropriate to the attributes value. One nice thing about this is
     that there's less if/then shenanigans in the ``allow_token`` method.
 
+    Parameters
+    ----------
+    attributes :
+        
+
+    Returns
+    -------
+
     """
     if callable(attributes):
         return attributes
@@ -208,6 +232,21 @@
     if isinstance(attributes, dict):
 
         def _attr_filter(tag, attr, value):
+            """
+
+            Parameters
+            ----------
+            tag :
+                
+            attr :
+                
+            value :
+                
+
+            Returns
+            -------
+
+            """
             if tag in attributes:
                 attr_val = attributes[tag]
                 if callable(attr_val):
@@ -230,6 +269,21 @@
     if isinstance(attributes, list):
 
         def _attr_filter(tag, attr, value):
+            """
+
+            Parameters
+            ----------
+            tag :
+                
+            attr :
+                
+            value :
+                
+
+            Returns
+            -------
+
+            """
             return attr in attributes
 
         return _attr_filter
@@ -239,8 +293,14 @@
 
 class BleachSanitizerFilter(html5lib_shim.SanitizerFilter):
     """html5lib Filter that sanitizes text
-
+    
     This filter can be used anywhere html5lib filters can be used.
+
+    Parameters
+    ----------
+
+    Returns
+    -------
 
     """
 
@@ -288,6 +348,17 @@
         return super(BleachSanitizerFilter, self).__init__(source, **kwargs)
 
     def sanitize_stream(self, token_iterator):
+        """
+
+        Parameters
+        ----------
+        token_iterator :
+            
+
+        Returns
+        -------
+
+        """
         for token in token_iterator:
             ret = self.sanitize_token(token)
 
@@ -301,7 +372,17 @@
                 yield ret
 
     def merge_characters(self, token_iterator):
-        """Merge consecutive Characters tokens in a stream"""
+        """Merge consecutive Characters tokens in a stream
+
+        Parameters
+        ----------
+        token_iterator :
+            
+
+        Returns
+        -------
+
+        """
         characters_buffer = []
 
         for token in token_iterator:
@@ -340,18 +421,26 @@
 
     def sanitize_token(self, token):
         """Sanitize a token either by HTML-encoding or dropping.
-
+        
         Unlike sanitizer.Filter, allowed_attributes can be a dict of {'tag':
         ['attribute', 'pairs'], 'tag': callable}.
-
+        
         Here callable is a function with two arguments of attribute name and
         value. It should return true of false.
-
+        
         Also gives the option to strip tags instead of encoding.
-
+        
         :arg dict token: token to sanitize
 
-        :returns: token or list of tokens
+        Parameters
+        ----------
+        token :
+            
+
+        Returns
+        -------
+        type
+            token or list of tokens
 
         """
         token_type = token["type"]
@@ -383,17 +472,25 @@
 
     def sanitize_characters(self, token):
         """Handles Characters tokens
-
+        
         Our overridden tokenizer doesn't do anything with entities. However,
         that means that the serializer will convert all ``&`` in Characters
         tokens to ``&amp;``.
-
+        
         Since we don't want that, we extract entities here and convert them to
         Entity tokens so the serializer will let them be.
-
+        
         :arg token: the Characters token to work on
 
-        :returns: a list of tokens
+        Parameters
+        ----------
+        token :
+            
+
+        Returns
+        -------
+        type
+            a list of tokens
 
         """
         data = token.get("data", "")
@@ -444,11 +541,21 @@
 
     def sanitize_uri_value(self, value, allowed_protocols):
         """Checks a uri value to see if it's allowed
-
+        
         :arg value: the uri value to sanitize
         :arg allowed_protocols: list of allowed protocols
 
-        :returns: allowed value or None
+        Parameters
+        ----------
+        value :
+            
+        allowed_protocols :
+            
+
+        Returns
+        -------
+        type
+            allowed value or None
 
         """
         # NOTE(willkg): This transforms the value into one that's easier to
@@ -498,7 +605,17 @@
         return None
 
     def allow_token(self, token):
-        """Handles the case where we're allowing the tag"""
+        """Handles the case where we're allowing the tag
+
+        Parameters
+        ----------
+        token :
+            
+
+        Returns
+        -------
+
+        """
         if "data" in token:
             # Loop through all the attributes and drop the ones that are not
             # allowed, are unsafe or break other rules. Additionally, fix
@@ -558,6 +675,17 @@
         return token
 
     def disallowed_token(self, token):
+        """
+
+        Parameters
+        ----------
+        token :
+            
+
+        Returns
+        -------
+
+        """
         token_type = token["type"]
         if token_type == "EndTag":
             token["data"] = "</%s>" % token["name"]
@@ -602,7 +730,17 @@
         return token
 
     def sanitize_css(self, style):
-        """Sanitizes css in style tags"""
+        """Sanitizes css in style tags
+
+        Parameters
+        ----------
+        style :
+            
+
+        Returns
+        -------
+
+        """
         # Convert entities in the style so that it can be parsed as CSS
         style = html5lib_shim.convert_entities(style)
 
