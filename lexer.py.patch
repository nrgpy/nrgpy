# Patch generated by Pyment v0.3.3

--- a/..//venv/lib/python3.8/site-packages/pygments/lexer.py
+++ b/..//venv/lib/python3.8/site-packages/pygments/lexer.py
@@ -35,9 +35,15 @@
 
 
 class LexerMeta(type):
-    """
-    This metaclass automagically converts ``analyse_text`` methods into
+    """This metaclass automagically converts ``analyse_text`` methods into
     static methods which always return float values.
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
 
     def __new__(mcs, name, bases, d):
@@ -47,9 +53,8 @@
 
 
 class Lexer(metaclass=LexerMeta):
-    """
-    Lexer for a specific language.
-
+    """Lexer for a specific language.
+    
     Basic options recognized:
     ``stripnl``
         Strip leading and trailing newlines from the input (default: True).
@@ -59,9 +64,9 @@
     ``ensurenl``
         Make sure that the input ends with a newline (default: True).  This
         is required for some lexers that consume input linewise.
-
+    
         .. versionadded:: 1.3
-
+    
     ``tabsize``
         If given and greater than 0, expand tabs in the input (default: 0).
     ``encoding``
@@ -72,6 +77,13 @@
         library, if it is installed.
     ``inencoding``
         Overrides the ``encoding`` if given.
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
 
     #: Name of the lexer
@@ -112,35 +124,64 @@
             return '<pygments.lexers.%s>' % self.__class__.__name__
 
     def add_filter(self, filter_, **options):
-        """
-        Add a new stream filter to this lexer.
+        """Add a new stream filter to this lexer.
+
+        Parameters
+        ----------
+        filter_ :
+            
+        **options :
+            
+
+        Returns
+        -------
+
         """
         if not isinstance(filter_, Filter):
             filter_ = get_filter_by_name(filter_, **options)
         self.filters.append(filter_)
 
     def analyse_text(text):
-        """
-        Has to return a float between ``0`` and ``1`` that indicates
+        """Has to return a float between ``0`` and ``1`` that indicates
         if a lexer wants to highlight this text. Used by ``guess_lexer``.
         If this method returns ``0`` it won't highlight it in any case, if
         it returns ``1`` highlighting with this lexer is guaranteed.
-
+        
         The `LexerMeta` metaclass automatically wraps this function so
         that it works like a static method (no ``self`` or ``cls``
         parameter) and the return value is automatically converted to
         `float`. If the return value is an object that is boolean `False`
         it's the same as if the return values was ``0.0``.
+
+        Parameters
+        ----------
+        text :
+            
+
+        Returns
+        -------
+
         """
 
     def get_tokens(self, text, unfiltered=False):
         """
-        Return an iterable of (tokentype, value) pairs generated from
-        `text`. If `unfiltered` is set to `True`, the filtering mechanism
-        is bypassed even if filters are defined.
-
-        Also preprocess the text, i.e. expand tabs and strip it if
-        wanted and applies registered filters.
+
+        Parameters
+        ----------
+        text :
+            
+        unfiltered :
+             (Default value = False)
+
+        Returns
+        -------
+        type
+            `text`. If `unfiltered` is set to `True`, the filtering mechanism
+            is bypassed even if filters are defined.
+            
+            Also preprocess the text, i.e. expand tabs and strip it if
+            wanted and applies registered filters.
+
         """
         if not isinstance(text, str):
             if self.encoding == 'guess':
@@ -185,6 +226,7 @@
             text += '\n'
 
         def streamer():
+            """ """
             for _, t, v in self.get_tokens_unprocessed(text):
                 yield t, v
         stream = streamer()
@@ -194,23 +236,38 @@
 
     def get_tokens_unprocessed(self, text):
         """
-        Return an iterable of (index, tokentype, value) pairs where "index"
-        is the starting position of the token within the input text.
-
-        In subclasses, implement this method as a generator to
-        maximize effectiveness.
+
+        Parameters
+        ----------
+        text :
+            
+
+        Returns
+        -------
+        type
+            is the starting position of the token within the input text.
+            
+            In subclasses, implement this method as a generator to
+            maximize effectiveness.
+
         """
         raise NotImplementedError
 
 
 class DelegatingLexer(Lexer):
-    """
-    This lexer takes two lexer as arguments. A root lexer and
+    """This lexer takes two lexer as arguments. A root lexer and
     a language lexer. First everything is scanned using the language
     lexer, afterwards all ``Other`` tokens are lexed using the root
     lexer.
-
+    
     The lexers from the ``template`` lexer package use this base lexer.
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
 
     def __init__(self, _root_lexer, _language_lexer, _needle=Other, **options):
@@ -220,6 +277,17 @@
         Lexer.__init__(self, **options)
 
     def get_tokens_unprocessed(self, text):
+        """
+
+        Parameters
+        ----------
+        text :
+            
+
+        Returns
+        -------
+
+        """
         buffered = ''
         insertions = []
         lng_buffer = []
@@ -243,16 +311,12 @@
 
 
 class include(str):  # pylint: disable=invalid-name
-    """
-    Indicates that a state should include rules from another state.
-    """
+    """Indicates that a state should include rules from another state."""
     pass
 
 
 class _inherit:
-    """
-    Indicates the a state should inherit from its superclass.
-    """
+    """Indicates the a state should inherit from its superclass."""
     def __repr__(self):
         return 'inherit'
 
@@ -260,9 +324,7 @@
 
 
 class combined(tuple):  # pylint: disable=invalid-name
-    """
-    Indicates a state combined from multiple states.
-    """
+    """Indicates a state combined from multiple states."""
 
     def __new__(cls, *args):
         return tuple.__new__(cls, args)
@@ -273,37 +335,93 @@
 
 
 class _PseudoMatch:
-    """
-    A pseudo match object constructed from a string.
-    """
+    """A pseudo match object constructed from a string."""
 
     def __init__(self, start, text):
         self._text = text
         self._start = start
 
     def start(self, arg=None):
+        """
+
+        Parameters
+        ----------
+        arg :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         return self._start
 
     def end(self, arg=None):
+        """
+
+        Parameters
+        ----------
+        arg :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         return self._start + len(self._text)
 
     def group(self, arg=None):
+        """
+
+        Parameters
+        ----------
+        arg :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         if arg:
             raise IndexError('No such group')
         return self._text
 
     def groups(self):
+        """ """
         return (self._text,)
 
     def groupdict(self):
+        """ """
         return {}
 
 
 def bygroups(*args):
-    """
-    Callback that yields multiple actions for each group in the match.
+    """Callback that yields multiple actions for each group in the match.
+
+    Parameters
+    ----------
+    *args :
+        
+
+    Returns
+    -------
+
     """
     def callback(lexer, match, ctx=None):
+        """
+
+        Parameters
+        ----------
+        lexer :
+            
+        match :
+            
+        ctx :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         for i, action in enumerate(args):
             if action is None:
                 continue
@@ -326,26 +444,42 @@
 
 
 class _This:
+    """Special singleton used for indicating the caller class.
+    Used by ``using``.
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
-    Special singleton used for indicating the caller class.
-    Used by ``using``.
-    """
 
 this = _This()
 
 
 def using(_other, **kwargs):
-    """
-    Callback that processes the match with a different lexer.
-
+    """Callback that processes the match with a different lexer.
+    
     The keyword arguments are forwarded to the lexer, except `state` which
     is handled separately.
-
+    
     `state` specifies the state that the new lexer will start in, and can
     be an enumerable such as ('root', 'inline', 'string') or a simple
     string which is assumed to be on top of the root state.
-
+    
     Note: For that to work, `_other` must not be an `ExtendedRegexLexer`.
+
+    Parameters
+    ----------
+    _other :
+        
+    **kwargs :
+        
+
+    Returns
+    -------
+
     """
     gt_kwargs = {}
     if 'state' in kwargs:
@@ -357,6 +491,21 @@
 
     if _other is this:
         def callback(lexer, match, ctx=None):
+            """
+
+            Parameters
+            ----------
+            lexer :
+                
+            match :
+                
+            ctx :
+                 (Default value = None)
+
+            Returns
+            -------
+
+            """
             # if keyword arguments are given the callback
             # function has to create a new lexer instance
             if kwargs:
@@ -372,6 +521,21 @@
                 ctx.pos = match.end()
     else:
         def callback(lexer, match, ctx=None):
+            """
+
+            Parameters
+            ----------
+            lexer :
+                
+            match :
+                
+            ctx :
+                 (Default value = None)
+
+            Returns
+            -------
+
+            """
             # XXX: cache that somehow
             kwargs.update(lexer.options)
             lx = _other(**kwargs)
@@ -385,23 +549,35 @@
 
 
 class default:
-    """
-    Indicates a state or state action (e.g. #pop) to apply.
+    """Indicates a state or state action (e.g. #pop) to apply.
     For example default('#pop') is equivalent to ('', Token, '#pop')
     Note that state tuples may be used as well.
-
+    
     .. versionadded:: 2.0
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     def __init__(self, state):
         self.state = state
 
 
 class words(Future):
-    """
-    Indicates a list of literal words that is transformed into an optimized
+    """Indicates a list of literal words that is transformed into an optimized
     regex that matches any of the words.
-
+    
     .. versionadded:: 2.0
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     def __init__(self, words, prefix='', suffix=''):
         self.words = words
@@ -409,29 +585,74 @@
         self.suffix = suffix
 
     def get(self):
+        """ """
         return regex_opt(self.words, prefix=self.prefix, suffix=self.suffix)
 
 
 class RegexLexerMeta(LexerMeta):
+    """Metaclass for RegexLexer, creates the self._tokens attribute from
+    self.tokens on the first instantiation.
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
-    Metaclass for RegexLexer, creates the self._tokens attribute from
-    self.tokens on the first instantiation.
-    """
 
     def _process_regex(cls, regex, rflags, state):
-        """Preprocess the regular expression component of a token definition."""
+        """Preprocess the regular expression component of a token definition.
+
+        Parameters
+        ----------
+        regex :
+            
+        rflags :
+            
+        state :
+            
+
+        Returns
+        -------
+
+        """
         if isinstance(regex, Future):
             regex = regex.get()
         return re.compile(regex, rflags).match
 
     def _process_token(cls, token):
-        """Preprocess the token component of a token definition."""
+        """Preprocess the token component of a token definition.
+
+        Parameters
+        ----------
+        token :
+            
+
+        Returns
+        -------
+
+        """
         assert type(token) is _TokenType or callable(token), \
             'token type must be simple type or callable, not %r' % (token,)
         return token
 
     def _process_new_state(cls, new_state, unprocessed, processed):
-        """Preprocess the state transition action of a token definition."""
+        """Preprocess the state transition action of a token definition.
+
+        Parameters
+        ----------
+        new_state :
+            
+        unprocessed :
+            
+        processed :
+            
+
+        Returns
+        -------
+
+        """
         if isinstance(new_state, str):
             # an existing state
             if new_state == '#pop':
@@ -466,7 +687,21 @@
             assert False, 'unknown new state def %r' % new_state
 
     def _process_state(cls, unprocessed, processed, state):
-        """Preprocess a single state definition."""
+        """Preprocess a single state definition.
+
+        Parameters
+        ----------
+        unprocessed :
+            
+        processed :
+            
+        state :
+            
+
+        Returns
+        -------
+
+        """
         assert type(state) is str, "wrong state name %r" % state
         assert state[0] != '#', "invalid state name %r" % state
         if state in processed:
@@ -510,7 +745,19 @@
         return tokens
 
     def process_tokendef(cls, name, tokendefs=None):
-        """Preprocess a dictionary of token definitions."""
+        """Preprocess a dictionary of token definitions.
+
+        Parameters
+        ----------
+        name :
+            
+        tokendefs :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         processed = cls._all_tokens[name] = {}
         tokendefs = tokendefs or cls.tokens[name]
         for state in list(tokendefs):
@@ -518,16 +765,22 @@
         return processed
 
     def get_tokendefs(cls):
-        """
-        Merge tokens from superclasses in MRO order, returning a single tokendef
+        """Merge tokens from superclasses in MRO order, returning a single tokendef
         dictionary.
-
+        
         Any state that is not defined by a subclass will be inherited
         automatically.  States that *are* defined by subclasses will, by
         default, override that state in the superclass.  If a subclass wishes to
         inherit definitions from a superclass, it can use the special value
         "inherit", which will cause the superclass' state definition to be
         included at that point in the state.
+
+        Parameters
+        ----------
+
+        Returns
+        -------
+
         """
         tokens = {}
         inheritable = {}
@@ -581,10 +834,16 @@
 
 
 class RegexLexer(Lexer, metaclass=RegexLexerMeta):
-    """
-    Base for simple stateful regular expression-based lexers.
+    """Base for simple stateful regular expression-based lexers.
     Simplifies the lexing process so that you need only
     provide a list of states and regular expressions.
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
 
     #: Flags for compiling the regular expressions.
@@ -611,10 +870,22 @@
     tokens = {}
 
     def get_tokens_unprocessed(self, text, stack=('root',)):
-        """
-        Split ``text`` into (tokentype, text) pairs.
-
+        """Split ``text`` into (tokentype, text) pairs.
+        
         ``stack`` is the inital stack (default: ``['root']``)
+
+        Parameters
+        ----------
+        text :
+            
+        stack :
+             (Default value = ('root')
+        ) :
+            
+
+        Returns
+        -------
+
         """
         pos = 0
         tokendefs = self._tokens
@@ -673,9 +944,7 @@
 
 
 class LexerContext:
-    """
-    A helper object that holds lexer position data.
-    """
+    """A helper object that holds lexer position data."""
 
     def __init__(self, text, pos, stack=None, end=None):
         self.text = text
@@ -689,14 +958,22 @@
 
 
 class ExtendedRegexLexer(RegexLexer):
-    """
-    A RegexLexer that uses a context object to store its state.
-    """
+    """A RegexLexer that uses a context object to store its state."""
 
     def get_tokens_unprocessed(self, text=None, context=None):
-        """
-        Split ``text`` into (tokentype, text) pairs.
+        """Split ``text`` into (tokentype, text) pairs.
         If ``context`` is given, use this lexer context instead.
+
+        Parameters
+        ----------
+        text :
+             (Default value = None)
+        context :
+             (Default value = None)
+
+        Returns
+        -------
+
         """
         tokendefs = self._tokens
         if not context:
@@ -761,18 +1038,28 @@
 
 
 def do_insertions(insertions, tokens):
-    """
-    Helper for lexers which must combine the results of several
+    """Helper for lexers which must combine the results of several
     sublexers.
-
+    
     ``insertions`` is a list of ``(index, itokens)`` pairs.
     Each ``itokens`` iterable should be inserted at position
     ``index`` into the token stream given by the ``tokens``
     argument.
-
+    
     The result is a combined token stream.
-
+    
     TODO: clean up the code here.
+
+    Parameters
+    ----------
+    insertions :
+        
+    tokens :
+        
+
+    Returns
+    -------
+
     """
     insertions = iter(insertions)
     try:
@@ -826,6 +1113,21 @@
     """Metaclass for ProfilingRegexLexer, collects regex timing info."""
 
     def _process_regex(cls, regex, rflags, state):
+        """
+
+        Parameters
+        ----------
+        regex :
+            
+        rflags :
+            
+        state :
+            
+
+        Returns
+        -------
+
+        """
         if isinstance(regex, words):
             rex = regex_opt(regex.words, prefix=regex.prefix,
                             suffix=regex.suffix)
@@ -834,6 +1136,21 @@
         compiled = re.compile(rex, rflags)
 
         def match_func(text, pos, endpos=sys.maxsize):
+            """
+
+            Parameters
+            ----------
+            text :
+                
+            pos :
+                
+            endpos :
+                 (Default value = sys.maxsize)
+
+            Returns
+            -------
+
+            """
             info = cls._prof_data[-1].setdefault((state, rex), [0, 0.0])
             t0 = time.time()
             res = compiled.match(text, pos, endpos)
@@ -851,6 +1168,21 @@
     _prof_sort_index = 4  # defaults to time per call
 
     def get_tokens_unprocessed(self, text, stack=('root',)):
+        """
+
+        Parameters
+        ----------
+        text :
+            
+        stack :
+             (Default value = ('root')
+        ) :
+            
+
+        Returns
+        -------
+
+        """
         # this needs to be a stack, since using(this) will produce nested calls
         self.__class__._prof_data.append({})
         yield from RegexLexer.get_tokens_unprocessed(self, text, stack)
