# Patch generated by Pyment v0.3.3

--- a/..//venv/lib/python3.8/site-packages/pygments/lexers/lisp.py
+++ b/..//venv/lib/python3.8/site-packages/pygments/lexers/lisp.py
@@ -23,18 +23,24 @@
 
 
 class SchemeLexer(RegexLexer):
-    """
-    A Scheme lexer, parsing a stream and outputting the tokens
+    """A Scheme lexer, parsing a stream and outputting the tokens
     needed to highlight scheme code.
     This lexer could be most probably easily subclassed to parse
     other LISP-Dialects like Common Lisp, Emacs Lisp or AutoLisp.
-
+    
     This parser is checked with pastes from the LISP pastebin
     at http://paste.lisp.org/ to cover as much syntax as possible.
-
+    
     It supports the full Scheme syntax as defined in R5RS.
-
+    
     .. versionadded:: 0.6
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'Scheme'
     aliases = ['scheme', 'scm']
@@ -166,10 +172,16 @@
 
 
 class CommonLispLexer(RegexLexer):
-    """
-    A Common Lisp lexer.
-
+    """A Common Lisp lexer.
+    
     .. versionadded:: 0.9
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'Common Lisp'
     aliases = ['common-lisp', 'cl', 'lisp']
@@ -203,6 +215,17 @@
         RegexLexer.__init__(self, **options)
 
     def get_tokens_unprocessed(self, text):
+        """
+
+        Parameters
+        ----------
+        text :
+            
+
+        Returns
+        -------
+
+        """
         stack = ['root']
         for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
             if token is Name.Variable:
@@ -344,10 +367,16 @@
 
 
 class HyLexer(RegexLexer):
-    """
-    Lexer for `Hy <http://hylang.org/>`_ source code.
-
+    """Lexer for `Hy <http://hylang.org/>`_ source code.
+    
     .. versionadded:: 2.0
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'Hy'
     aliases = ['hylang']
@@ -385,6 +414,17 @@
     valid_name = r'(?!#)[\w!$%*+<=>?/.#:-]+'
 
     def _multi_escape(entries):
+        """
+
+        Parameters
+        ----------
+        entries :
+            
+
+        Returns
+        -------
+
+        """
         return words(entries, suffix=' ')
 
     tokens = {
@@ -452,16 +492,33 @@
     }
 
     def analyse_text(text):
+        """
+
+        Parameters
+        ----------
+        text :
+            
+
+        Returns
+        -------
+
+        """
         if '(import ' in text or '(defn ' in text:
             return 0.9
 
 
 class RacketLexer(RegexLexer):
-    """
-    Lexer for `Racket <http://racket-lang.org/>`_ source code (formerly
+    """Lexer for `Racket <http://racket-lang.org/>`_ source code (formerly
     known as PLT Scheme).
-
+    
     .. versionadded:: 1.6
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
 
     name = 'Racket'
@@ -1399,10 +1456,16 @@
 
 
 class NewLispLexer(RegexLexer):
-    """
-    For `newLISP. <http://www.newlisp.org/>`_ source code (version 10.3.0).
-
+    """For `newLISP. <http://www.newlisp.org/>`_ source code (version 10.3.0).
+    
     .. versionadded:: 1.5
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
 
     name = 'NewLisp'
@@ -1530,11 +1593,17 @@
 
 
 class EmacsLispLexer(RegexLexer):
-    """
-    An ELisp lexer, parsing a stream and outputting the tokens
+    """An ELisp lexer, parsing a stream and outputting the tokens
     needed to highlight elisp code.
-
+    
     .. versionadded:: 2.1
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'EmacsLisp'
     aliases = ['emacs', 'elisp', 'emacs-lisp']
@@ -2070,6 +2139,17 @@
     }
 
     def get_tokens_unprocessed(self, text):
+        """
+
+        Parameters
+        ----------
+        text :
+            
+
+        Returns
+        -------
+
+        """
         stack = ['root']
         for index, token, value in RegexLexer.get_tokens_unprocessed(self, text, stack):
             if token is Name.Variable:
@@ -2175,10 +2255,16 @@
 
 
 class ShenLexer(RegexLexer):
-    """
-    Lexer for `Shen <http://shenlanguage.org/>`_ source code.
-
+    """Lexer for `Shen <http://shenlanguage.org/>`_ source code.
+    
     .. versionadded:: 2.1
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'Shen'
     aliases = ['shen']
@@ -2262,15 +2348,48 @@
     }
 
     def get_tokens_unprocessed(self, text):
+        """
+
+        Parameters
+        ----------
+        text :
+            
+
+        Returns
+        -------
+
+        """
         tokens = RegexLexer.get_tokens_unprocessed(self, text)
         tokens = self._process_symbols(tokens)
         tokens = self._process_declarations(tokens)
         return tokens
 
     def _relevant(self, token):
+        """
+
+        Parameters
+        ----------
+        token :
+            
+
+        Returns
+        -------
+
+        """
         return token not in (Text, Comment.Single, Comment.Multiline)
 
     def _process_declarations(self, tokens):
+        """
+
+        Parameters
+        ----------
+        tokens :
+            
+
+        Returns
+        -------
+
+        """
         opening_paren = False
         for index, token, value in tokens:
             yield index, token, value
@@ -2281,6 +2400,17 @@
                 opening_paren = value == '(' and token == Punctuation
 
     def _process_symbols(self, tokens):
+        """
+
+        Parameters
+        ----------
+        tokens :
+            
+
+        Returns
+        -------
+
+        """
         opening_paren = False
         for index, token, value in tokens:
             if opening_paren and token in (Literal, Name.Variable):
@@ -2291,6 +2421,19 @@
             yield index, token, value
 
     def _process_declaration(self, declaration, tokens):
+        """
+
+        Parameters
+        ----------
+        declaration :
+            
+        tokens :
+            
+
+        Returns
+        -------
+
+        """
         for index, token, value in tokens:
             if self._relevant(token):
                 break
@@ -2329,6 +2472,17 @@
         return
 
     def _process_signature(self, tokens):
+        """
+
+        Parameters
+        ----------
+        tokens :
+            
+
+        Returns
+        -------
+
+        """
         for index, token, value in tokens:
             if token == Literal and value == '}':
                 yield index, Punctuation, value
@@ -2339,10 +2493,16 @@
 
 
 class CPSALexer(SchemeLexer):
-    """
-    A CPSA lexer based on the CPSA language as of version 2.2.12
-
+    """A CPSA lexer based on the CPSA language as of version 2.2.12
+    
     .. versionadded:: 2.1
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'CPSA'
     aliases = ['cpsa']
@@ -2419,12 +2579,19 @@
 class XtlangLexer(RegexLexer):
     """An xtlang lexer for the `Extempore programming environment
     <http://extempore.moso.com.au>`_.
-
+    
     This is a mixture of Scheme and xtlang, really. Keyword lists are
     taken from the Extempore Emacs mode
     (https://github.com/extemporelang/extempore-emacs-mode)
-
+    
     .. versionadded:: 2.2
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'xtlang'
     aliases = ['extempore']
@@ -2622,11 +2789,18 @@
 
 class FennelLexer(RegexLexer):
     """A lexer for the `Fennel programming language <https://fennel-lang.org>`_.
-
+    
     Fennel compiles to Lua, so all the Lua builtins are recognized as well
     as the special forms that are particular to the Fennel compiler.
-
+    
     .. versionadded:: 2.3
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'Fennel'
     aliases = ['fennel', 'fnl']
