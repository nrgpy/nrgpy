# Patch generated by Pyment v0.3.3

--- a/..//venv/lib/python3.8/site-packages/pandas/tests/io/parser/test_network.py
+++ b/..//venv/lib/python3.8/site-packages/pandas/tests/io/parser/test_network.py
@@ -25,11 +25,49 @@
 @pytest.mark.parametrize("mode", ["explicit", "infer"])
 @pytest.mark.parametrize("engine", ["python", "c"])
 def test_compressed_urls(salaries_table, compress_type, extension, mode, engine):
+    """
+
+    Parameters
+    ----------
+    salaries_table :
+        
+    compress_type :
+        
+    extension :
+        
+    mode :
+        
+    engine :
+        
+
+    Returns
+    -------
+
+    """
     check_compressed_urls(salaries_table, compress_type, extension, mode, engine)
 
 
 @tm.network
 def check_compressed_urls(salaries_table, compression, extension, mode, engine):
+    """
+
+    Parameters
+    ----------
+    salaries_table :
+        
+    compression :
+        
+    extension :
+        
+    mode :
+        
+    engine :
+        
+
+    Returns
+    -------
+
+    """
     # test reading compressed urls with various engines and
     # extension inference
     base_url = (
@@ -48,15 +86,37 @@
 
 @pytest.fixture
 def tips_df(datapath):
-    """DataFrame with the tips dataset."""
+    """DataFrame with the tips dataset.
+
+    Parameters
+    ----------
+    datapath :
+        
+
+    Returns
+    -------
+
+    """
     return read_csv(datapath("io", "data", "csv", "tips.csv"))
 
 
 @pytest.mark.usefixtures("s3_resource")
 @td.skip_if_not_us_locale()
 class TestS3:
+    """ """
     @td.skip_if_no("s3fs")
     def test_parse_public_s3_bucket(self, tips_df):
+        """
+
+        Parameters
+        ----------
+        tips_df :
+            
+
+        Returns
+        -------
+
+        """
 
         # more of an integration test due to the not-public contents portion
         # can probably mock this though.
@@ -73,6 +133,17 @@
         tm.assert_frame_equal(df, tips_df)
 
     def test_parse_public_s3n_bucket(self, tips_df):
+        """
+
+        Parameters
+        ----------
+        tips_df :
+            
+
+        Returns
+        -------
+
+        """
 
         # Read from AWS s3 as "s3n" URL
         df = read_csv("s3n://pandas-test/tips.csv", nrows=10)
@@ -81,6 +152,17 @@
         tm.assert_frame_equal(tips_df.iloc[:10], df)
 
     def test_parse_public_s3a_bucket(self, tips_df):
+        """
+
+        Parameters
+        ----------
+        tips_df :
+            
+
+        Returns
+        -------
+
+        """
         # Read from AWS s3 as "s3a" URL
         df = read_csv("s3a://pandas-test/tips.csv", nrows=10)
         assert isinstance(df, DataFrame)
@@ -88,6 +170,17 @@
         tm.assert_frame_equal(tips_df.iloc[:10], df)
 
     def test_parse_public_s3_bucket_nrows(self, tips_df):
+        """
+
+        Parameters
+        ----------
+        tips_df :
+            
+
+        Returns
+        -------
+
+        """
         for ext, comp in [("", None), (".gz", "gzip"), (".bz2", "bz2")]:
             df = read_csv("s3://pandas-test/tips.csv" + ext, nrows=10, compression=comp)
             assert isinstance(df, DataFrame)
@@ -95,6 +188,17 @@
             tm.assert_frame_equal(tips_df.iloc[:10], df)
 
     def test_parse_public_s3_bucket_chunked(self, tips_df):
+        """
+
+        Parameters
+        ----------
+        tips_df :
+            
+
+        Returns
+        -------
+
+        """
         # Read with a chunksize
         chunksize = 5
         for ext, comp in [("", None), (".gz", "gzip"), (".bz2", "bz2")]:
@@ -112,6 +216,17 @@
                 tm.assert_frame_equal(true_df, df)
 
     def test_parse_public_s3_bucket_chunked_python(self, tips_df):
+        """
+
+        Parameters
+        ----------
+        tips_df :
+            
+
+        Returns
+        -------
+
+        """
         # Read with a chunksize using the Python parser
         chunksize = 5
         for ext, comp in [("", None), (".gz", "gzip"), (".bz2", "bz2")]:
@@ -131,6 +246,17 @@
                 tm.assert_frame_equal(true_df, df)
 
     def test_parse_public_s3_bucket_python(self, tips_df):
+        """
+
+        Parameters
+        ----------
+        tips_df :
+            
+
+        Returns
+        -------
+
+        """
         for ext, comp in [("", None), (".gz", "gzip"), (".bz2", "bz2")]:
             df = read_csv(
                 "s3://pandas-test/tips.csv" + ext, engine="python", compression=comp
@@ -140,6 +266,17 @@
             tm.assert_frame_equal(df, tips_df)
 
     def test_infer_s3_compression(self, tips_df):
+        """
+
+        Parameters
+        ----------
+        tips_df :
+            
+
+        Returns
+        -------
+
+        """
         for ext in ["", ".gz", ".bz2"]:
             df = read_csv(
                 "s3://pandas-test/tips.csv" + ext, engine="python", compression="infer"
@@ -149,6 +286,17 @@
             tm.assert_frame_equal(df, tips_df)
 
     def test_parse_public_s3_bucket_nrows_python(self, tips_df):
+        """
+
+        Parameters
+        ----------
+        tips_df :
+            
+
+        Returns
+        -------
+
+        """
         for ext, comp in [("", None), (".gz", "gzip"), (".bz2", "bz2")]:
             df = read_csv(
                 "s3://pandas-test/tips.csv" + ext,
@@ -161,6 +309,7 @@
             tm.assert_frame_equal(tips_df.iloc[:10], df)
 
     def test_read_s3_fails(self):
+        """ """
         with pytest.raises(IOError):
             read_csv("s3://nyqpug/asdf.csv")
 
@@ -170,6 +319,17 @@
             read_csv("s3://cant_get_it/file.csv")
 
     def test_write_s3_csv_fails(self, tips_df):
+        """
+
+        Parameters
+        ----------
+        tips_df :
+            
+
+        Returns
+        -------
+
+        """
         # GH 32486
         # Attempting to write to an invalid S3 path should raise
         import botocore
@@ -184,6 +344,17 @@
 
     @td.skip_if_no("pyarrow")
     def test_write_s3_parquet_fails(self, tips_df):
+        """
+
+        Parameters
+        ----------
+        tips_df :
+            
+
+        Returns
+        -------
+
+        """
         # GH 27679
         # Attempting to write to an invalid S3 path should raise
         import botocore
@@ -197,6 +368,19 @@
             tips_df.to_parquet("s3://an_s3_bucket_data_doesnt_exit/not_real.parquet")
 
     def test_read_csv_handles_boto_s3_object(self, s3_resource, tips_file):
+        """
+
+        Parameters
+        ----------
+        s3_resource :
+            
+        tips_file :
+            
+
+        Returns
+        -------
+
+        """
         # see gh-16135
 
         s3_object = s3_resource.meta.client.get_object(
@@ -211,6 +395,19 @@
         tm.assert_frame_equal(result, expected)
 
     def test_read_csv_chunked_download(self, s3_resource, caplog):
+        """
+
+        Parameters
+        ----------
+        s3_resource :
+            
+        caplog :
+            
+
+        Returns
+        -------
+
+        """
         # 8 MB, S3FS usees 5MB chunks
         import s3fs
 
@@ -235,12 +432,34 @@
             assert (0, 5505024) in (x.args[-2:] for x in caplog.records)
 
     def test_read_s3_with_hash_in_key(self, tips_df):
+        """
+
+        Parameters
+        ----------
+        tips_df :
+            
+
+        Returns
+        -------
+
+        """
         # GH 25945
         result = read_csv("s3://pandas-test/tips#1.csv")
         tm.assert_frame_equal(tips_df, result)
 
     @td.skip_if_no("pyarrow")
     def test_read_feather_s3_file_path(self, feather_file):
+        """
+
+        Parameters
+        ----------
+        feather_file :
+            
+
+        Returns
+        -------
+
+        """
         # GH 29055
         expected = read_feather(feather_file)
         res = read_feather("s3://pandas-test/simple_dataset.feather")
