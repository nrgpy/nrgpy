# Patch generated by Pyment v0.3.3

--- a/..//venv/lib/python3.8/site-packages/parso/parser.py
+++ b/..//venv/lib/python3.8/site-packages/parso/parser.py
@@ -28,10 +28,16 @@
 
 
 class ParserSyntaxError(Exception):
-    """
-    Contains error information about the parser tree.
-
+    """Contains error information about the parser tree.
+    
     May be raised as an exception.
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     def __init__(self, message, error_leaf):
         self.message = message
@@ -40,9 +46,20 @@
 
 class InternalParseError(Exception):
     """
-    Exception to signal the parser is stuck and error recovery didn't help.
-    Basically this shouldn't happen. It's a sign that something is really
-    wrong.
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
+    Raises
+    ------
+    Basically
+        this shouldn
+    wrong
+        
+
     """
 
     def __init__(self, msg, type_, value, start_pos):
@@ -55,8 +72,11 @@
 
 
 class Stack(list):
+    """ """
     def _allowed_transition_names_and_token_types(self):
+        """ """
         def iterate():
+            """ """
             # An API just for Jedi.
             for stack_node in reversed(self):
                 for transition in stack_node.dfa.transitions:
@@ -72,12 +92,14 @@
 
 
 class StackNode(object):
+    """ """
     def __init__(self, dfa):
         self.dfa = dfa
         self.nodes = []
 
     @property
     def nonterminal(self):
+        """ """
         return self.dfa.from_rule
 
     def __repr__(self):
@@ -85,6 +107,21 @@
 
 
 def _token_to_transition(grammar, type_, value):
+    """
+
+    Parameters
+    ----------
+    grammar :
+        
+    type_ :
+        
+    value :
+        
+
+    Returns
+    -------
+
+    """
     # Map from token to label
     if type_.contains_syntax:
         # Check for reserved words (keywords)
@@ -98,14 +135,21 @@
 
 class BaseParser(object):
     """Parser engine.
-
+    
     A Parser instance contains state pertaining to the current token
     sequence, and should not be used concurrently by different threads
     to parse separate token sequences.
-
+    
     See python/tokenize.py for how to get input tokens by a string.
-
+    
     When a syntax error occurs, error_recovery() is called.
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
 
     node_map = {}
@@ -121,6 +165,17 @@
         self._error_recovery = error_recovery
 
     def parse(self, tokens):
+        """
+
+        Parameters
+        ----------
+        tokens :
+            
+
+        Returns
+        -------
+
+        """
         first_dfa = self._pgen_grammar.nonterminal_to_dfas[self._start_nonterminal][0]
         self.stack = Stack([StackNode(first_dfa)])
 
@@ -143,6 +198,17 @@
                 return self.convert_node(tos.nonterminal, tos.nodes)
 
     def error_recovery(self, token):
+        """
+
+        Parameters
+        ----------
+        token :
+            
+
+        Returns
+        -------
+
+        """
         if self._error_recovery:
             raise NotImplementedError("Error Recovery is not implemented")
         else:
@@ -151,6 +217,19 @@
             raise ParserSyntaxError('SyntaxError: invalid syntax', error_leaf)
 
     def convert_node(self, nonterminal, children):
+        """
+
+        Parameters
+        ----------
+        nonterminal :
+            
+        children :
+            
+
+        Returns
+        -------
+
+        """
         try:
             node = self.node_map[nonterminal](children)
         except KeyError:
@@ -160,16 +239,41 @@
         return node
 
     def convert_leaf(self, type_, value, prefix, start_pos):
+        """
+
+        Parameters
+        ----------
+        type_ :
+            
+        value :
+            
+        prefix :
+            
+        start_pos :
+            
+
+        Returns
+        -------
+
+        """
         try:
             return self.leaf_map[type_](value, start_pos, prefix)
         except KeyError:
             return self.default_leaf(value, start_pos, prefix)
 
     def _add_token(self, token):
-        """
-        This is the only core function for parsing. Here happens basically
+        """This is the only core function for parsing. Here happens basically
         everything. Everything is well prepared by the parser generator and we
         only apply the necessary steps here.
+
+        Parameters
+        ----------
+        token :
+            
+
+        Returns
+        -------
+
         """
         grammar = self._pgen_grammar
         stack = self.stack
@@ -198,6 +302,7 @@
         stack[-1].nodes.append(leaf)
 
     def _pop(self):
+        """ """
         tos = self.stack.pop()
         # If there's exactly one child, return that child instead of
         # creating a new node.  We still create expr_stmt and
