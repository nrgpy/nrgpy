# Patch generated by Pyment v0.3.3

--- a/..//venv/lib/python3.8/site-packages/pandas/io/parquet.py
+++ b/..//venv/lib/python3.8/site-packages/pandas/io/parquet.py
@@ -13,7 +13,19 @@
 
 
 def get_engine(engine: str) -> "BaseImpl":
-    """ return our implementation """
+    """
+
+    Parameters
+    ----------
+    engine: str :
+        
+
+    Returns
+    -------
+    type
+        
+
+    """
     if engine == "auto":
         engine = get_option("io.parquet.engine")
 
@@ -47,8 +59,20 @@
 
 
 class BaseImpl:
+    """ """
     @staticmethod
     def validate_dataframe(df: DataFrame):
+        """
+
+        Parameters
+        ----------
+        df: DataFrame :
+            
+
+        Returns
+        -------
+
+        """
 
         if not isinstance(df, DataFrame):
             raise ValueError("to_parquet only supports IO with DataFrames")
@@ -65,13 +89,46 @@
             raise ValueError("Index level names must be strings")
 
     def write(self, df: DataFrame, path, compression, **kwargs):
+        """
+
+        Parameters
+        ----------
+        df: DataFrame :
+            
+        path :
+            
+        compression :
+            
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         raise AbstractMethodError(self)
 
     def read(self, path, columns=None, **kwargs):
+        """
+
+        Parameters
+        ----------
+        path :
+            
+        columns :
+             (Default value = None)
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         raise AbstractMethodError(self)
 
 
 class PyArrowImpl(BaseImpl):
+    """ """
     def __init__(self):
         import_optional_dependency(
             "pyarrow", extra="pyarrow is required for parquet support."
@@ -92,6 +149,27 @@
         partition_cols: Optional[List[str]] = None,
         **kwargs,
     ):
+        """
+
+        Parameters
+        ----------
+        df: DataFrame :
+            
+        path: FilePathOrBuffer[AnyStr] :
+            
+        compression: Optional[str] :
+             (Default value = "snappy")
+        index: Optional[bool] :
+             (Default value = None)
+        partition_cols: Optional[List[str]] :
+             (Default value = None)
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         self.validate_dataframe(df)
 
         from_pandas_kwargs: Dict[str, Any] = {"schema": kwargs.pop("schema", None)}
@@ -123,6 +201,21 @@
             self.api.parquet.write_table(table, path, compression=compression, **kwargs)
 
     def read(self, path, columns=None, **kwargs):
+        """
+
+        Parameters
+        ----------
+        path :
+            
+        columns :
+             (Default value = None)
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         if is_fsspec_url(path) and "filesystem" not in kwargs:
             import_optional_dependency("fsspec")
             import fsspec.core
@@ -148,6 +241,7 @@
 
 
 class FastParquetImpl(BaseImpl):
+    """ """
     def __init__(self):
         # since pandas is a dependency of fastparquet
         # we need to import on first use
@@ -165,6 +259,27 @@
         partition_cols=None,
         **kwargs,
     ):
+        """
+
+        Parameters
+        ----------
+        df: DataFrame :
+            
+        path :
+            
+        compression :
+             (Default value = "snappy")
+        index :
+             (Default value = None)
+        partition_cols :
+             (Default value = None)
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         self.validate_dataframe(df)
         # thriftpy/protocol/compact.py:339:
         # DeprecationWarning: tostring() is deprecated.
@@ -200,6 +315,21 @@
             )
 
     def read(self, path, columns=None, **kwargs):
+        """
+
+        Parameters
+        ----------
+        path :
+            
+        columns :
+             (Default value = None)
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         if is_fsspec_url(path):
             fsspec = import_optional_dependency("fsspec")
 
@@ -221,48 +351,29 @@
     partition_cols: Optional[List[str]] = None,
     **kwargs,
 ):
-    """
-    Write a DataFrame to the parquet format.
+    """Write a DataFrame to the parquet format.
 
     Parameters
     ----------
-    df : DataFrame
-    path : str or file-like object
-        If a string, it will be used as Root Directory path
-        when writing a partitioned dataset. By file-like object,
-        we refer to objects with a write() method, such as a file handler
-        (e.g. via builtin open function) or io.BytesIO. The engine
-        fastparquet does not accept file-like objects.
-
-        .. versionchanged:: 0.24.0
-
-    engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'
-        Parquet library to use. If 'auto', then the option
-        ``io.parquet.engine`` is used. The default ``io.parquet.engine``
-        behavior is to try 'pyarrow', falling back to 'fastparquet' if
-        'pyarrow' is unavailable.
-    compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy'
-        Name of the compression to use. Use ``None`` for no compression.
-    index : bool, default None
-        If ``True``, include the dataframe's index(es) in the file output. If
-        ``False``, they will not be written to the file.
-        If ``None``, similar to ``True`` the dataframe's index(es)
-        will be saved. However, instead of being saved as values,
-        the RangeIndex will be stored as a range in the metadata so it
-        doesn't require much space and is faster. Other indexes will
-        be included as columns in the file output.
-
-        .. versionadded:: 0.24.0
-
-    partition_cols : str or list, optional, default None
-        Column names by which to partition the dataset.
-        Columns are partitioned in the order they are given.
-        Must be None if path is not a string.
-
-        .. versionadded:: 0.24.0
-
-    kwargs
-        Additional keyword arguments passed to the engine
+    df: DataFrame :
+        
+    path: FilePathOrBuffer[AnyStr] :
+        
+    engine: str :
+         (Default value = "auto")
+    compression: Optional[str] :
+         (Default value = "snappy")
+    index: Optional[bool] :
+         (Default value = None)
+    partition_cols: Optional[List[str]] :
+         (Default value = None)
+    **kwargs :
+        
+
+    Returns
+    -------
+
+    
     """
     if isinstance(partition_cols, str):
         partition_cols = [partition_cols]
@@ -278,8 +389,7 @@
 
 
 def read_parquet(path, engine: str = "auto", columns=None, **kwargs):
-    """
-    Load a parquet object from the file path, returning a DataFrame.
+    """Load a parquet object from the file path, returning a DataFrame.
 
     Parameters
     ----------
@@ -292,10 +402,8 @@
         partitioned parquet files. Both pyarrow and fastparquet support
         paths to directories as well as file URLs. A directory path could be:
         ``file://localhost/path/to/tables`` or ``s3://bucket/partition_dir``
-
         If you want to pass in a path object, pandas accepts any
         ``os.PathLike``.
-
         By file-like object, we refer to objects with a ``read()`` method,
         such as a file handler (e.g. via builtin ``open`` function)
         or ``StringIO``.
@@ -305,13 +413,16 @@
         behavior is to try 'pyarrow', falling back to 'fastparquet' if
         'pyarrow' is unavailable.
     columns : list, default=None
-        If not None, only these columns will be read from the file.
-    **kwargs
+        If not None, only these columns will be read from the file. (Default value = None)
+    **kwargs :
         Any additional kwargs are passed to the engine.
+    engine: str :
+         (Default value = "auto")
 
     Returns
     -------
-    DataFrame
+
+    
     """
     impl = get_engine(engine)
     return impl.read(path, columns=columns, **kwargs)
