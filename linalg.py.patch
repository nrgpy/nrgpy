# Patch generated by Pyment v0.3.3

--- a/..//venv/lib/python3.8/site-packages/numpy/linalg/linalg.py
+++ b/..//venv/lib/python3.8/site-packages/numpy/linalg/linalg.py
@@ -42,9 +42,8 @@
 
 @set_module('numpy.linalg')
 class LinAlgError(Exception):
-    """
-    Generic Python-exception-derived object raised by linalg functions.
-
+    """Generic Python-exception-derived object raised by linalg functions.
+    
     General purpose exception class, derived from Python's exception.Exception
     class, programmatically raised in linalg functions when a Linear
     Algebra-related condition would prevent further correct execution of the
@@ -52,7 +51,11 @@
 
     Parameters
     ----------
-    None
+    None :
+        
+
+    Returns
+    -------
 
     Examples
     --------
@@ -66,11 +69,11 @@
         in solve
         raise LinAlgError('Singular matrix')
     numpy.linalg.LinAlgError: Singular matrix
-
     """
 
 
 def _determine_error_states():
+    """ """
     errobj = geterrobj()
     bufsize = errobj[0]
 
@@ -85,31 +88,129 @@
 del _determine_error_states
 
 def _raise_linalgerror_singular(err, flag):
+    """
+
+    Parameters
+    ----------
+    err :
+        
+    flag :
+        
+
+    Returns
+    -------
+
+    """
     raise LinAlgError("Singular matrix")
 
 def _raise_linalgerror_nonposdef(err, flag):
+    """
+
+    Parameters
+    ----------
+    err :
+        
+    flag :
+        
+
+    Returns
+    -------
+
+    """
     raise LinAlgError("Matrix is not positive definite")
 
 def _raise_linalgerror_eigenvalues_nonconvergence(err, flag):
+    """
+
+    Parameters
+    ----------
+    err :
+        
+    flag :
+        
+
+    Returns
+    -------
+
+    """
     raise LinAlgError("Eigenvalues did not converge")
 
 def _raise_linalgerror_svd_nonconvergence(err, flag):
+    """
+
+    Parameters
+    ----------
+    err :
+        
+    flag :
+        
+
+    Returns
+    -------
+
+    """
     raise LinAlgError("SVD did not converge")
 
 def _raise_linalgerror_lstsq(err, flag):
+    """
+
+    Parameters
+    ----------
+    err :
+        
+    flag :
+        
+
+    Returns
+    -------
+
+    """
     raise LinAlgError("SVD did not converge in Linear Least Squares")
 
 def get_linalg_error_extobj(callback):
+    """
+
+    Parameters
+    ----------
+    callback :
+        
+
+    Returns
+    -------
+
+    """
     extobj = list(_linalg_error_extobj)  # make a copy
     extobj[2] = callback
     return extobj
 
 def _makearray(a):
+    """
+
+    Parameters
+    ----------
+    a :
+        
+
+    Returns
+    -------
+
+    """
     new = asarray(a)
     wrap = getattr(a, "__array_prepare__", new.__array_wrap__)
     return new, wrap
 
 def isComplexType(t):
+    """
+
+    Parameters
+    ----------
+    t :
+        
+
+    Returns
+    -------
+
+    """
     return issubclass(t, complexfloating)
 
 _real_types_map = {single : single,
@@ -123,16 +224,63 @@
                       cdouble : cdouble}
 
 def _realType(t, default=double):
+    """
+
+    Parameters
+    ----------
+    t :
+        
+    default :
+         (Default value = double)
+
+    Returns
+    -------
+
+    """
     return _real_types_map.get(t, default)
 
 def _complexType(t, default=cdouble):
+    """
+
+    Parameters
+    ----------
+    t :
+        
+    default :
+         (Default value = cdouble)
+
+    Returns
+    -------
+
+    """
     return _complex_types_map.get(t, default)
 
 def _linalgRealType(t):
-    """Cast the type t to either double or cdouble."""
+    """Cast the type t to either double or cdouble.
+
+    Parameters
+    ----------
+    t :
+        
+
+    Returns
+    -------
+
+    """
     return double
 
 def _commonType(*arrays):
+    """
+
+    Parameters
+    ----------
+    *arrays :
+        
+
+    Returns
+    -------
+
+    """
     # in lite version, use higher precision (always double or cdouble)
     result_type = single
     is_complex = False
@@ -162,6 +310,17 @@
 _fastCT = fastCopyAndTranspose
 
 def _to_native_byte_order(*arrays):
+    """
+
+    Parameters
+    ----------
+    *arrays :
+        
+
+    Returns
+    -------
+
+    """
     ret = []
     for arr in arrays:
         if arr.dtype.byteorder not in ('=', '|'):
@@ -174,6 +333,19 @@
         return ret
 
 def _fastCopyAndTranspose(type, *arrays):
+    """
+
+    Parameters
+    ----------
+    type :
+        
+    *arrays :
+        
+
+    Returns
+    -------
+
+    """
     cast_arrays = ()
     for a in arrays:
         if a.dtype.type is type:
@@ -186,61 +358,131 @@
         return cast_arrays
 
 def _assert_2d(*arrays):
+    """
+
+    Parameters
+    ----------
+    *arrays :
+        
+
+    Returns
+    -------
+
+    """
     for a in arrays:
         if a.ndim != 2:
             raise LinAlgError('%d-dimensional array given. Array must be '
                     'two-dimensional' % a.ndim)
 
 def _assert_stacked_2d(*arrays):
+    """
+
+    Parameters
+    ----------
+    *arrays :
+        
+
+    Returns
+    -------
+
+    """
     for a in arrays:
         if a.ndim < 2:
             raise LinAlgError('%d-dimensional array given. Array must be '
                     'at least two-dimensional' % a.ndim)
 
 def _assert_stacked_square(*arrays):
+    """
+
+    Parameters
+    ----------
+    *arrays :
+        
+
+    Returns
+    -------
+
+    """
     for a in arrays:
         m, n = a.shape[-2:]
         if m != n:
             raise LinAlgError('Last 2 dimensions of the array must be square')
 
 def _assert_finite(*arrays):
+    """
+
+    Parameters
+    ----------
+    *arrays :
+        
+
+    Returns
+    -------
+
+    """
     for a in arrays:
         if not isfinite(a).all():
             raise LinAlgError("Array must not contain infs or NaNs")
 
 def _is_empty_2d(arr):
+    """
+
+    Parameters
+    ----------
+    arr :
+        
+
+    Returns
+    -------
+
+    """
     # check size first for efficiency
     return arr.size == 0 and product(arr.shape[-2:]) == 0
 
 
 def transpose(a):
-    """
-    Transpose each matrix in a stack of matrices.
-
+    """Transpose each matrix in a stack of matrices.
+    
     Unlike np.transpose, this only swaps the last two axes, rather than all of
     them
 
     Parameters
     ----------
     a : (...,M,N) array_like
-
-    Returns
-    -------
-    aT : (...,N,M) ndarray
+        
+
+    Returns
+    -------
+
+    
     """
     return swapaxes(a, -1, -2)
 
 # Linear equations
 
 def _tensorsolve_dispatcher(a, b, axes=None):
+    """
+
+    Parameters
+    ----------
+    a :
+        
+    b :
+        
+    axes :
+         (Default value = None)
+
+    Returns
+    -------
+
+    """
     return (a, b)
 
 
 @array_function_dispatch(_tensorsolve_dispatcher)
 def tensorsolve(a, b, axes=None):
-    """
-    Solve the tensor equation ``a x = b`` for x.
-
+    """Solve the tensor equation ``a x = b`` for x.
+    
     It is assumed that all indices of `x` are summed over in the product,
     together with the rightmost indices of `a`, as is done in, for example,
     ``tensordot(a, x, axes=b.ndim)``.
@@ -262,6 +504,7 @@
     Returns
     -------
     x : ndarray, shape Q
+        
 
     Raises
     ------
@@ -271,7 +514,6 @@
     See Also
     --------
     numpy.tensordot, tensorinv, numpy.einsum
-
     Examples
     --------
     >>> a = np.eye(2*3*4)
@@ -282,7 +524,6 @@
     (2, 3, 4)
     >>> np.allclose(np.tensordot(a, x, axes=3), b)
     True
-
     """
     a, wrap = _makearray(a)
     b = asarray(b)
@@ -308,14 +549,26 @@
 
 
 def _solve_dispatcher(a, b):
+    """
+
+    Parameters
+    ----------
+    a :
+        
+    b :
+        
+
+    Returns
+    -------
+
+    """
     return (a, b)
 
 
 @array_function_dispatch(_solve_dispatcher)
 def solve(a, b):
-    """
-    Solve a linear matrix equation, or system of linear scalar equations.
-
+    """Solve a linear matrix equation, or system of linear scalar equations.
+    
     Computes the "exact" solution, `x`, of the well-determined, i.e., full
     rank, linear matrix equation `ax = b`.
 
@@ -339,42 +592,38 @@
     See Also
     --------
     scipy.linalg.solve : Similar function in SciPy.
-
     Notes
     -----
-
+    
     .. versionadded:: 1.8.0
-
+    
     Broadcasting rules apply, see the `numpy.linalg` documentation for
     details.
-
+    
     The solutions are computed using LAPACK routine ``_gesv``.
-
+    
     `a` must be square and of full-rank, i.e., all rows (or, equivalently,
     columns) must be linearly independent; if either is not true, use
     `lstsq` for the least-squares best "solution" of the
     system/equation.
-
     References
     ----------
     .. [1] G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando,
            FL, Academic Press, Inc., 1980, pg. 22.
-
     Examples
     --------
     Solve the system of equations ``3 * x0 + x1 = 9`` and ``x0 + 2 * x1 = 8``:
-
+    
+    
+    Check that the solution is correct:
     >>> a = np.array([[3,1], [1,2]])
     >>> b = np.array([9,8])
     >>> x = np.linalg.solve(a, b)
     >>> x
     array([2.,  3.])
-
-    Check that the solution is correct:
-
+    
     >>> np.allclose(np.dot(a, x), b)
     True
-
     """
     a, _ = _makearray(a)
     _assert_stacked_2d(a)
@@ -397,14 +646,26 @@
 
 
 def _tensorinv_dispatcher(a, ind=None):
+    """
+
+    Parameters
+    ----------
+    a :
+        
+    ind :
+         (Default value = None)
+
+    Returns
+    -------
+
+    """
     return (a,)
 
 
 @array_function_dispatch(_tensorinv_dispatcher)
 def tensorinv(a, ind=2):
-    """
-    Compute the 'inverse' of an N-dimensional array.
-
+    """Compute the 'inverse' of an N-dimensional array.
+    
     The result is an inverse for `a` relative to the tensordot operation
     ``tensordot(a, b, ind)``, i. e., up to floating-point accuracy,
     ``tensordot(tensorinv(a), a, ind)`` is the "identity" tensor for the
@@ -432,7 +693,6 @@
     See Also
     --------
     numpy.tensordot, tensorsolve
-
     Examples
     --------
     >>> a = np.eye(4*6)
@@ -443,7 +703,7 @@
     >>> b = np.random.randn(4, 6)
     >>> np.allclose(np.tensordot(ainv, b), np.linalg.tensorsolve(a, b))
     True
-
+    
     >>> a = np.eye(4*6)
     >>> a.shape = (24, 8, 3)
     >>> ainv = np.linalg.tensorinv(a, ind=1)
@@ -452,7 +712,6 @@
     >>> b = np.random.randn(24)
     >>> np.allclose(np.tensordot(ainv, b, 1), np.linalg.tensorsolve(a, b))
     True
-
     """
     a = asarray(a)
     oldshape = a.shape
@@ -471,14 +730,24 @@
 # Matrix inversion
 
 def _unary_dispatcher(a):
+    """
+
+    Parameters
+    ----------
+    a :
+        
+
+    Returns
+    -------
+
+    """
     return (a,)
 
 
 @array_function_dispatch(_unary_dispatcher)
 def inv(a):
-    """
-    Compute the (multiplicative) inverse of a matrix.
-
+    """Compute the (multiplicative) inverse of a matrix.
+    
     Given a square matrix `a`, return the matrix `ainv` satisfying
     ``dot(a, ainv) = dot(ainv, a) = eye(a.shape[0])``.
 
@@ -500,17 +769,20 @@
     See Also
     --------
     scipy.linalg.inv : Similar function in SciPy.
-
     Notes
     -----
-
+    
     .. versionadded:: 1.8.0
-
+    
     Broadcasting rules apply, see the `numpy.linalg` documentation for
     details.
-
     Examples
     --------
+    
+    If a is a matrix object, then the return value is a matrix as well:
+    
+    
+    Inverses of several matrices can be computed at once:
     >>> from numpy.linalg import inv
     >>> a = np.array([[1., 2.], [3., 4.]])
     >>> ainv = inv(a)
@@ -518,23 +790,18 @@
     True
     >>> np.allclose(np.dot(ainv, a), np.eye(2))
     True
-
-    If a is a matrix object, then the return value is a matrix as well:
-
+    
     >>> ainv = inv(np.matrix(a))
     >>> ainv
     matrix([[-2. ,  1. ],
             [ 1.5, -0.5]])
-
-    Inverses of several matrices can be computed at once:
-
+    
     >>> a = np.array([[[1., 2.], [3., 4.]], [[1, 3], [3, 5]]])
     >>> inv(a)
     array([[[-2.  ,  1.  ],
             [ 1.5 , -0.5 ]],
            [[-1.25,  0.75],
             [ 0.75, -0.25]]])
-
     """
     a, wrap = _makearray(a)
     _assert_stacked_2d(a)
@@ -548,19 +815,31 @@
 
 
 def _matrix_power_dispatcher(a, n):
+    """
+
+    Parameters
+    ----------
+    a :
+        
+    n :
+        
+
+    Returns
+    -------
+
+    """
     return (a,)
 
 
 @array_function_dispatch(_matrix_power_dispatcher)
 def matrix_power(a, n):
-    """
-    Raise a square matrix to the (integer) power `n`.
-
+    """Raise a square matrix to the (integer) power `n`.
+    
     For positive integers `n`, the power is computed by repeated matrix
     squarings and matrix multiplications. If ``n == 0``, the identity matrix
     of the same shape as M is returned. If ``n < 0``, the inverse
     is computed and then raised to the ``abs(n)``.
-
+    
     .. note:: Stacks of object matrices are not currently supported.
 
     Parameters
@@ -587,6 +866,8 @@
 
     Examples
     --------
+    
+    Somewhat more sophisticated example
     >>> from numpy.linalg import matrix_power
     >>> i = np.array([[0, 1], [-1, 0]]) # matrix equiv. of the imaginary unit
     >>> matrix_power(i, 3) # should = -i
@@ -598,9 +879,7 @@
     >>> matrix_power(i, -3) # should = 1/(-i) = i, but w/ f.p. elements
     array([[ 0.,  1.],
            [-1.,  0.]])
-
-    Somewhat more sophisticated example
-
+    
     >>> q = np.zeros((4, 4))
     >>> q[0:2, 0:2] = -i
     >>> q[2:4, 2:4] = i
@@ -614,7 +893,6 @@
            [ 0., -1.,  0.,  0.],
            [ 0.,  0., -1.,  0.],
            [ 0.,  0.,  0., -1.]])
-
     """
     a = asanyarray(a)
     _assert_stacked_2d(a)
@@ -672,9 +950,8 @@
 
 @array_function_dispatch(_unary_dispatcher)
 def cholesky(a):
-    """
-    Cholesky decomposition.
-
+    """Cholesky decomposition.
+    
     Return the Cholesky decomposition, `L * L.H`, of the square matrix `a`,
     where `L` is lower-triangular and .H is the conjugate transpose operator
     (which is the ordinary transpose if `a` is real-valued).  `a` must be
@@ -698,8 +975,8 @@
     Raises
     ------
     LinAlgError
-       If the decomposition fails, for example, if `a` is not
-       positive-definite.
+        If the decomposition fails, for example, if `a` is not
+        positive-definite.
 
     See Also
     --------
@@ -708,29 +985,27 @@
                                    positive-definite matrix.
     scipy.linalg.cho_factor : Cholesky decomposition of a matrix, to use in
                               `scipy.linalg.cho_solve`.
-
     Notes
     -----
-
+    
     .. versionadded:: 1.8.0
-
+    
     Broadcasting rules apply, see the `numpy.linalg` documentation for
     details.
-
+    
     The Cholesky decomposition is often used as a fast way of solving
-
+    
     .. math:: A \\mathbf{x} = \\mathbf{b}
-
+    
     (when `A` is both Hermitian/symmetric and positive-definite).
-
+    
     First, we solve for :math:`\\mathbf{y}` in
-
+    
     .. math:: L \\mathbf{y} = \\mathbf{b},
-
+    
     and then for :math:`\\mathbf{x}` in
-
+    
     .. math:: L.H \\mathbf{x} = \\mathbf{y}.
-
     Examples
     --------
     >>> A = np.array([[1,-2j],[2j,5]])
@@ -752,7 +1027,6 @@
     >>> np.linalg.cholesky(np.matrix(A))
     matrix([[ 1.+0.j,  0.+0.j],
             [ 0.+2.j,  1.+0.j]])
-
     """
     extobj = get_linalg_error_extobj(_raise_linalgerror_nonposdef)
     gufunc = _umath_linalg.cholesky_lo
@@ -768,14 +1042,26 @@
 # QR decomposition
 
 def _qr_dispatcher(a, mode=None):
+    """
+
+    Parameters
+    ----------
+    a :
+        
+    mode :
+         (Default value = None)
+
+    Returns
+    -------
+
+    """
     return (a,)
 
 
 @array_function_dispatch(_qr_dispatcher)
 def qr(a, mode='reduced'):
-    """
-    Compute the qr factorization of a matrix.
-
+    """Compute the qr factorization of a matrix.
+    
     Factor the matrix `a` as *qr*, where `q` is orthonormal and `r` is
     upper-triangular.
 
@@ -785,12 +1071,10 @@
         Matrix to be factored.
     mode : {'reduced', 'complete', 'r', 'raw'}, optional
         If K = min(M, N), then
-
         * 'reduced'  : returns q, r with dimensions (M, K), (K, N) (default)
         * 'complete' : returns q, r with dimensions (M, M), (M, N)
         * 'r'        : returns r only with dimensions (K, N)
         * 'raw'      : returns h, tau with dimensions (N, M), (K,)
-
         The options 'reduced', 'complete, and 'raw' are new in numpy 1.8,
         see the notes for more information. The default is 'reduced', and to
         maintain backward compatibility with earlier versions of numpy both
@@ -800,7 +1084,6 @@
         be passed using only the first letter for backwards compatibility,
         but all others must be spelled out. See the Notes for more
         explanation.
-
 
     Returns
     -------
@@ -825,18 +1108,17 @@
     --------
     scipy.linalg.qr : Similar function in SciPy.
     scipy.linalg.rq : Compute RQ decomposition of a matrix.
-
     Notes
     -----
     This is an interface to the LAPACK routines ``dgeqrf``, ``zgeqrf``,
     ``dorgqr``, and ``zungqr``.
-
+    
     For more information on the qr factorization, see for example:
     https://en.wikipedia.org/wiki/QR_factorization
-
+    
     Subclasses of `ndarray` are preserved except for the 'raw' mode. So if
     `a` is of type `matrix`, all the return values will be matrices too.
-
+    
     New 'reduced', 'complete', and 'raw' options for mode were added in
     NumPy 1.8.0 and the old option 'full' was made an alias of 'reduced'.  In
     addition the options 'full' and 'economic' were deprecated.  Because
@@ -848,9 +1130,24 @@
     the h array is transposed to be FORTRAN compatible.  No routines using
     the 'raw' return are currently exposed by numpy, but some are available
     in lapack_lite and just await the necessary work.
-
     Examples
     --------
+    
+    Example illustrating a common use of `qr`: solving of least squares
+    problems
+    
+    What are the least-squares-best `m` and `y0` in ``y = y0 + mx`` for
+    the following data: {(0,1), (1,0), (1,2), (2,1)}. (Graph the points
+    and you'll see that it should be y0 = 0, m = 1.)  The answer is provided
+    by solving the over-determined matrix equation ``Ax = b``, where::
+    
+      A = array([[0, 1], [1, 1], [1, 1], [2, 1]])
+      x = array([[y0], [m]])
+      b = array([[1], [0], [2], [1]])
+    
+    If A = qr such that q is orthonormal (which is always possible via
+    Gram-Schmidt), then ``x = inv(r) * (q.T) * b``.  (In numpy practice,
+    however, we simply use `lstsq`.)
     >>> a = np.random.randn(9, 6)
     >>> q, r = np.linalg.qr(a)
     >>> np.allclose(a, np.dot(q, r))  # a does equal qr
@@ -858,23 +1155,7 @@
     >>> r2 = np.linalg.qr(a, mode='r')
     >>> np.allclose(r, r2)  # mode='r' returns the same r as mode='full'
     True
-
-    Example illustrating a common use of `qr`: solving of least squares
-    problems
-
-    What are the least-squares-best `m` and `y0` in ``y = y0 + mx`` for
-    the following data: {(0,1), (1,0), (1,2), (2,1)}. (Graph the points
-    and you'll see that it should be y0 = 0, m = 1.)  The answer is provided
-    by solving the over-determined matrix equation ``Ax = b``, where::
-
-      A = array([[0, 1], [1, 1], [1, 1], [2, 1]])
-      x = array([[y0], [m]])
-      b = array([[1], [0], [2], [1]])
-
-    If A = qr such that q is orthonormal (which is always possible via
-    Gram-Schmidt), then ``x = inv(r) * (q.T) * b``.  (In numpy practice,
-    however, we simply use `lstsq`.)
-
+    
     >>> A = np.array([[0, 1], [1, 1], [1, 1], [2, 1]])
     >>> A
     array([[0, 1],
@@ -886,7 +1167,6 @@
     >>> p = np.dot(q.T, b)
     >>> np.dot(np.linalg.inv(r), p)
     array([  1.1e-16,   1.0e+00])
-
     """
     if mode not in ('reduced', 'complete', 'r', 'raw'):
         if mode in ('f', 'full'):
@@ -988,9 +1268,8 @@
 
 @array_function_dispatch(_unary_dispatcher)
 def eigvals(a):
-    """
-    Compute the eigenvalues of a general matrix.
-
+    """Compute the eigenvalues of a general matrix.
+    
     Main difference between `eigvals` and `eig`: the eigenvectors aren't
     returned.
 
@@ -1019,18 +1298,16 @@
     eigh : eigenvalues and eigenvectors of real symmetric or complex
            Hermitian (conjugate symmetric) arrays.
     scipy.linalg.eigvals : Similar function in SciPy.
-
     Notes
     -----
-
+    
     .. versionadded:: 1.8.0
-
+    
     Broadcasting rules apply, see the `numpy.linalg` documentation for
     details.
-
+    
     This is implemented using the ``_geev`` LAPACK routines which compute
     the eigenvalues and eigenvectors of general square arrays.
-
     Examples
     --------
     Illustration, using the fact that the eigenvalues of a diagonal matrix
@@ -1039,15 +1316,15 @@
     of `Q`), preserves the eigenvalues of the "middle" matrix.  In other words,
     if `Q` is orthogonal, then ``Q * A * Q.T`` has the same eigenvalues as
     ``A``:
-
+    
+    
+    Now multiply a diagonal matrix by ``Q`` on one side and by ``Q.T`` on the other:
     >>> from numpy import linalg as LA
     >>> x = np.random.random()
     >>> Q = np.array([[np.cos(x), -np.sin(x)], [np.sin(x), np.cos(x)]])
     >>> LA.norm(Q[0, :]), LA.norm(Q[1, :]), np.dot(Q[0, :],Q[1, :])
     (1.0, 1.0, 0.0)
-
-    Now multiply a diagonal matrix by ``Q`` on one side and by ``Q.T`` on the other:
-
+    
     >>> D = np.diag((-1,1))
     >>> LA.eigvals(D)
     array([-1.,  1.])
@@ -1055,7 +1332,6 @@
     >>> A = np.dot(A, Q.T)
     >>> LA.eigvals(A)
     array([ 1., -1.]) # random
-
     """
     a, wrap = _makearray(a)
     _assert_stacked_2d(a)
@@ -1079,14 +1355,26 @@
 
 
 def _eigvalsh_dispatcher(a, UPLO=None):
+    """
+
+    Parameters
+    ----------
+    a :
+        
+    UPLO :
+         (Default value = None)
+
+    Returns
+    -------
+
+    """
     return (a,)
 
 
 @array_function_dispatch(_eigvalsh_dispatcher)
 def eigvalsh(a, UPLO='L'):
-    """
-    Compute the eigenvalues of a complex Hermitian or real symmetric matrix.
-
+    """Compute the eigenvalues of a complex Hermitian or real symmetric matrix.
+    
     Main difference from eigh: the eigenvectors are not computed.
 
     Parameters
@@ -1121,24 +1409,22 @@
     eig : eigenvalues and right eigenvectors of general real or complex
           arrays.
     scipy.linalg.eigvalsh : Similar function in SciPy.
-
     Notes
     -----
-
+    
     .. versionadded:: 1.8.0
-
+    
     Broadcasting rules apply, see the `numpy.linalg` documentation for
     details.
-
+    
     The eigenvalues are computed using LAPACK routines ``_syevd``, ``_heevd``.
-
     Examples
     --------
     >>> from numpy import linalg as LA
     >>> a = np.array([[1, -2j], [2j, 5]])
     >>> LA.eigvalsh(a)
     array([ 0.17157288,  5.82842712]) # may vary
-
+    
     >>> # demonstrate the treatment of the imaginary part of the diagonal
     >>> a = np.array([[5+2j, 9-2j], [0+2j, 2-1j]])
     >>> a
@@ -1155,7 +1441,6 @@
     >>> wa; wb
     array([1., 6.])
     array([6.+0.j, 1.+0.j])
-
     """
     UPLO = UPLO.upper()
     if UPLO not in ('L', 'U'):
@@ -1177,6 +1462,17 @@
     return w.astype(_realType(result_t), copy=False)
 
 def _convertarray(a):
+    """
+
+    Parameters
+    ----------
+    a :
+        
+
+    Returns
+    -------
+
+    """
     t, result_t = _commonType(a)
     a = _fastCT(a.astype(t))
     return a, t, result_t
@@ -1187,8 +1483,7 @@
 
 @array_function_dispatch(_unary_dispatcher)
 def eig(a):
-    """
-    Compute the eigenvalues and right eigenvectors of a square array.
+    """Compute the eigenvalues and right eigenvectors of a square array.
 
     Parameters
     ----------
@@ -1205,7 +1500,6 @@
         zero in which case it will be cast to a real type. When `a`
         is real the resulting eigenvalues will be real (0 imaginary
         part) or occur in conjugate pairs
-
     v : (..., M, M) array
         The normalized (unit "length") eigenvectors, such that the
         column ``v[:,i]`` is the eigenvector corresponding to the
@@ -1227,82 +1521,82 @@
                        generalized eigenvalue problem.
     scipy.linalg.schur : Best choice for unitary and other non-Hermitian
                          normal matrices.
-
     Notes
     -----
-
+    
     .. versionadded:: 1.8.0
-
+    
     Broadcasting rules apply, see the `numpy.linalg` documentation for
     details.
-
+    
     This is implemented using the ``_geev`` LAPACK routines which compute
     the eigenvalues and eigenvectors of general square arrays.
-
+    
     The number `w` is an eigenvalue of `a` if there exists a vector
     `v` such that ``a @ v = w * v``. Thus, the arrays `a`, `w`, and
     `v` satisfy the equations ``a @ v[:,i] = w[i] * v[:,i]``
     for :math:`i \\in \\{0,...,M-1\\}`.
-
+    
     The array `v` of eigenvectors may not be of maximum rank, that is, some
     of the columns may be linearly dependent, although round-off error may
     obscure that fact. If the eigenvalues are all different, then theoretically
     the eigenvectors are linearly independent and `a` can be diagonalized by
     a similarity transformation using `v`, i.e, ``inv(v) @ a @ v`` is diagonal.
-
+    
     For non-Hermitian normal matrices the SciPy function `scipy.linalg.schur`
     is preferred because the matrix `v` is guaranteed to be unitary, which is
     not the case when using `eig`. The Schur factorization produces an
     upper triangular matrix rather than a diagonal matrix, but for normal
     matrices only the diagonal of the upper triangular matrix is needed, the
     rest is roundoff error.
-
+    
     Finally, it is emphasized that `v` consists of the *right* (as in
     right-hand side) eigenvectors of `a`.  A vector `y` satisfying
     ``y.T @ a = z * y.T`` for some number `z` is called a *left*
     eigenvector of `a`, and, in general, the left and right eigenvectors
     of a matrix are not necessarily the (perhaps conjugate) transposes
     of each other.
-
     References
     ----------
     G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando, FL,
     Academic Press, Inc., 1980, Various pp.
-
     Examples
     --------
+    
+    (Almost) trivial example with real e-values and e-vectors.
+    
+    
+    Real matrix possessing complex e-values and e-vectors; note that the
+    e-values are complex conjugates of each other.
+    
+    
+    Complex-valued matrix with real e-values (but complex-valued e-vectors);
+    note that ``a.conj().T == a``, i.e., `a` is Hermitian.
+    
+    
+    Be careful about round-off error!
     >>> from numpy import linalg as LA
-
-    (Almost) trivial example with real e-values and e-vectors.
-
+    
     >>> w, v = LA.eig(np.diag((1, 2, 3)))
     >>> w; v
     array([1., 2., 3.])
     array([[1., 0., 0.],
            [0., 1., 0.],
            [0., 0., 1.]])
-
-    Real matrix possessing complex e-values and e-vectors; note that the
-    e-values are complex conjugates of each other.
-
+    
     >>> w, v = LA.eig(np.array([[1, -1], [1, 1]]))
     >>> w; v
     array([1.+1.j, 1.-1.j])
     array([[0.70710678+0.j        , 0.70710678-0.j        ],
            [0.        -0.70710678j, 0.        +0.70710678j]])
-
-    Complex-valued matrix with real e-values (but complex-valued e-vectors);
-    note that ``a.conj().T == a``, i.e., `a` is Hermitian.
-
+    
     >>> a = np.array([[1, 1j], [-1j, 1]])
     >>> w, v = LA.eig(a)
     >>> w; v
     array([2.+0.j, 0.+0.j])
     array([[ 0.        +0.70710678j,  0.70710678+0.j        ], # may vary
            [ 0.70710678+0.j        , -0.        +0.70710678j]])
-
-    Be careful about round-off error!
-
+    
     >>> a = np.array([[1 + 1e-9, 0], [0, 1 - 1e-9]])
     >>> # Theor. e-values are 1 +/- 1e-9
     >>> w, v = LA.eig(a)
@@ -1310,7 +1604,6 @@
     array([1., 1.])
     array([[1., 0.],
            [0., 1.]])
-
     """
     a, wrap = _makearray(a)
     _assert_stacked_2d(a)
@@ -1336,10 +1629,9 @@
 
 @array_function_dispatch(_eigvalsh_dispatcher)
 def eigh(a, UPLO='L'):
-    """
-    Return the eigenvalues and eigenvectors of a complex Hermitian
+    """Return the eigenvalues and eigenvectors of a complex Hermitian
     (conjugate symmetric) or a real symmetric matrix.
-
+    
     Returns two objects, a 1-D array containing the eigenvalues of `a`, and
     a 2-D square array or matrix (depending on the input type) of the
     corresponding eigenvectors (in columns).
@@ -1380,28 +1672,25 @@
     eigvals : eigenvalues of non-symmetric arrays.
     scipy.linalg.eigh : Similar function in SciPy (but also solves the
                         generalized eigenvalue problem).
-
     Notes
     -----
-
+    
     .. versionadded:: 1.8.0
-
+    
     Broadcasting rules apply, see the `numpy.linalg` documentation for
     details.
-
+    
     The eigenvalues/eigenvectors are computed using LAPACK routines ``_syevd``,
     ``_heevd``.
-
+    
     The eigenvalues of real symmetric or complex Hermitian matrices are
     always real. [1]_ The array `v` of (column) eigenvectors is unitary
     and `a`, `w`, and `v` satisfy the equations
     ``dot(a, v[:, i]) = w[i] * v[:, i]``.
-
     References
     ----------
     .. [1] G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando,
            FL, Academic Press, Inc., 1980, pg. 222.
-
     Examples
     --------
     >>> from numpy import linalg as LA
@@ -1414,12 +1703,12 @@
     array([0.17157288, 5.82842712])
     array([[-0.92387953+0.j        , -0.38268343+0.j        ], # may vary
            [ 0.        +0.38268343j,  0.        -0.92387953j]])
-
+    
     >>> np.dot(a, v[:, 0]) - w[0] * v[:, 0] # verify 1st e-val/vec pair
     array([5.55111512e-17+0.0000000e+00j, 0.00000000e+00+1.2490009e-16j])
     >>> np.dot(a, v[:, 1]) - w[1] * v[:, 1] # verify 2nd e-val/vec pair
     array([0.+0.j, 0.+0.j])
-
+    
     >>> A = np.matrix(a) # what happens if input is a matrix object
     >>> A
     matrix([[ 1.+0.j, -0.-2.j],
@@ -1429,7 +1718,7 @@
     array([0.17157288, 5.82842712])
     matrix([[-0.92387953+0.j        , -0.38268343+0.j        ], # may vary
             [ 0.        +0.38268343j,  0.        -0.92387953j]])
-
+    
     >>> # demonstrate the treatment of the imaginary part of the diagonal
     >>> a = np.array([[5+2j, 9-2j], [0+2j, 2-1j]])
     >>> a
@@ -1477,14 +1766,30 @@
 # Singular value decomposition
 
 def _svd_dispatcher(a, full_matrices=None, compute_uv=None, hermitian=None):
+    """
+
+    Parameters
+    ----------
+    a :
+        
+    full_matrices :
+         (Default value = None)
+    compute_uv :
+         (Default value = None)
+    hermitian :
+         (Default value = None)
+
+    Returns
+    -------
+
+    """
     return (a,)
 
 
 @array_function_dispatch(_svd_dispatcher)
 def svd(a, full_matrices=True, compute_uv=True, hermitian=False):
-    """
-    Singular Value Decomposition.
-
+    """Singular Value Decomposition.
+    
     When `a` is a 2D array, it is factorized as ``u @ np.diag(s) @ vh
     = (u * s) @ vh``, where `u` and `vh` are 2D unitary arrays and `s` is a 1D
     array of `a`'s singular values. When `a` is higher-dimensional, SVD is
@@ -1506,7 +1811,6 @@
         If True, `a` is assumed to be Hermitian (symmetric if real-valued),
         enabling a more efficient method for finding singular values.
         Defaults to False.
-
         .. versionadded:: 1.17.0
 
     Returns
@@ -1535,16 +1839,15 @@
     --------
     scipy.linalg.svd : Similar function in SciPy.
     scipy.linalg.svdvals : Compute singular values of a matrix.
-
     Notes
     -----
-
+    
     .. versionchanged:: 1.8.0
        Broadcasting rules apply, see the `numpy.linalg` documentation for
        details.
-
+    
     The decomposition is performed using LAPACK routine ``_gesdd``.
-
+    
     SVD is usually described for the factorization of a 2D matrix :math:`A`.
     The higher-dimensional case will be discussed below. In the 2D case, SVD is
     written as :math:`A = U S V^H`, where :math:`A = a`, :math:`U= u`,
@@ -1553,7 +1856,7 @@
     of `vh` are the eigenvectors of :math:`A^H A` and the columns of `u` are
     the eigenvectors of :math:`A A^H`. In both cases the corresponding
     (possibly non-zero) eigenvalues are given by ``s**2``.
-
+    
     If `a` has more than two dimensions, then broadcasting rules apply, as
     explained in :ref:`routines.linalg-broadcasting`. This means that SVD is
     working in "stacked" mode: it iterates over all indices of the first
@@ -1562,17 +1865,25 @@
     decomposition with either ``(u * s[..., None, :]) @ vh`` or
     ``u @ (s[..., None] * vh)``. (The ``@`` operator can be replaced by the
     function ``np.matmul`` for python versions below 3.5.)
-
+    
     If `a` is a ``matrix`` object (as opposed to an ``ndarray``), then so are
     all the return values.
-
     Examples
     --------
+    
+    Reconstruction based on full SVD, 2D case:
+    
+    
+    Reconstruction based on reduced SVD, 2D case:
+    
+    
+    Reconstruction based on full SVD, 4D case:
+    
+    
+    Reconstruction based on reduced SVD, 4D case:
     >>> a = np.random.randn(9, 6) + 1j*np.random.randn(9, 6)
     >>> b = np.random.randn(2, 7, 8, 3) + 1j*np.random.randn(2, 7, 8, 3)
-
-    Reconstruction based on full SVD, 2D case:
-
+    
     >>> u, s, vh = np.linalg.svd(a, full_matrices=True)
     >>> u.shape, s.shape, vh.shape
     ((9, 9), (6,), (6, 6))
@@ -1582,9 +1893,7 @@
     >>> smat[:6, :6] = np.diag(s)
     >>> np.allclose(a, np.dot(u, np.dot(smat, vh)))
     True
-
-    Reconstruction based on reduced SVD, 2D case:
-
+    
     >>> u, s, vh = np.linalg.svd(a, full_matrices=False)
     >>> u.shape, s.shape, vh.shape
     ((9, 6), (6,), (6, 6))
@@ -1593,9 +1902,7 @@
     >>> smat = np.diag(s)
     >>> np.allclose(a, np.dot(u, np.dot(smat, vh)))
     True
-
-    Reconstruction based on full SVD, 4D case:
-
+    
     >>> u, s, vh = np.linalg.svd(b, full_matrices=True)
     >>> u.shape, s.shape, vh.shape
     ((2, 7, 8, 8), (2, 7, 3), (2, 7, 3, 3))
@@ -1603,9 +1910,7 @@
     True
     >>> np.allclose(b, np.matmul(u[..., :3], s[..., None] * vh))
     True
-
-    Reconstruction based on reduced SVD, 4D case:
-
+    
     >>> u, s, vh = np.linalg.svd(b, full_matrices=False)
     >>> u.shape, s.shape, vh.shape
     ((2, 7, 8, 3), (2, 7, 3), (2, 7, 3, 3))
@@ -1613,7 +1918,6 @@
     True
     >>> np.allclose(b, np.matmul(u, s[..., None] * vh))
     True
-
     """
     import numpy as _nx
     a, wrap = _makearray(a)
@@ -1676,14 +1980,26 @@
 
 
 def _cond_dispatcher(x, p=None):
+    """
+
+    Parameters
+    ----------
+    x :
+        
+    p :
+         (Default value = None)
+
+    Returns
+    -------
+
+    """
     return (x,)
 
 
 @array_function_dispatch(_cond_dispatcher)
 def cond(x, p=None):
-    """
-    Compute the condition number of a matrix.
-
+    """Compute the condition number of a matrix.
+    
     This function is capable of returning the condition number using
     one of seven different norms, depending on the value of `p` (see
     Parameters below).
@@ -1694,7 +2010,6 @@
         The matrix whose condition number is sought.
     p : {None, 1, -1, 2, -2, inf, -inf, 'fro'}, optional
         Order of the norm:
-
         =====  ============================
         p      norm for matrices
         =====  ============================
@@ -1707,9 +2022,8 @@
         2      2-norm (largest sing. value)
         -2     smallest singular value
         =====  ============================
-
         inf means the numpy.inf object, and the Frobenius norm is
-        the root-of-sum-of-squares norm.
+        the root-of-sum-of-squares norm. (Default value = None)
 
     Returns
     -------
@@ -1719,18 +2033,15 @@
     See Also
     --------
     numpy.linalg.norm
-
     Notes
     -----
     The condition number of `x` is defined as the norm of `x` times the
     norm of the inverse of `x` [1]_; the norm can be the usual L2-norm
     (root-of-sum-of-squares) or one of a number of other matrix norms.
-
     References
     ----------
     .. [1] G. Strang, *Linear Algebra and Its Applications*, Orlando, FL,
            Academic Press, Inc., 1980, pg. 285.
-
     Examples
     --------
     >>> from numpy import linalg as LA
@@ -1757,7 +2068,6 @@
     0.70710678118654746 # may vary
     >>> min(LA.svd(a, compute_uv=False))*min(LA.svd(LA.inv(a), compute_uv=False))
     0.70710678118654746 # may vary
-
     """
     x = asarray(x)  # in case we have a matrix
     if _is_empty_2d(x):
@@ -1799,17 +2109,31 @@
 
 
 def _matrix_rank_dispatcher(M, tol=None, hermitian=None):
+    """
+
+    Parameters
+    ----------
+    M :
+        
+    tol :
+         (Default value = None)
+    hermitian :
+         (Default value = None)
+
+    Returns
+    -------
+
+    """
     return (M,)
 
 
 @array_function_dispatch(_matrix_rank_dispatcher)
 def matrix_rank(M, tol=None, hermitian=False):
-    """
-    Return matrix rank of array using SVD method
-
+    """Return matrix rank of array using SVD method
+    
     Rank of the array is the number of singular values of the array that are
     greater than `tol`.
-
+    
     .. versionchanged:: 1.14
        Can now operate on stacks of matrices
 
@@ -1822,14 +2146,12 @@
         None, and ``S`` is an array with singular values for `M`, and
         ``eps`` is the epsilon value for datatype of ``S``, then `tol` is
         set to ``S.max() * max(M.shape) * eps``.
-
         .. versionchanged:: 1.14
-           Broadcasted against the stack of matrices
+        Broadcasted against the stack of matrices (Default value = None)
     hermitian : bool, optional
         If True, `M` is assumed to be Hermitian (symmetric if real-valued),
         enabling a more efficient method for finding singular values.
         Defaults to False.
-
         .. versionadded:: 1.14
 
     Returns
@@ -1845,7 +2167,7 @@
     the symbols defined above). This is the algorithm MATLAB uses [1].  It also
     appears in *Numerical recipes* in the discussion of SVD solutions for linear
     least squares [2].
-
+    
     This default threshold is designed to detect rank deficiency accounting for
     the numerical errors of the SVD computation.  Imagine that there is a column
     in `M` that is an exact (in floating point) linear combination of other
@@ -1858,13 +2180,13 @@
     the linear combination of some columns of `M` is not exactly equal to
     another column of `M` but only numerically very close to another column of
     `M`.
-
+    
     We chose our default threshold because it is in wide use.  Other thresholds
     are possible.  For example, elsewhere in the 2007 edition of *Numerical
     recipes* there is an alternative threshold of ``S.max() *
     np.finfo(M.dtype).eps / 2. * np.sqrt(m + n + 1.)``. The authors describe
     this threshold as being based on "expected roundoff error" (p 71).
-
+    
     The thresholds above deal with floating point roundoff error in the
     calculation of the SVD.  However, you may have more information about the
     sources of error in `M` that would make you consider other tolerance values
@@ -1874,7 +2196,6 @@
     greater than floating point epsilon, choosing a tolerance near that
     uncertainty may be preferable.  The tolerance may be absolute if the
     uncertainties are absolute rather than relative.
-
     References
     ----------
     .. [1] MATLAB reference documention, "Rank"
@@ -1882,7 +2203,6 @@
     .. [2] W. H. Press, S. A. Teukolsky, W. T. Vetterling and B. P. Flannery,
            "Numerical Recipes (3rd edition)", Cambridge University Press, 2007,
            page 795.
-
     Examples
     --------
     >>> from numpy.linalg import matrix_rank
@@ -1910,18 +2230,32 @@
 # Generalized inverse
 
 def _pinv_dispatcher(a, rcond=None, hermitian=None):
+    """
+
+    Parameters
+    ----------
+    a :
+        
+    rcond :
+         (Default value = None)
+    hermitian :
+         (Default value = None)
+
+    Returns
+    -------
+
+    """
     return (a,)
 
 
 @array_function_dispatch(_pinv_dispatcher)
 def pinv(a, rcond=1e-15, hermitian=False):
-    """
-    Compute the (Moore-Penrose) pseudo-inverse of a matrix.
-
+    """Compute the (Moore-Penrose) pseudo-inverse of a matrix.
+    
     Calculate the generalized inverse of a matrix using its
     singular-value decomposition (SVD) and including all
     *large* singular values.
-
+    
     .. versionchanged:: 1.14
        Can now operate on stacks of matrices
 
@@ -1933,12 +2267,11 @@
         Cutoff for small singular values.
         Singular values less than or equal to
         ``rcond * largest_singular_value`` are set to zero.
-        Broadcasts against the stack of matrices.
+        Broadcasts against the stack of matrices. (Default value = 1e-15)
     hermitian : bool, optional
         If True, `a` is assumed to be Hermitian (symmetric if real-valued),
         enabling a more efficient method for finding singular values.
         Defaults to False.
-
         .. versionadded:: 1.17.0
 
     Returns
@@ -1958,14 +2291,13 @@
     scipy.linalg.pinv2 : Similar function in SciPy (SVD-based).
     scipy.linalg.pinvh : Compute the (Moore-Penrose) pseudo-inverse of a
                          Hermitian matrix.
-
     Notes
     -----
     The pseudo-inverse of a matrix A, denoted :math:`A^+`, is
     defined as: "the matrix that 'solves' [the least-squares problem]
     :math:`Ax = b`," i.e., if :math:`\\bar{x}` is said solution, then
     :math:`A^+` is that matrix such that :math:`\\bar{x} = A^+b`.
-
+    
     It can be shown that if :math:`Q_1 \\Sigma Q_2^T = A` is the singular
     value decomposition of A, then
     :math:`A^+ = Q_2 \\Sigma^+ Q_1^T`, where :math:`Q_{1,2}` are
@@ -1974,24 +2306,20 @@
     zeros), and then :math:`\\Sigma^+` is simply the diagonal matrix
     consisting of the reciprocals of A's singular values
     (again, followed by zeros). [1]_
-
     References
     ----------
     .. [1] G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando,
            FL, Academic Press, Inc., 1980, pp. 139-142.
-
     Examples
     --------
     The following example checks that ``a * a+ * a == a`` and
     ``a+ * a * a+ == a+``:
-
     >>> a = np.random.randn(9, 6)
     >>> B = np.linalg.pinv(a)
     >>> np.allclose(a, np.dot(a, np.dot(B, a)))
     True
     >>> np.allclose(B, np.dot(B, np.dot(a, B)))
     True
-
     """
     a, wrap = _makearray(a)
     rcond = asarray(rcond)
@@ -2017,9 +2345,8 @@
 
 @array_function_dispatch(_unary_dispatcher)
 def slogdet(a):
-    """
-    Compute the sign and (natural) logarithm of the determinant of an array.
-
+    """Compute the sign and (natural) logarithm of the determinant of an array.
+    
     If an array has a very small or very large determinant, then a call to
     `det` may overflow or underflow. This routine is more robust against such
     issues, because it computes the logarithm of the determinant rather than
@@ -2038,41 +2365,43 @@
         with absolute value 1 (i.e., it is on the unit circle), or else 0.
     logdet : (...) array_like
         The natural log of the absolute value of the determinant.
-
     If the determinant is zero, then `sign` will be 0 and `logdet` will be
+        
     -Inf. In all cases, the determinant is equal to ``sign * np.exp(logdet)``.
+        
 
     See Also
     --------
     det
-
     Notes
     -----
-
+    
     .. versionadded:: 1.8.0
-
+    
     Broadcasting rules apply, see the `numpy.linalg` documentation for
     details.
-
+    
     .. versionadded:: 1.6.0
-
+    
     The determinant is computed via LU factorization using the LAPACK
     routine ``z/dgetrf``.
-
-
+    
     Examples
     --------
     The determinant of a 2-D array ``[[a, b], [c, d]]`` is ``ad - bc``:
-
+    
+    
+    Computing log-determinants for a stack of matrices:
+    
+    
+    This routine succeeds where ordinary `det` does not:
     >>> a = np.array([[1, 2], [3, 4]])
     >>> (sign, logdet) = np.linalg.slogdet(a)
     >>> (sign, logdet)
     (-1, 0.69314718055994529) # may vary
     >>> sign * np.exp(logdet)
     -2.0
-
-    Computing log-determinants for a stack of matrices:
-
+    
     >>> a = np.array([ [[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]] ])
     >>> a.shape
     (3, 2, 2)
@@ -2081,14 +2410,11 @@
     (array([-1., -1., -1.]), array([ 0.69314718,  1.09861229,  2.07944154]))
     >>> sign * np.exp(logdet)
     array([-2., -3., -8.])
-
-    This routine succeeds where ordinary `det` does not:
-
+    
     >>> np.linalg.det(np.eye(500) * 0.1)
     0.0
     >>> np.linalg.slogdet(np.eye(500) * 0.1)
     (1, -1151.2925464970228)
-
     """
     a = asarray(a)
     _assert_stacked_2d(a)
@@ -2104,8 +2430,7 @@
 
 @array_function_dispatch(_unary_dispatcher)
 def det(a):
-    """
-    Compute the determinant of an array.
+    """Compute the determinant of an array.
 
     Parameters
     ----------
@@ -2122,34 +2447,31 @@
     slogdet : Another way to represent the determinant, more suitable
       for large matrices where underflow/overflow may occur.
     scipy.linalg.det : Similar function in SciPy.
-
     Notes
     -----
-
+    
     .. versionadded:: 1.8.0
-
+    
     Broadcasting rules apply, see the `numpy.linalg` documentation for
     details.
-
+    
     The determinant is computed via LU factorization using the LAPACK
     routine ``z/dgetrf``.
-
     Examples
     --------
     The determinant of a 2-D array [[a, b], [c, d]] is ad - bc:
-
+    
+    
+    Computing determinants for a stack of matrices:
     >>> a = np.array([[1, 2], [3, 4]])
     >>> np.linalg.det(a)
     -2.0 # may vary
-
-    Computing determinants for a stack of matrices:
-
+    
     >>> a = np.array([ [[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]] ])
     >>> a.shape
     (3, 2, 2)
     >>> np.linalg.det(a)
     array([-2., -3., -8.])
-
     """
     a = asarray(a)
     _assert_stacked_2d(a)
@@ -2164,14 +2486,29 @@
 # Linear Least Squares
 
 def _lstsq_dispatcher(a, b, rcond=None):
+    """
+
+    Parameters
+    ----------
+    a :
+        
+    b :
+        
+    rcond :
+         (Default value = None)
+
+    Returns
+    -------
+
+    """
     return (a, b)
 
 
 @array_function_dispatch(_lstsq_dispatcher)
 def lstsq(a, b, rcond="warn"):
-    r"""
+    """r"""
     Return the least-squares solution to a linear matrix equation.
-
+    
     Computes the vector x that approximatively solves the equation
     ``a @ x = b``. The equation may be under-, well-, or over-determined
     (i.e., the number of linearly independent rows of `a` can be less than,
@@ -2193,13 +2530,12 @@
         For the purposes of rank determination, singular values are treated
         as zero if they are smaller than `rcond` times the largest singular
         value of `a`.
-
         .. versionchanged:: 1.14.0
-           If not set, a FutureWarning is given. The previous default
-           of ``-1`` will use the machine precision as `rcond` parameter,
-           the new default will use the machine precision times `max(M, N)`.
-           To silence the warning and use the new default, use ``rcond=None``,
-           to keep using the old behavior, use ``rcond=-1``.
+        If not set, a FutureWarning is given. The previous default
+        of ``-1`` will use the machine precision as `rcond` parameter,
+        the new default will use the machine precision times `max(M, N)`.
+        To silence the warning and use the new default, use ``rcond=None``,
+        to keep using the old behavior, use ``rcond=-1``.
 
     Returns
     -------
@@ -2225,44 +2561,42 @@
     See Also
     --------
     scipy.linalg.lstsq : Similar function in SciPy.
-
     Notes
     -----
     If `b` is a matrix, then all array results are returned as matrices.
-
     Examples
     --------
     Fit a line, ``y = mx + c``, through some noisy data-points:
-
+    
+    
+    By examining the coefficients, we see that the line should have a
+    gradient of roughly 1 and cut the y-axis at, more or less, -1.
+    
+    We can rewrite the line equation as ``y = Ap``, where ``A = [[x 1]]``
+    and ``p = [[m], [c]]``.  Now use `lstsq` to solve for `p`:
+    
+    
+    
+    Plot the data along with the fitted line:
     >>> x = np.array([0, 1, 2, 3])
     >>> y = np.array([-1, 0.2, 0.9, 2.1])
-
-    By examining the coefficients, we see that the line should have a
-    gradient of roughly 1 and cut the y-axis at, more or less, -1.
-
-    We can rewrite the line equation as ``y = Ap``, where ``A = [[x 1]]``
-    and ``p = [[m], [c]]``.  Now use `lstsq` to solve for `p`:
-
+    
     >>> A = np.vstack([x, np.ones(len(x))]).T
     >>> A
     array([[ 0.,  1.],
            [ 1.,  1.],
            [ 2.,  1.],
            [ 3.,  1.]])
-
+    
     >>> m, c = np.linalg.lstsq(A, y, rcond=None)[0]
     >>> m, c
     (1.0 -0.95) # may vary
-
-    Plot the data along with the fitted line:
-
+    
     >>> import matplotlib.pyplot as plt
     >>> _ = plt.plot(x, y, 'o', label='Original data', markersize=10)
     >>> _ = plt.plot(x, m*x + c, 'r', label='Fitted line')
     >>> _ = plt.legend()
     >>> plt.show()
-
-    """
     a, _ = _makearray(a)
     b, wrap = _makearray(b)
     is_1d = b.ndim == 1
@@ -2330,26 +2664,26 @@
 
 def _multi_svd_norm(x, row_axis, col_axis, op):
     """Compute a function of the singular values of the 2-D matrices in `x`.
-
+    
     This is a private utility function used by `numpy.linalg.norm()`.
 
     Parameters
     ----------
     x : ndarray
+        
     row_axis, col_axis : int
         The axes of `x` that hold the 2-D matrices.
     op : callable
         This should be either numpy.amin or `numpy.amax` or `numpy.sum`.
-
-    Returns
-    -------
-    result : float or ndarray
-        If `x` is 2-D, the return values is a float.
-        Otherwise, it is an array with ``x.ndim - 2`` dimensions.
-        The return values are either the minimum or maximum or sum of the
-        singular values of the matrices, depending on whether `op`
-        is `numpy.amin` or `numpy.amax` or `numpy.sum`.
-
+    row_axis :
+        
+    col_axis :
+        
+
+    Returns
+    -------
+
+    
     """
     y = moveaxis(x, (row_axis, col_axis), (-2, -1))
     result = op(svd(y, compute_uv=False), axis=-1)
@@ -2357,14 +2691,30 @@
 
 
 def _norm_dispatcher(x, ord=None, axis=None, keepdims=None):
+    """
+
+    Parameters
+    ----------
+    x :
+        
+    ord :
+         (Default value = None)
+    axis :
+         (Default value = None)
+    keepdims :
+         (Default value = None)
+
+    Returns
+    -------
+
+    """
     return (x,)
 
 
 @array_function_dispatch(_norm_dispatcher)
 def norm(x, ord=None, axis=None, keepdims=False):
-    """
-    Matrix or vector norm.
-
+    """Matrix or vector norm.
+    
     This function is able to return one of eight different matrix norms,
     or one of an infinite number of vector norms (described below), depending
     on the value of the ``ord`` parameter.
@@ -2385,15 +2735,12 @@
         are computed.  If `axis` is None then either a vector norm (when `x`
         is 1-D) or a matrix norm (when `x` is 2-D) is returned. The default
         is None.
-
         .. versionadded:: 1.8.0
-
     keepdims : bool, optional
         If this is set to True, the axes which are normed over are left in the
         result as dimensions with size one.  With this option the result will
         broadcast correctly against the original `x`.
-
-        .. versionadded:: 1.10.0
+        .. versionadded:: 1.10.0 (Default value = False)
 
     Returns
     -------
@@ -2403,15 +2750,14 @@
     See Also
     --------
     scipy.linalg.norm : Similar function in SciPy.
-
     Notes
     -----
     For values of ``ord < 1``, the result is, strictly speaking, not a
     mathematical 'norm', but it may still be useful for various numerical
     purposes.
-
+    
     The following norms can be calculated:
-
+    
     =====  ============================  ==========================
     ord    norm for matrices             norm for vectors
     =====  ============================  ==========================
@@ -2427,23 +2773,29 @@
     -2     smallest singular value       as below
     other  --                            sum(abs(x)**ord)**(1./ord)
     =====  ============================  ==========================
-
+    
     The Frobenius norm is given by [1]_:
-
+    
         :math:`||A||_F = [\\sum_{i,j} abs(a_{i,j})^2]^{1/2}`
-
+    
     The nuclear norm is the sum of the singular values.
-
+    
     Both the Frobenius and nuclear norm orders are only defined for
     matrices and raise a ValueError when ``x.ndim != 2``.
-
     References
     ----------
     .. [1] G. H. Golub and C. F. Van Loan, *Matrix Computations*,
            Baltimore, MD, Johns Hopkins University Press, 1985, pg. 15
-
     Examples
     --------
+    
+    
+    
+    
+    Using the `axis` argument to compute vector norms:
+    
+    
+    Using the `axis` argument to compute matrix norms:
     >>> from numpy import linalg as LA
     >>> a = np.arange(9) - 4
     >>> a
@@ -2453,7 +2805,7 @@
     array([[-4, -3, -2],
            [-1,  0,  1],
            [ 2,  3,  4]])
-
+    
     >>> LA.norm(a)
     7.745966692414834
     >>> LA.norm(b)
@@ -2468,7 +2820,7 @@
     0.0
     >>> LA.norm(b, -np.inf)
     2.0
-
+    
     >>> LA.norm(a, 1)
     20.0
     >>> LA.norm(b, 1)
@@ -2481,7 +2833,7 @@
     7.745966692414834
     >>> LA.norm(b, 2)
     7.3484692283495345
-
+    
     >>> LA.norm(a, -2)
     0.0
     >>> LA.norm(b, -2)
@@ -2490,9 +2842,7 @@
     5.8480354764257312 # may vary
     >>> LA.norm(a, -3)
     0.0
-
-    Using the `axis` argument to compute vector norms:
-
+    
     >>> c = np.array([[ 1, 2, 3],
     ...               [-1, 1, 4]])
     >>> LA.norm(c, axis=0)
@@ -2501,15 +2851,12 @@
     array([ 3.74165739,  4.24264069])
     >>> LA.norm(c, ord=1, axis=1)
     array([ 6.,  6.])
-
-    Using the `axis` argument to compute matrix norms:
-
+    
     >>> m = np.arange(8).reshape(2,2,2)
     >>> LA.norm(m, axis=(1,2))
     array([  3.74165739,  11.22497216])
     >>> LA.norm(m[0, :, :]), LA.norm(m[1, :, :])
     (3.7416573867739413, 11.224972160321824)
-
     """
     x = asarray(x)
 
@@ -2614,28 +2961,41 @@
 # multi_dot
 
 def _multidot_dispatcher(arrays, *, out=None):
+    """
+
+    Parameters
+    ----------
+    arrays :
+        
+    * :
+        
+    out :
+         (Default value = None)
+
+    Returns
+    -------
+
+    """
     yield from arrays
     yield out
 
 
 @array_function_dispatch(_multidot_dispatcher)
 def multi_dot(arrays, *, out=None):
-    """
-    Compute the dot product of two or more arrays in a single function call,
+    """Compute the dot product of two or more arrays in a single function call,
     while automatically selecting the fastest evaluation order.
-
+    
     `multi_dot` chains `numpy.dot` and uses optimal parenthesization
     of the matrices [1]_ [2]_. Depending on the shapes of the matrices,
     this can speed up the multiplication a lot.
-
+    
     If the first argument is 1-D it is treated as a row vector.
     If the last argument is 1-D it is treated as a column vector.
     The other arguments must be 2-D.
-
+    
     Think of `multi_dot` as::
-
+    
         def multi_dot(arrays): return functools.reduce(np.dot, arrays)
-
 
     Parameters
     ----------
@@ -2650,8 +3010,9 @@
         for `dot(a, b)`. This is a performance feature. Therefore, if these
         conditions are not met, an exception is raised, instead of attempting
         to be flexible.
-
-        .. versionadded:: 1.19.0
+        .. versionadded:: 1.19.0 (Default value = None)
+    * :
+        
 
     Returns
     -------
@@ -2661,17 +3022,33 @@
     See Also
     --------
     dot : dot multiplication with two arguments.
-
     References
     ----------
-
+    
     .. [1] Cormen, "Introduction to Algorithms", Chapter 15.2, p. 370-378
     .. [2] https://en.wikipedia.org/wiki/Matrix_chain_multiplication
-
     Examples
     --------
     `multi_dot` allows you to write::
-
+    
+    
+    instead of::
+    
+    Notes
+    -----
+    The cost for a matrix multiplication can be calculated with the
+    following function::
+    
+        def cost(A, B):
+            return A.shape[0] * A.shape[1] * B.shape[1]
+    
+    Assume we have three matrices
+    :math:`A_{10x100}, B_{100x5}, C_{5x50}`.
+    
+    The costs for the two different parenthesizations are as follows::
+    
+        cost((AB)C) = 10*100*5 + 10*5*50   = 5000 + 2500   = 7500
+        cost(A(BC)) = 10*100*50 + 100*5*50 = 50000 + 25000 = 75000
     >>> from numpy.linalg import multi_dot
     >>> # Prepare some data
     >>> A = np.random.random((10000, 100))
@@ -2680,29 +3057,10 @@
     >>> D = np.random.random((5, 333))
     >>> # the actual dot multiplication
     >>> _ = multi_dot([A, B, C, D])
-
-    instead of::
-
+    
     >>> _ = np.dot(np.dot(np.dot(A, B), C), D)
     >>> # or
     >>> _ = A.dot(B).dot(C).dot(D)
-
-    Notes
-    -----
-    The cost for a matrix multiplication can be calculated with the
-    following function::
-
-        def cost(A, B):
-            return A.shape[0] * A.shape[1] * B.shape[1]
-
-    Assume we have three matrices
-    :math:`A_{10x100}, B_{100x5}, C_{5x50}`.
-
-    The costs for the two different parenthesizations are as follows::
-
-        cost((AB)C) = 10*100*5 + 10*5*50   = 5000 + 2500   = 7500
-        cost(A(BC)) = 10*100*50 + 100*5*50 = 50000 + 25000 = 75000
-
     """
     n = len(arrays)
     # optimization only makes sense for len(arrays) > 2
@@ -2740,11 +3098,24 @@
 
 
 def _multi_dot_three(A, B, C, out=None):
-    """
-    Find the best order for three arrays and do the multiplication.
-
+    """Find the best order for three arrays and do the multiplication.
+    
     For three arguments `_multi_dot_three` is approximately 15 times faster
     than `_multi_dot_matrix_chain_order`
+
+    Parameters
+    ----------
+    A :
+        
+    B :
+        
+    C :
+        
+    out :
+         (Default value = None)
+
+    Returns
+    -------
 
     """
     a0, a1b0 = A.shape
@@ -2762,19 +3133,28 @@
 
 def _multi_dot_matrix_chain_order(arrays, return_costs=False):
     """
-    Return a np.array that encodes the optimal order of mutiplications.
-
-    The optimal order array is then used by `_multi_dot()` to do the
-    multiplication.
-
-    Also return the cost matrix if `return_costs` is `True`
-
-    The implementation CLOSELY follows Cormen, "Introduction to Algorithms",
-    Chapter 15.2, p. 370-378.  Note that Cormen uses 1-based indices.
-
+
+    Parameters
+    ----------
+    arrays :
+        
+    return_costs :
+         (Default value = False)
+
+    Returns
+    -------
+    type
+        The optimal order array is then used by `_multi_dot()` to do the
+        multiplication.
+        
+        Also return the cost matrix if `return_costs` is `True`
+        
+        The implementation CLOSELY follows Cormen, "Introduction to Algorithms",
+        Chapter 15.2, p. 370-378.  Note that Cormen uses 1-based indices.
+        
         cost[i, j] = min([
-            cost[prefix] + cost[suffix] + cost_mult(prefix, suffix)
-            for k in range(i, j)])
+        cost[prefix] + cost[suffix] + cost_mult(prefix, suffix)
+        for k in range(i, j)])
 
     """
     n = len(arrays)
@@ -2802,7 +3182,25 @@
 
 
 def _multi_dot(arrays, order, i, j, out=None):
-    """Actually do the multiplication with the given order."""
+    """Actually do the multiplication with the given order.
+
+    Parameters
+    ----------
+    arrays :
+        
+    order :
+        
+    i :
+        
+    j :
+        
+    out :
+         (Default value = None)
+
+    Returns
+    -------
+
+    """
     if i == j:
         # the initial call with non-None out should never get here
         assert out is None
