# Patch generated by Pyment v0.3.3

--- a/..//venv/lib/python3.8/site-packages/parso/python/tokenize.py
+++ b/..//venv/lib/python3.8/site-packages/parso/python/tokenize.py
@@ -58,10 +58,34 @@
     # This just means that Python 2 code matches a few identifiers too much,
     # but that doesn't really matter.
     def is_identifier(s):
+        """
+
+        Parameters
+        ----------
+        s :
+            
+
+        Returns
+        -------
+
+        """
         return True
 
 
 def group(*choices, **kwargs):
+    """
+
+    Parameters
+    ----------
+    *choices :
+        
+    **kwargs :
+        
+
+    Returns
+    -------
+
+    """
     capture = kwargs.pop('capture', False)  # Python 2, arrghhhhh :(
     assert not kwargs
 
@@ -72,12 +96,49 @@
 
 
 def maybe(*choices):
+    """
+
+    Parameters
+    ----------
+    *choices :
+        
+
+    Returns
+    -------
+
+    """
     return group(*choices) + '?'
 
 
 # Return the empty string, plus all of the valid string prefixes.
 def _all_string_prefixes(version_info, include_fstring=False, only_fstring=False):
+    """
+
+    Parameters
+    ----------
+    version_info :
+        
+    include_fstring :
+         (Default value = False)
+    only_fstring :
+         (Default value = False)
+
+    Returns
+    -------
+
+    """
     def different_case_versions(prefix):
+        """
+
+        Parameters
+        ----------
+        prefix :
+            
+
+        Returns
+        -------
+
+        """
         for s in _itertools.product(*[(c, c.upper()) for c in prefix]):
             yield ''.join(s)
     # The valid string prefixes. Only contain the lower case versions,
@@ -112,10 +173,32 @@
 
 
 def _compile(expr):
+    """
+
+    Parameters
+    ----------
+    expr :
+        
+
+    Returns
+    -------
+
+    """
     return re.compile(expr, re.UNICODE)
 
 
 def _get_token_collection(version_info):
+    """
+
+    Parameters
+    ----------
+    version_info :
+        
+
+    Returns
+    -------
+
+    """
     try:
         return _token_collection_cache[tuple(version_info)]
     except KeyError:
@@ -131,6 +214,17 @@
 
 
 def _create_token_collection(version_info):
+    """
+
+    Parameters
+    ----------
+    version_info :
+        
+
+    Returns
+    -------
+
+    """
     # Note: we use unicode matching for names ("\w") but ascii matching for
     # number literals.
     Whitespace = r'[ \f\t]*'
@@ -269,8 +363,10 @@
 
 
 class Token(namedtuple('Token', ['type', 'string', 'start_pos', 'prefix'])):
+    """ """
     @property
     def end_pos(self):
+        """ """
         lines = split_lines(self.string)
         if len(lines) > 1:
             return self.start_pos[0] + len(lines) - 1, 0
@@ -279,12 +375,14 @@
 
 
 class PythonToken(Token):
+    """ """
     def __repr__(self):
         return ('TokenInfo(type=%s, string=%r, start_pos=%r, prefix=%r)' %
                 self._replace(type=self.type.name))
 
 
 class FStringNode(object):
+    """ """
     def __init__(self, quote):
         self.quote = quote
         self.parentheses_count = 0
@@ -295,25 +393,69 @@
         self.format_spec_count = 0
 
     def open_parentheses(self, character):
+        """
+
+        Parameters
+        ----------
+        character :
+            
+
+        Returns
+        -------
+
+        """
         self.parentheses_count += 1
 
     def close_parentheses(self, character):
+        """
+
+        Parameters
+        ----------
+        character :
+            
+
+        Returns
+        -------
+
+        """
         self.parentheses_count -= 1
         if self.parentheses_count == 0:
             # No parentheses means that the format spec is also finished.
             self.format_spec_count = 0
 
     def allow_multiline(self):
+        """ """
         return len(self.quote) == 3
 
     def is_in_expr(self):
+        """ """
         return self.parentheses_count > self.format_spec_count
 
     def is_in_format_spec(self):
+        """ """
         return not self.is_in_expr() and self.format_spec_count
 
 
 def _close_fstring_if_necessary(fstring_stack, string, line_nr, column, additional_prefix):
+    """
+
+    Parameters
+    ----------
+    fstring_stack :
+        
+    string :
+        
+    line_nr :
+        
+    column :
+        
+    additional_prefix :
+        
+
+    Returns
+    -------
+
+    """
     for fstring_stack_index, node in enumerate(fstring_stack):
         lstripped_string = string.lstrip()
         len_lstrip = len(string) - len(lstripped_string)
@@ -332,6 +474,25 @@
 
 
 def _find_fstring_string(endpats, fstring_stack, line, lnum, pos):
+    """
+
+    Parameters
+    ----------
+    endpats :
+        
+    fstring_stack :
+        
+    line :
+        
+    lnum :
+        
+    pos :
+        
+
+    Returns
+    -------
+
+    """
     tos = fstring_stack[-1]
     allow_multiline = tos.allow_multiline()
     if tos.is_in_format_spec():
@@ -372,16 +533,53 @@
 
 
 def tokenize(code, version_info, start_pos=(1, 0)):
-    """Generate tokens from a the source code (string)."""
+    """Generate tokens from a the source code (string).
+
+    Parameters
+    ----------
+    code :
+        
+    version_info :
+        
+    start_pos :
+         (Default value = (1)
+    0) :
+        
+
+    Returns
+    -------
+
+    """
     lines = split_lines(code, keepends=True)
     return tokenize_lines(lines, version_info, start_pos=start_pos)
 
 
 def _print_tokens(func):
-    """
-    A small helper function to help debug the tokenize_lines function.
+    """A small helper function to help debug the tokenize_lines function.
+
+    Parameters
+    ----------
+    func :
+        
+
+    Returns
+    -------
+
     """
     def wrapper(*args, **kwargs):
+        """
+
+        Parameters
+        ----------
+        *args :
+            
+        **kwargs :
+            
+
+        Returns
+        -------
+
+        """
         for token in func(*args, **kwargs):
             print(token)  # This print is intentional for debugging!
             yield token
@@ -391,14 +589,43 @@
 
 # @_print_tokens
 def tokenize_lines(lines, version_info, start_pos=(1, 0), indents=None, is_first_token=True):
-    """
-    A heavily modified Python standard library tokenizer.
-
+    """A heavily modified Python standard library tokenizer.
+    
     Additionally to the default information, yields also the prefix of each
     token. This idea comes from lib2to3. The prefix contains all information
     that is irrelevant for the parser like newlines in parentheses or comments.
+
+    Parameters
+    ----------
+    lines :
+        
+    version_info :
+        
+    start_pos :
+         (Default value = (1)
+    0) :
+        
+    indents :
+         (Default value = None)
+    is_first_token :
+         (Default value = True)
+
+    Returns
+    -------
+
     """
     def dedent_if_necessary(start):
+        """
+
+        Parameters
+        ----------
+        start :
+            
+
+        Returns
+        -------
+
+        """
         while start < indents[-1]:
             if start > indents[-2]:
                 yield PythonToken(ERROR_DEDENT, '', (lnum, start), '')
@@ -673,7 +900,23 @@
 
 
 def _split_illegal_unicode_name(token, start_pos, prefix):
+    """
+
+    Parameters
+    ----------
+    token :
+        
+    start_pos :
+        
+    prefix :
+        
+
+    Returns
+    -------
+
+    """
     def create_token():
+        """ """
         return PythonToken(ERRORTOKEN if is_illegal else NAME, found, pos, prefix)
 
     found = ''
