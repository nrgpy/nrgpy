# Patch generated by Pyment v0.3.3

--- a/..//venv/lib/python3.8/site-packages/pygments/lexers/mime.py
+++ b/..//venv/lib/python3.8/site-packages/pygments/lexers/mime.py
@@ -20,35 +20,41 @@
 
 
 class MIMELexer(RegexLexer):
-    """
-    Lexer for Multipurpose Internet Mail Extensions (MIME) data. This lexer is
+    """Lexer for Multipurpose Internet Mail Extensions (MIME) data. This lexer is
     designed to process the nested mulitpart data.
-
+    
     It assumes that the given data contains both header and body (and is
     splitted by empty line). If no valid header is found, then the entire data
     would be treated as body.
-
+    
     Additional options accepted:
-
+    
     `MIME-max-level`
         Max recurssion level for nested MIME structure. Any negative number
         would treated as unlimited. (default: -1)
-
+    
     `Content-Type`
         Treat the data as specific content type. Useful when header is
         missing, or this lexer would try to parse from header. (default:
         `text/plain`)
-
+    
     `Multipart-Boundary`
         Set the default multipart boundary delimiter. This option is only used
         when `Content-Type` is `multipart` and header is missing. This lexer
         would try to parse from header by default. (default: None)
-
+    
     `Content-Transfer-Encoding`
         Treat the data as specific encoding. Or this lexer would try to parse
         from header by default. (default: None)
-
+    
     .. versionadded:: 2.5
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
 
     name = "MIME"
@@ -65,6 +71,17 @@
         self.max_nested_level = get_int_opt(options, "MIME-max-level", -1)
 
     def analyse_text(text):
+        """
+
+        Parameters
+        ----------
+        text :
+            
+
+        Returns
+        -------
+
+        """
         try:
             header, body = text.strip().split("\n\n", 1)
             if not body.strip():
@@ -80,6 +97,17 @@
             return 0.1
 
     def get_header_tokens(self, match):
+        """
+
+        Parameters
+        ----------
+        match :
+            
+
+        Returns
+        -------
+
+        """
         field = match.group(1)
 
         if field.lower() in self.attention_headers:
@@ -95,6 +123,17 @@
             yield match.start(), Comment, match.group()
 
     def get_body_tokens(self, match):
+        """
+
+        Parameters
+        ----------
+        match :
+            
+
+        Returns
+        -------
+
+        """
         pos_body_start = match.start()
         entire_body = match.group()
 
@@ -144,6 +183,17 @@
             yield pos_part_start, Text, entire_body[lpos_start:]
 
     def get_bodypart_tokens(self, text):
+        """
+
+        Parameters
+        ----------
+        text :
+            
+
+        Returns
+        -------
+
+        """
         # return if:
         #  * no content
         #  * no content type specific
@@ -171,6 +221,17 @@
         return lexer.get_tokens_unprocessed(text)
 
     def store_content_type(self, match):
+        """
+
+        Parameters
+        ----------
+        match :
+            
+
+        Returns
+        -------
+
+        """
         self.content_type = match.group(1)
 
         prefix_len = match.start(1) - match.start(0)
@@ -180,6 +241,17 @@
         yield match.start(3), Name.Label, match.group(3)
 
     def get_content_type_subtokens(self, match):
+        """
+
+        Parameters
+        ----------
+        match :
+            
+
+        Returns
+        -------
+
+        """
         yield match.start(1), Text, match.group(1)
         yield match.start(2), Text.Whitespace, match.group(2)
         yield match.start(3), Name.Attribute, match.group(3)
@@ -193,6 +265,17 @@
             self.boundary = boundary
 
     def store_content_transfer_encoding(self, match):
+        """
+
+        Parameters
+        ----------
+        match :
+            
+
+        Returns
+        -------
+
+        """
         self.content_transfer_encoding = match.group(0).lower()
         yield match.start(0), Name.Constant, match.group(0)
 
