# Patch generated by Pyment v0.3.3

--- a/..//venv/lib/python3.8/site-packages/pygments/lexers/dsls.py
+++ b/..//venv/lib/python3.8/site-packages/pygments/lexers/dsls.py
@@ -22,11 +22,17 @@
 
 
 class ProtoBufLexer(RegexLexer):
-    """
-    Lexer for `Protocol Buffer <http://code.google.com/p/protobuf/>`_
+    """Lexer for `Protocol Buffer <http://code.google.com/p/protobuf/>`_
     definition files.
-
+    
     .. versionadded:: 1.4
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
 
     name = 'Protocol Buffer'
@@ -84,10 +90,16 @@
 
 
 class ThriftLexer(RegexLexer):
-    """
-    For `Thrift <https://thrift.apache.org/>`__ interface definitions.
-
+    """For `Thrift <https://thrift.apache.org/>`__ interface definitions.
+    
     .. versionadded:: 2.1
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'Thrift'
     aliases = ['thrift']
@@ -189,10 +201,16 @@
 
 
 class ZeekLexer(RegexLexer):
-    """
-    For `Zeek <https://www.zeek.org/>`_ scripts.
-
+    """For `Zeek <https://www.zeek.org/>`_ scripts.
+    
     .. versionadded:: 2.5
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'Zeek'
     aliases = ['zeek', 'bro']
@@ -349,10 +367,16 @@
 
 
 class PuppetLexer(RegexLexer):
-    """
-    For `Puppet <http://puppetlabs.com/>`__ configuration DSL.
-
+    """For `Puppet <http://puppetlabs.com/>`__ configuration DSL.
+    
     .. versionadded:: 1.6
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'Puppet'
     aliases = ['puppet']
@@ -436,12 +460,18 @@
 
 
 class RslLexer(RegexLexer):
-    """
-    `RSL <http://en.wikipedia.org/wiki/RAISE>`_ is the formal specification
+    """`RSL <http://en.wikipedia.org/wiki/RAISE>`_ is the formal specification
     language used in RAISE (Rigorous Approach to Industrial Software Engineering)
     method.
-
+    
     .. versionadded:: 2.0
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'RSL'
     aliases = ['rsl']
@@ -488,18 +518,32 @@
     }
 
     def analyse_text(text):
-        """
-        Check for the most common text in the beginning of a RSL file.
+        """Check for the most common text in the beginning of a RSL file.
+
+        Parameters
+        ----------
+        text :
+            
+
+        Returns
+        -------
+
         """
         if re.search(r'scheme\s*.*?=\s*class\s*type', text, re.I) is not None:
             return 1.0
 
 
 class MscgenLexer(RegexLexer):
-    """
-    For `Mscgen <http://www.mcternan.me.uk/mscgen/>`_ files.
-
+    """For `Mscgen <http://www.mcternan.me.uk/mscgen/>`_ files.
+    
     .. versionadded:: 1.6
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'Mscgen'
     aliases = ['mscgen', 'msc']
@@ -545,11 +589,17 @@
 
 
 class VGLLexer(RegexLexer):
-    """
-    For `SampleManager VGL <http://www.thermoscientific.com/samplemanager>`_
+    """For `SampleManager VGL <http://www.thermoscientific.com/samplemanager>`_
     source code.
-
+    
     .. versionadded:: 1.6
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'VGL'
     aliases = ['vgl']
@@ -579,10 +629,16 @@
 
 
 class AlloyLexer(RegexLexer):
-    """
-    For `Alloy <http://alloy.mit.edu>`_ source code.
-
+    """For `Alloy <http://alloy.mit.edu>`_ source code.
+    
     .. versionadded:: 2.0
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
 
     name = 'Alloy'
@@ -639,12 +695,18 @@
 
 
 class PanLexer(RegexLexer):
-    """
-    Lexer for `pan <https://github.com/quattor/pan/>`_ source files.
-
+    """Lexer for `pan <https://github.com/quattor/pan/>`_ source files.
+    
     Based on tcsh lexer.
-
+    
     .. versionadded:: 2.0
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
 
     name = 'Pan'
@@ -707,11 +769,17 @@
 
 
 class CrmshLexer(RegexLexer):
-    """
-    Lexer for `crmsh <http://crmsh.github.io/>`_ configuration files
+    """Lexer for `crmsh <http://crmsh.github.io/>`_ configuration files
     for Pacemaker clusters.
-
+    
     .. versionadded:: 2.1
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'Crmsh'
     aliases = ['crmsh', 'pcmk']
@@ -777,10 +845,16 @@
 
 
 class FlatlineLexer(RegexLexer):
-    """
-    Lexer for `Flatline <https://github.com/bigmlcom/flatline>`_ expressions.
-
+    """Lexer for `Flatline <https://github.com/bigmlcom/flatline>`_ expressions.
+    
     .. versionadded:: 2.2
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
     name = 'Flatline'
     aliases = ['flatline']
@@ -852,10 +926,16 @@
 
 
 class SnowballLexer(ExtendedRegexLexer):
-    """
-    Lexer for `Snowball <http://snowballstem.org/>`_ source code.
-
+    """Lexer for `Snowball <http://snowballstem.org/>`_ source code.
+    
     .. versionadded:: 2.2
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
     """
 
     name = 'Snowball'
@@ -869,11 +949,38 @@
         ExtendedRegexLexer.__init__(self, **options)
 
     def _reset_stringescapes(self):
+        """ """
         self._start = "'"
         self._end = "'"
 
     def _string(do_string_first):
+        """
+
+        Parameters
+        ----------
+        do_string_first :
+            
+
+        Returns
+        -------
+
+        """
         def callback(lexer, match, ctx):
+            """
+
+            Parameters
+            ----------
+            lexer :
+                
+            match :
+                
+            ctx :
+                
+
+            Returns
+            -------
+
+            """
             s = match.start()
             text = match.group()
             string = re.compile(r'([^%s]*)(.)' % re.escape(lexer._start)).match
@@ -901,6 +1008,21 @@
         return callback
 
     def _stringescapes(lexer, match, ctx):
+        """
+
+        Parameters
+        ----------
+        lexer :
+            
+        match :
+            
+        ctx :
+            
+
+        Returns
+        -------
+
+        """
         lexer._start = match.group(3)
         lexer._end = match.group(5)
         return bygroups(Keyword.Reserved, Text, String.Escape, Text,
@@ -956,5 +1078,18 @@
     }
 
     def get_tokens_unprocessed(self, text=None, context=None):
+        """
+
+        Parameters
+        ----------
+        text :
+             (Default value = None)
+        context :
+             (Default value = None)
+
+        Returns
+        -------
+
+        """
         self._reset_stringescapes()
         return ExtendedRegexLexer.get_tokens_unprocessed(self, text, context)
